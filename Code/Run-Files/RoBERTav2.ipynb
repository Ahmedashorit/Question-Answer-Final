{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f45ffd66-c93f-4cad-819c-3bbf812bf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 20.2 MB/s eta 0:00:01    |██▌                             | 64.5 MB 19.6 MB/s eta 0:00:40     |██████████████▉                 | 385.1 MB 20.1 MB/s eta 0:00:23     |██████████████████████▍         | 580.3 MB 8.1 MB/s eta 0:00:31     |██████████████████████████▍     | 685.8 MB 22.8 MB/s eta 0:00:07\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab34aa0e-8641-4b7b-a5cd-67b9e69ff18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 14.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 31.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 47.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 42.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 41.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 22.4 MB/s eta 0:00:01     |█████████████████████████▏      | 23.1 MB 32.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 28.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 37.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.13-py38-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 40.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 38.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 47.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, pyarrow, multiprocess, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.7.0 multiprocess-0.70.12.2 pyarrow-8.0.0 responses-0.18.0 tokenizers-0.12.1 transformers-4.19.2 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d4bee0-3fa7-4089-982f-e92c5f2215f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a3b6094-0709-4a67-9656-d895e3d4595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/25/2022 19:06:09 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/25/2022 19:06:09 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_roberta_base/runs/May25_19-06-09_n9rdura0yr,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_roberta_base,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_roberta_base,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpr_bm3slp\n",
      "Downloading builder script: 5.27kB [00:00, 2.19MB/s]                            \n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpyo9rowke\n",
      "Downloading metadata: 2.36kB [00:00, 1.13MB/s]                                  \n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/25/2022 19:06:10 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/25/2022 19:06:10 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/25/2022 19:06:10 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/25/2022 19:06:10 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "05/25/2022 19:06:10 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpng18ivfy\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  69%|█████████████▊      | 5.59M/8.12M [00:00<00:00, 55.8MB/s]\u001b[A\n",
      "Downloading data: 11.2MB [00:00, 54.4MB/s]                                      \u001b[A\n",
      "Downloading data: 16.6MB [00:00, 53.8MB/s]\u001b[A\n",
      "Downloading data: 22.0MB [00:00, 53.0MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 52.3MB/s]\u001b[A\n",
      "05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.38s/it]05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqr32815y\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 53.9MB/s]                                      \u001b[A\n",
      "05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "05/25/2022 19:06:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.22it/s]\n",
      "05/25/2022 19:06:11 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "05/25/2022 19:06:12 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1090.00it/s]\n",
      "05/25/2022 19:06:12 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "05/25/2022 19:06:12 - INFO - datasets.builder - Generating train split\n",
      "05/25/2022 19:06:21 - INFO - datasets.builder - Generating validation split     \n",
      "05/25/2022 19:06:22 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 541.13it/s]\n",
      "[INFO|hub.py:583] 2022-05-25 19:06:22,869 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp58tnejhz\n",
      "Downloading: 100%|██████████████████████████████| 481/481 [00:00<00:00, 244kB/s]\n",
      "[INFO|hub.py:587] 2022-05-25 19:06:22,947 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|hub.py:595] 2022-05-25 19:06:22,947 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:659] 2022-05-25 19:06:22,948 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-25 19:06:22,950 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:371] 2022-05-25 19:06:23,031 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:659] 2022-05-25 19:06:23,099 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-25 19:06:23,100 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-25 19:06:23,240 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmppc7ygplq\n",
      "Downloading: 100%|███████████████████████████| 878k/878k [00:00<00:00, 4.30MB/s]\n",
      "[INFO|hub.py:587] 2022-05-25 19:06:23,540 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:595] 2022-05-25 19:06:23,540 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:583] 2022-05-25 19:06:23,605 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8_e4s3g1\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 5.12MB/s]\n",
      "[INFO|hub.py:587] 2022-05-25 19:06:23,776 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:595] 2022-05-25 19:06:23,776 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:583] 2022-05-25 19:06:23,842 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpwkjd4ks2\n",
      "Downloading: 100%|█████████████████████████| 1.29M/1.29M [00:00<00:00, 7.39MB/s]\n",
      "[INFO|hub.py:587] 2022-05-25 19:06:24,108 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|hub.py:595] 2022-05-25 19:06:24,108 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,374 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,374 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,375 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,375 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,375 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-25 19:06:24,375 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:659] 2022-05-25 19:06:24,442 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-25 19:06:24,443 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-25 19:06:24,659 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6ikcqrf1\n",
      "Downloading: 100%|███████████████████████████| 478M/478M [00:21<00:00, 23.0MB/s]\n",
      "[INFO|hub.py:587] 2022-05-25 19:06:46,522 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|hub.py:595] 2022-05-25 19:06:46,522 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1953] 2022-05-25 19:06:46,523 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-25 19:06:48,596 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-25 19:06:48,596 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/25/2022 19:06:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-6a6d6a24c53ee304.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:52<00:00,  1.66ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/25/2022 19:07:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-3dbbab53f88860ed.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:52<00:00,  4.82s/ba]\n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp0qoie_lu\n",
      "Downloading builder script: 4.50kB [00:00, 1.71MB/s]                            \n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpafjp4hs1\n",
      "Downloading extra modules: 3.31kB [00:00, 1.49MB/s]                             \n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "05/25/2022 19:08:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-25 19:08:39,687 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-25 19:08:39,687 >>   Num examples = 88568\n",
      "[INFO|trainer.py:1421] 2022-05-25 19:08:39,687 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1422] 2022-05-25 19:08:39,687 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-25 19:08:39,687 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-25 19:08:39,688 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-25 19:08:39,688 >>   Total optimization steps = 14762\n",
      "{'loss': 1.821, 'learning_rate': 2.898387752337082e-05, 'epoch': 0.07}          \n",
      "  3%|█▎                                   | 500/14762 [05:27<2:36:27,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:14:07,648 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:14:07,650 >> Configuration saved in ./squad_roberta_base/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:14:09,038 >> Model weights saved in ./squad_roberta_base/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:14:09,041 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:14:09,077 >> Special tokens file saved in ./squad_roberta_base/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.2343, 'learning_rate': 2.7967755046741633e-05, 'epoch': 0.14}        \n",
      "  7%|██▍                                 | 1000/14762 [11:01<2:30:25,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:19:40,767 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:19:40,769 >> Configuration saved in ./squad_roberta_base/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:19:42,173 >> Model weights saved in ./squad_roberta_base/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:19:42,178 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:19:42,179 >> Special tokens file saved in ./squad_roberta_base/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.1515, 'learning_rate': 2.695163257011245e-05, 'epoch': 0.2}          \n",
      " 10%|███▋                                | 1500/14762 [16:33<2:24:40,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 19:25:12,910 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:25:12,913 >> Configuration saved in ./squad_roberta_base/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:25:14,230 >> Model weights saved in ./squad_roberta_base/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:25:14,265 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:25:14,266 >> Special tokens file saved in ./squad_roberta_base/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.0736, 'learning_rate': 2.593551009348327e-05, 'epoch': 0.27}         \n",
      " 14%|████▉                               | 2000/14762 [22:04<2:19:33,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:30:44,703 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:30:44,705 >> Configuration saved in ./squad_roberta_base/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:30:46,116 >> Model weights saved in ./squad_roberta_base/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:30:46,118 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:30:46,153 >> Special tokens file saved in ./squad_roberta_base/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.0215, 'learning_rate': 2.4919387616854083e-05, 'epoch': 0.34}        \n",
      " 17%|██████                              | 2500/14762 [27:37<2:13:40,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 19:36:17,582 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:36:17,584 >> Configuration saved in ./squad_roberta_base/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:36:18,941 >> Model weights saved in ./squad_roberta_base/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:36:18,953 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:36:19,013 >> Special tokens file saved in ./squad_roberta_base/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.0228, 'learning_rate': 2.39032651402249e-05, 'epoch': 0.41}          \n",
      " 20%|███████▎                            | 3000/14762 [33:10<2:08:45,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:41:50,486 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:41:50,523 >> Configuration saved in ./squad_roberta_base/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:41:51,960 >> Model weights saved in ./squad_roberta_base/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:41:51,961 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:41:52,001 >> Special tokens file saved in ./squad_roberta_base/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 0.9987, 'learning_rate': 2.288714266359572e-05, 'epoch': 0.47}         \n",
      " 24%|████████▌                           | 3500/14762 [38:43<2:02:52,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 19:47:23,120 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:47:23,122 >> Configuration saved in ./squad_roberta_base/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:47:24,420 >> Model weights saved in ./squad_roberta_base/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:47:24,421 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:47:24,468 >> Special tokens file saved in ./squad_roberta_base/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 0.9371, 'learning_rate': 2.1871020186966534e-05, 'epoch': 0.54}        \n",
      " 27%|█████████▊                          | 4000/14762 [44:15<1:57:58,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:52:55,536 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:52:55,538 >> Configuration saved in ./squad_roberta_base/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:52:56,898 >> Model weights saved in ./squad_roberta_base/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:52:56,900 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:52:56,901 >> Special tokens file saved in ./squad_roberta_base/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 0.961, 'learning_rate': 2.085489771033735e-05, 'epoch': 0.61}          \n",
      " 30%|██████████▉                         | 4500/14762 [49:48<1:52:17,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 19:58:28,062 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 19:58:28,064 >> Configuration saved in ./squad_roberta_base/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 19:58:29,328 >> Model weights saved in ./squad_roberta_base/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 19:58:29,330 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 19:58:29,363 >> Special tokens file saved in ./squad_roberta_base/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 0.9085, 'learning_rate': 1.983877523370817e-05, 'epoch': 0.68}         \n",
      " 34%|████████████▏                       | 5000/14762 [55:20<1:46:39,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:04:00,712 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:04:00,760 >> Configuration saved in ./squad_roberta_base/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:04:02,103 >> Model weights saved in ./squad_roberta_base/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:04:02,105 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:04:02,106 >> Special tokens file saved in ./squad_roberta_base/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 0.9207, 'learning_rate': 1.8822652757078987e-05, 'epoch': 0.75}        \n",
      " 37%|████████████▋                     | 5500/14762 [1:00:53<1:41:25,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 20:09:32,808 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:09:32,810 >> Configuration saved in ./squad_roberta_base/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:09:34,208 >> Model weights saved in ./squad_roberta_base/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:09:34,209 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:09:34,210 >> Special tokens file saved in ./squad_roberta_base/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 0.8969, 'learning_rate': 1.7806530280449805e-05, 'epoch': 0.81}        \n",
      " 41%|█████████████▊                    | 6000/14762 [1:06:24<1:35:20,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:15:04,408 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:15:04,410 >> Configuration saved in ./squad_roberta_base/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:15:05,659 >> Model weights saved in ./squad_roberta_base/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:15:05,668 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:15:05,669 >> Special tokens file saved in ./squad_roberta_base/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 0.8942, 'learning_rate': 1.6790407803820623e-05, 'epoch': 0.88}        \n",
      " 44%|██████████████▉                   | 6500/14762 [1:11:55<1:29:54,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:20:35,325 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:20:35,328 >> Configuration saved in ./squad_roberta_base/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:20:36,731 >> Model weights saved in ./squad_roberta_base/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:20:36,733 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:20:36,733 >> Special tokens file saved in ./squad_roberta_base/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 0.899, 'learning_rate': 1.5774285327191438e-05, 'epoch': 0.95}         \n",
      " 47%|████████████████                  | 7000/14762 [1:17:26<1:24:55,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 20:26:06,479 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:26:06,481 >> Configuration saved in ./squad_roberta_base/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:26:07,981 >> Model weights saved in ./squad_roberta_base/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:26:07,982 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:26:08,025 >> Special tokens file saved in ./squad_roberta_base/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.8348, 'learning_rate': 1.4758162850562254e-05, 'epoch': 1.02}        \n",
      " 51%|█████████████████▎                | 7500/14762 [1:22:58<1:19:14,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:31:38,212 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-7500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:31:38,214 >> Configuration saved in ./squad_roberta_base/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:31:39,532 >> Model weights saved in ./squad_roberta_base/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:31:39,534 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:31:39,535 >> Special tokens file saved in ./squad_roberta_base/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.6817, 'learning_rate': 1.3742040373933072e-05, 'epoch': 1.08}        \n",
      " 54%|██████████████████▍               | 8000/14762 [1:28:30<1:13:48,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:37:10,337 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-8000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:37:10,339 >> Configuration saved in ./squad_roberta_base/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:37:11,679 >> Model weights saved in ./squad_roberta_base/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:37:11,694 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:37:11,734 >> Special tokens file saved in ./squad_roberta_base/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.6834, 'learning_rate': 1.272591789730389e-05, 'epoch': 1.15}         \n",
      " 58%|███████████████████▌              | 8500/14762 [1:34:02<1:08:39,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 20:42:42,700 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-8500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:42:42,707 >> Configuration saved in ./squad_roberta_base/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:42:44,144 >> Model weights saved in ./squad_roberta_base/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:42:44,172 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:42:44,173 >> Special tokens file saved in ./squad_roberta_base/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.6637, 'learning_rate': 1.1709795420674706e-05, 'epoch': 1.22}        \n",
      " 61%|████████████████████▋             | 9000/14762 [1:39:35<1:02:51,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:48:14,980 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-9000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:48:14,982 >> Configuration saved in ./squad_roberta_base/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:48:16,336 >> Model weights saved in ./squad_roberta_base/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:48:16,354 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:48:16,355 >> Special tokens file saved in ./squad_roberta_base/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.6761, 'learning_rate': 1.0693672944045522e-05, 'epoch': 1.29}        \n",
      " 64%|███████████████████████▏            | 9500/14762 [1:45:07<57:36,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 20:53:47,060 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-9500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:53:47,062 >> Configuration saved in ./squad_roberta_base/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:53:48,289 >> Model weights saved in ./squad_roberta_base/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:53:48,291 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:53:48,292 >> Special tokens file saved in ./squad_roberta_base/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.6653, 'learning_rate': 9.67755046741634e-06, 'epoch': 1.35}          \n",
      " 68%|███████████████████████▋           | 10000/14762 [1:50:39<52:01,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 20:59:18,771 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-10000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 20:59:18,773 >> Configuration saved in ./squad_roberta_base/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 20:59:20,188 >> Model weights saved in ./squad_roberta_base/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 20:59:20,189 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 20:59:20,229 >> Special tokens file saved in ./squad_roberta_base/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.6777, 'learning_rate': 8.661427990787156e-06, 'epoch': 1.42}         \n",
      " 71%|████████████████████████▉          | 10500/14762 [1:56:11<46:36,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:04:51,052 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-10500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:04:51,094 >> Configuration saved in ./squad_roberta_base/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:04:52,369 >> Model weights saved in ./squad_roberta_base/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:04:52,370 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:04:52,371 >> Special tokens file saved in ./squad_roberta_base/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.6319, 'learning_rate': 7.645305514157972e-06, 'epoch': 1.49}         \n",
      " 75%|██████████████████████████         | 11000/14762 [2:01:43<41:15,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:10:23,129 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-11000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:10:23,131 >> Configuration saved in ./squad_roberta_base/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:10:24,579 >> Model weights saved in ./squad_roberta_base/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:10:24,627 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:10:24,633 >> Special tokens file saved in ./squad_roberta_base/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.6631, 'learning_rate': 6.629183037528791e-06, 'epoch': 1.56}         \n",
      " 78%|███████████████████████████▎       | 11500/14762 [2:07:15<35:40,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:15:55,747 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-11500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:15:55,750 >> Configuration saved in ./squad_roberta_base/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:15:57,122 >> Model weights saved in ./squad_roberta_base/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:15:57,184 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:15:57,203 >> Special tokens file saved in ./squad_roberta_base/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.6453, 'learning_rate': 5.613060560899607e-06, 'epoch': 1.63}         \n",
      " 81%|████████████████████████████▍      | 12000/14762 [2:12:48<30:11,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:21:28,373 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-12000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:21:28,375 >> Configuration saved in ./squad_roberta_base/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:21:29,744 >> Model weights saved in ./squad_roberta_base/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:21:29,745 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:21:29,746 >> Special tokens file saved in ./squad_roberta_base/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.6546, 'learning_rate': 4.596938084270424e-06, 'epoch': 1.69}         \n",
      " 85%|█████████████████████████████▋     | 12500/14762 [2:18:20<24:47,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:27:00,681 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-12500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:27:00,684 >> Configuration saved in ./squad_roberta_base/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:27:02,086 >> Model weights saved in ./squad_roberta_base/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:27:02,087 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:27:02,088 >> Special tokens file saved in ./squad_roberta_base/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.6626, 'learning_rate': 3.580815607641241e-06, 'epoch': 1.76}         \n",
      " 88%|██████████████████████████████▊    | 13000/14762 [2:23:53<19:17,  1.52it/s][INFO|trainer.py:2340] 2022-05-25 21:32:32,810 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-13000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:32:32,811 >> Configuration saved in ./squad_roberta_base/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:32:34,259 >> Model weights saved in ./squad_roberta_base/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:32:34,271 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:32:34,308 >> Special tokens file saved in ./squad_roberta_base/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.621, 'learning_rate': 2.5646931310120583e-06, 'epoch': 1.83}         \n",
      " 91%|████████████████████████████████   | 13500/14762 [2:29:25<13:46,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 21:38:04,956 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-13500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:38:04,958 >> Configuration saved in ./squad_roberta_base/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:38:06,499 >> Model weights saved in ./squad_roberta_base/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:38:06,501 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:38:06,502 >> Special tokens file saved in ./squad_roberta_base/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.6442, 'learning_rate': 1.548570654382875e-06, 'epoch': 1.9}          \n",
      " 95%|█████████████████████████████████▏ | 14000/14762 [2:34:57<08:19,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 21:43:37,188 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-14000\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:43:37,224 >> Configuration saved in ./squad_roberta_base/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:43:38,548 >> Model weights saved in ./squad_roberta_base/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:43:38,549 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:43:38,550 >> Special tokens file saved in ./squad_roberta_base/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.6272, 'learning_rate': 5.32448177753692e-07, 'epoch': 1.96}          \n",
      " 98%|██████████████████████████████████▍| 14500/14762 [2:40:29<02:51,  1.53it/s][INFO|trainer.py:2340] 2022-05-25 21:49:08,944 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-14500\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:49:08,946 >> Configuration saved in ./squad_roberta_base/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:49:10,186 >> Model weights saved in ./squad_roberta_base/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:49:10,188 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:49:10,189 >> Special tokens file saved in ./squad_roberta_base/checkpoint-14500/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 14762/14762 [2:43:24<00:00,  1.67it/s][INFO|trainer.py:1662] 2022-05-25 21:52:04,540 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 9804.854, 'train_samples_per_second': 18.066, 'train_steps_per_second': 1.506, 'train_loss': 0.850282254314022, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████| 14762/14762 [2:43:25<00:00,  1.51it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-25 21:52:04,755 >> Saving model checkpoint to ./squad_roberta_base\n",
      "[INFO|configuration_utils.py:446] 2022-05-25 21:52:04,757 >> Configuration saved in ./squad_roberta_base/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-25 21:52:06,206 >> Model weights saved in ./squad_roberta_base/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-25 21:52:06,220 >> tokenizer config file saved in ./squad_roberta_base/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-25 21:52:06,221 >> Special tokens file saved in ./squad_roberta_base/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8503\n",
      "  train_runtime            = 2:43:24.85\n",
      "  train_samples            =      88568\n",
      "  train_samples_per_second =     18.066\n",
      "  train_steps_per_second   =      1.506\n",
      "05/25/2022 21:52:06 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-25 21:52:06,452 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-25 21:52:06,463 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-25 21:52:06,463 >>   Num examples = 10790\n",
      "[INFO|trainer.py:2595] 2022-05-25 21:52:06,463 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1349/1349 [03:24<00:00,  7.12it/s]05/25/2022 21:55:44 - INFO - utils_qa - Post-processing 10570 example predictions split into 10790 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                       | 24/10570 [00:00<00:44, 236.47it/s]\u001b[A\n",
      "  1%|▏                                      | 55/10570 [00:00<00:37, 277.87it/s]\u001b[A\n",
      "  1%|▎                                      | 92/10570 [00:00<00:33, 317.47it/s]\u001b[A\n",
      "  1%|▍                                     | 126/10570 [00:00<00:32, 325.29it/s]\u001b[A\n",
      "  2%|▌                                     | 159/10570 [00:00<00:34, 303.64it/s]\u001b[A\n",
      "  2%|▋                                     | 190/10570 [00:00<00:34, 302.76it/s]\u001b[A\n",
      "  2%|▊                                     | 221/10570 [00:00<00:34, 297.96it/s]\u001b[A\n",
      "  2%|▉                                     | 251/10570 [00:00<00:36, 282.98it/s]\u001b[A\n",
      "  3%|█                                     | 280/10570 [00:01<00:43, 235.97it/s]\u001b[A\n",
      "  3%|█                                     | 311/10570 [00:01<00:40, 254.24it/s]\u001b[A\n",
      "  3%|█▏                                    | 347/10570 [00:01<00:36, 282.21it/s]\u001b[A\n",
      "  4%|█▍                                    | 384/10570 [00:01<00:33, 306.42it/s]\u001b[A\n",
      "  4%|█▍                                    | 416/10570 [00:01<00:34, 295.68it/s]\u001b[A\n",
      "  4%|█▌                                    | 451/10570 [00:01<00:32, 308.24it/s]\u001b[A\n",
      "  5%|█▋                                    | 486/10570 [00:01<00:31, 318.83it/s]\u001b[A\n",
      "  5%|█▉                                    | 522/10570 [00:01<00:30, 328.64it/s]\u001b[A\n",
      "  5%|██                                    | 558/10570 [00:01<00:29, 337.26it/s]\u001b[A\n",
      "  6%|██▏                                   | 593/10570 [00:01<00:29, 339.65it/s]\u001b[A\n",
      "  6%|██▎                                   | 629/10570 [00:02<00:28, 345.17it/s]\u001b[A\n",
      "  6%|██▍                                   | 664/10570 [00:02<00:29, 337.65it/s]\u001b[A\n",
      "  7%|██▌                                   | 702/10570 [00:02<00:28, 349.54it/s]\u001b[A\n",
      "  7%|██▋                                   | 738/10570 [00:02<00:28, 339.47it/s]\u001b[A\n",
      "  7%|██▊                                   | 773/10570 [00:02<00:29, 331.30it/s]\u001b[A\n",
      "  8%|██▉                                   | 807/10570 [00:02<00:31, 313.91it/s]\u001b[A\n",
      "  8%|███                                   | 839/10570 [00:02<00:32, 295.60it/s]\u001b[A\n",
      "  8%|███                                   | 869/10570 [00:02<00:33, 285.40it/s]\u001b[A\n",
      "  9%|███▏                                  | 900/10570 [00:02<00:33, 291.29it/s]\u001b[A\n",
      "  9%|███▎                                  | 930/10570 [00:03<00:34, 276.25it/s]\u001b[A\n",
      "  9%|███▍                                  | 958/10570 [00:03<00:34, 275.83it/s]\u001b[A\n",
      "  9%|███▌                                  | 986/10570 [00:03<00:35, 273.35it/s]\u001b[A\n",
      " 10%|███▌                                 | 1014/10570 [00:03<00:35, 269.04it/s]\u001b[A\n",
      " 10%|███▋                                 | 1045/10570 [00:03<00:34, 279.87it/s]\u001b[A\n",
      " 10%|███▊                                 | 1075/10570 [00:03<00:33, 284.65it/s]\u001b[A\n",
      " 10%|███▊                                 | 1104/10570 [00:03<00:33, 285.78it/s]\u001b[A\n",
      " 11%|███▉                                 | 1136/10570 [00:03<00:31, 295.42it/s]\u001b[A\n",
      " 11%|████                                 | 1170/10570 [00:03<00:30, 306.68it/s]\u001b[A\n",
      " 11%|████▏                                | 1203/10570 [00:03<00:30, 311.54it/s]\u001b[A\n",
      " 12%|████▎                                | 1235/10570 [00:04<00:30, 307.65it/s]\u001b[A\n",
      " 12%|████▍                                | 1266/10570 [00:04<00:31, 294.72it/s]\u001b[A\n",
      " 12%|████▌                                | 1299/10570 [00:04<00:30, 303.12it/s]\u001b[A\n",
      " 13%|████▋                                | 1333/10570 [00:04<00:29, 312.90it/s]\u001b[A\n",
      " 13%|████▊                                | 1365/10570 [00:04<00:29, 313.13it/s]\u001b[A\n",
      " 13%|████▉                                | 1397/10570 [00:04<00:29, 307.74it/s]\u001b[A\n",
      " 14%|█████                                | 1430/10570 [00:04<00:29, 312.97it/s]\u001b[A\n",
      " 14%|█████                                | 1464/10570 [00:04<00:28, 318.96it/s]\u001b[A\n",
      " 14%|█████▏                               | 1498/10570 [00:04<00:28, 323.71it/s]\u001b[A\n",
      " 14%|█████▎                               | 1531/10570 [00:05<00:27, 323.20it/s]\u001b[A\n",
      " 15%|█████▍                               | 1564/10570 [00:05<00:28, 312.74it/s]\u001b[A\n",
      " 15%|█████▌                               | 1596/10570 [00:05<00:28, 310.85it/s]\u001b[A\n",
      " 15%|█████▋                               | 1632/10570 [00:05<00:27, 323.34it/s]\u001b[A\n",
      " 16%|█████▊                               | 1667/10570 [00:05<00:27, 329.47it/s]\u001b[A\n",
      " 16%|█████▉                               | 1704/10570 [00:05<00:26, 339.17it/s]\u001b[A\n",
      " 16%|██████                               | 1738/10570 [00:05<00:26, 327.79it/s]\u001b[A\n",
      " 17%|██████▏                              | 1771/10570 [00:05<00:27, 323.92it/s]\u001b[A\n",
      " 17%|██████▎                              | 1804/10570 [00:05<00:29, 294.91it/s]\u001b[A\n",
      " 17%|██████▍                              | 1834/10570 [00:06<00:30, 284.25it/s]\u001b[A\n",
      " 18%|██████▌                              | 1867/10570 [00:06<00:29, 296.18it/s]\u001b[A\n",
      " 18%|██████▋                              | 1905/10570 [00:06<00:27, 318.51it/s]\u001b[A\n",
      " 18%|██████▊                              | 1942/10570 [00:06<00:25, 333.13it/s]\u001b[A\n",
      " 19%|██████▉                              | 1979/10570 [00:06<00:25, 341.62it/s]\u001b[A\n",
      " 19%|███████                              | 2014/10570 [00:06<00:25, 336.03it/s]\u001b[A\n",
      " 19%|███████▏                             | 2049/10570 [00:06<00:25, 339.06it/s]\u001b[A\n",
      " 20%|███████▎                             | 2086/10570 [00:06<00:24, 346.00it/s]\u001b[A\n",
      " 20%|███████▍                             | 2121/10570 [00:06<00:25, 335.54it/s]\u001b[A\n",
      " 20%|███████▌                             | 2155/10570 [00:06<00:25, 329.86it/s]\u001b[A\n",
      " 21%|███████▋                             | 2190/10570 [00:07<00:25, 334.55it/s]\u001b[A\n",
      " 21%|███████▊                             | 2228/10570 [00:07<00:24, 346.05it/s]\u001b[A\n",
      " 21%|███████▉                             | 2264/10570 [00:07<00:23, 349.72it/s]\u001b[A\n",
      " 22%|████████                             | 2304/10570 [00:07<00:22, 363.76it/s]\u001b[A\n",
      " 22%|████████▏                            | 2344/10570 [00:07<00:22, 372.20it/s]\u001b[A\n",
      " 23%|████████▎                            | 2382/10570 [00:07<00:22, 364.05it/s]\u001b[A\n",
      " 23%|████████▍                            | 2419/10570 [00:07<00:22, 358.56it/s]\u001b[A\n",
      " 23%|████████▌                            | 2455/10570 [00:07<00:23, 347.30it/s]\u001b[A\n",
      " 24%|████████▋                            | 2490/10570 [00:07<00:24, 327.37it/s]\u001b[A\n",
      " 24%|████████▊                            | 2523/10570 [00:08<00:24, 325.35it/s]\u001b[A\n",
      " 24%|████████▉                            | 2556/10570 [00:08<00:25, 317.50it/s]\u001b[A\n",
      " 24%|█████████                            | 2588/10570 [00:08<00:25, 313.83it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2625/10570 [00:08<00:24, 327.83it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2659/10570 [00:08<00:24, 329.14it/s]\u001b[A\n",
      " 25%|█████████▍                           | 2692/10570 [00:08<00:23, 328.70it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2725/10570 [00:08<00:23, 327.16it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2758/10570 [00:08<00:24, 318.19it/s]\u001b[A\n",
      " 26%|█████████▊                           | 2794/10570 [00:08<00:23, 329.86it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2828/10570 [00:08<00:24, 319.93it/s]\u001b[A\n",
      " 27%|██████████                           | 2861/10570 [00:09<00:24, 318.78it/s]\u001b[A\n",
      " 27%|██████████▏                          | 2893/10570 [00:09<00:24, 314.98it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2928/10570 [00:09<00:23, 323.75it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2965/10570 [00:09<00:22, 334.53it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2999/10570 [00:09<00:23, 328.68it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3032/10570 [00:09<00:23, 326.74it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3065/10570 [00:09<00:23, 324.96it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3098/10570 [00:09<00:23, 311.61it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3131/10570 [00:09<00:23, 315.45it/s]\u001b[A\n",
      " 30%|███████████                          | 3163/10570 [00:10<00:23, 313.17it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3195/10570 [00:10<00:25, 293.62it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3225/10570 [00:10<00:29, 252.79it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3252/10570 [00:10<00:29, 245.46it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3278/10570 [00:10<00:30, 236.90it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3303/10570 [00:10<00:31, 228.68it/s]\u001b[A\n",
      " 31%|███████████▋                         | 3327/10570 [00:10<00:32, 221.33it/s]\u001b[A\n",
      " 32%|███████████▋                         | 3350/10570 [00:10<00:32, 222.21it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3377/10570 [00:10<00:30, 234.98it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3407/10570 [00:11<00:28, 251.48it/s]\u001b[A\n",
      " 33%|████████████                         | 3438/10570 [00:11<00:26, 266.16it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3469/10570 [00:11<00:25, 277.35it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3500/10570 [00:11<00:24, 286.45it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3535/10570 [00:11<00:23, 303.32it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3566/10570 [00:11<00:23, 303.40it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3599/10570 [00:11<00:22, 306.76it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3635/10570 [00:11<00:21, 319.79it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3668/10570 [00:11<00:21, 317.75it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3700/10570 [00:12<00:21, 316.30it/s]\u001b[A\n",
      " 35%|█████████████                        | 3735/10570 [00:12<00:21, 325.46it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3768/10570 [00:12<00:21, 315.38it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3802/10570 [00:12<00:21, 320.57it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3836/10570 [00:12<00:20, 325.08it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3869/10570 [00:12<00:20, 320.82it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3902/10570 [00:12<00:20, 318.08it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3934/10570 [00:12<00:20, 318.19it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3971/10570 [00:12<00:19, 332.36it/s]\u001b[A\n",
      " 38%|██████████████                       | 4005/10570 [00:12<00:21, 311.40it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4037/10570 [00:13<00:21, 309.12it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4072/10570 [00:13<00:20, 320.60it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4105/10570 [00:13<00:21, 299.65it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4136/10570 [00:13<00:22, 289.46it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4166/10570 [00:13<00:26, 244.96it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4192/10570 [00:13<00:29, 218.25it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4216/10570 [00:13<00:29, 212.09it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4245/10570 [00:13<00:27, 229.84it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4269/10570 [00:14<00:33, 190.49it/s]\u001b[A\n",
      " 41%|███████████████                      | 4290/10570 [00:14<00:36, 170.91it/s]\u001b[A\n",
      " 41%|███████████████                      | 4309/10570 [00:14<00:38, 161.53it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4340/10570 [00:14<00:31, 194.99it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4369/10570 [00:14<00:28, 217.54it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4401/10570 [00:14<00:25, 243.03it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4435/10570 [00:14<00:22, 268.65it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4468/10570 [00:14<00:21, 283.76it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4498/10570 [00:15<00:22, 270.03it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4526/10570 [00:15<00:23, 262.12it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4556/10570 [00:15<00:22, 271.17it/s]\u001b[A\n",
      " 43%|████████████████                     | 4585/10570 [00:15<00:21, 275.47it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4613/10570 [00:15<00:22, 259.37it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4641/10570 [00:15<00:22, 259.64it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4668/10570 [00:15<00:24, 236.94it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4696/10570 [00:15<00:23, 246.24it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4722/10570 [00:15<00:23, 248.17it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4748/10570 [00:16<00:23, 250.58it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4774/10570 [00:16<00:23, 247.39it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4807/10570 [00:16<00:21, 270.05it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4837/10570 [00:16<00:20, 277.93it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4865/10570 [00:16<00:21, 265.98it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4897/10570 [00:16<00:20, 280.66it/s]\u001b[A\n",
      " 47%|█████████████████▏                   | 4926/10570 [00:16<00:20, 274.47it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4958/10570 [00:16<00:19, 285.30it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4987/10570 [00:16<00:19, 286.11it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5018/10570 [00:17<00:19, 291.94it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5048/10570 [00:17<00:19, 289.17it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5080/10570 [00:17<00:18, 296.35it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5115/10570 [00:17<00:17, 312.03it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5147/10570 [00:17<00:17, 311.88it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5182/10570 [00:17<00:16, 321.47it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5215/10570 [00:17<00:17, 313.19it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5248/10570 [00:17<00:16, 318.04it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5284/10570 [00:17<00:16, 328.72it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5319/10570 [00:17<00:15, 331.28it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5353/10570 [00:18<00:17, 300.38it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5384/10570 [00:18<00:18, 282.86it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5413/10570 [00:18<00:18, 284.74it/s]\u001b[A\n",
      " 51%|███████████████████                  | 5442/10570 [00:18<00:19, 269.06it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5470/10570 [00:18<00:21, 238.76it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5495/10570 [00:18<00:23, 218.68it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5518/10570 [00:18<00:24, 210.07it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 5544/10570 [00:18<00:22, 221.21it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5567/10570 [00:19<00:23, 214.37it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5589/10570 [00:19<00:23, 211.21it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5613/10570 [00:19<00:22, 217.92it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5636/10570 [00:19<00:22, 221.13it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5659/10570 [00:19<00:23, 213.09it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5686/10570 [00:19<00:21, 228.54it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5714/10570 [00:19<00:20, 242.59it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5740/10570 [00:19<00:19, 246.95it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5771/10570 [00:19<00:18, 264.15it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5803/10570 [00:20<00:17, 280.23it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5832/10570 [00:20<00:16, 281.80it/s]\u001b[A\n",
      " 55%|████████████████████▌                | 5861/10570 [00:20<00:16, 280.17it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5893/10570 [00:20<00:16, 290.11it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5923/10570 [00:20<00:17, 271.06it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5951/10570 [00:20<00:17, 265.83it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5978/10570 [00:20<00:17, 262.12it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6005/10570 [00:20<00:18, 248.89it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6033/10570 [00:20<00:17, 253.84it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6059/10570 [00:20<00:18, 247.76it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6087/10570 [00:21<00:17, 254.70it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6117/10570 [00:21<00:16, 266.58it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6151/10570 [00:21<00:15, 287.62it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6184/10570 [00:21<00:14, 298.07it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6216/10570 [00:21<00:14, 301.93it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6247/10570 [00:21<00:14, 290.65it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6277/10570 [00:21<00:14, 286.69it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6312/10570 [00:21<00:14, 302.79it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6344/10570 [00:21<00:13, 307.06it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6375/10570 [00:22<00:13, 302.52it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6407/10570 [00:22<00:13, 305.80it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6443/10570 [00:22<00:12, 320.75it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6480/10570 [00:22<00:12, 333.70it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6514/10570 [00:22<00:12, 332.96it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6548/10570 [00:22<00:12, 334.61it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6582/10570 [00:22<00:12, 323.51it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6615/10570 [00:22<00:12, 317.57it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6654/10570 [00:22<00:11, 338.15it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6688/10570 [00:23<00:12, 298.97it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6719/10570 [00:23<00:13, 293.76it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6751/10570 [00:23<00:12, 299.57it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6782/10570 [00:23<00:12, 294.72it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6812/10570 [00:23<00:12, 294.57it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6842/10570 [00:23<00:12, 292.62it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6877/10570 [00:23<00:12, 307.03it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6908/10570 [00:23<00:12, 305.09it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6942/10570 [00:23<00:11, 313.17it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6974/10570 [00:23<00:11, 310.52it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7006/10570 [00:24<00:11, 306.79it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7037/10570 [00:24<00:11, 302.57it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7068/10570 [00:24<00:11, 303.70it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7103/10570 [00:24<00:10, 315.24it/s]\u001b[A\n",
      " 68%|████████████████████████▉            | 7136/10570 [00:24<00:10, 318.30it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7168/10570 [00:24<00:10, 314.16it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7200/10570 [00:24<00:10, 308.27it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7231/10570 [00:24<00:10, 306.39it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7262/10570 [00:24<00:10, 303.57it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7293/10570 [00:24<00:10, 302.19it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7324/10570 [00:25<00:11, 287.47it/s]\u001b[A\n",
      " 70%|█████████████████████████▋           | 7353/10570 [00:25<00:12, 265.94it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7384/10570 [00:25<00:11, 275.78it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7416/10570 [00:25<00:10, 288.00it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7446/10570 [00:25<00:11, 282.04it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7475/10570 [00:25<00:11, 274.24it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7504/10570 [00:25<00:11, 278.34it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7537/10570 [00:25<00:10, 291.48it/s]\u001b[A\n",
      " 72%|██████████████████████████▍          | 7569/10570 [00:25<00:10, 299.43it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7601/10570 [00:26<00:09, 305.11it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7632/10570 [00:26<00:09, 306.12it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7663/10570 [00:26<00:10, 285.82it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7696/10570 [00:26<00:09, 298.26it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7728/10570 [00:26<00:09, 304.21it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7759/10570 [00:26<00:09, 293.07it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7789/10570 [00:26<00:09, 283.19it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7818/10570 [00:26<00:09, 284.64it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7847/10570 [00:26<00:09, 285.11it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7876/10570 [00:27<00:09, 283.49it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7908/10570 [00:27<00:09, 292.01it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7943/10570 [00:27<00:08, 306.90it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7974/10570 [00:27<00:08, 295.03it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8004/10570 [00:27<00:08, 295.97it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8039/10570 [00:27<00:08, 308.82it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8070/10570 [00:27<00:08, 304.15it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8107/10570 [00:27<00:07, 322.29it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8140/10570 [00:27<00:07, 318.32it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8172/10570 [00:27<00:07, 310.60it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8204/10570 [00:28<00:08, 287.89it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8236/10570 [00:28<00:07, 294.57it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8266/10570 [00:28<00:07, 289.82it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 8296/10570 [00:28<00:08, 273.26it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8330/10570 [00:28<00:07, 289.53it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8364/10570 [00:28<00:07, 303.09it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8396/10570 [00:28<00:07, 307.08it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8430/10570 [00:28<00:06, 315.44it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8463/10570 [00:28<00:06, 319.41it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8496/10570 [00:29<00:06, 311.59it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8530/10570 [00:29<00:06, 317.82it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8564/10570 [00:29<00:06, 323.36it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8598/10570 [00:29<00:06, 325.21it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8631/10570 [00:29<00:06, 316.69it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8663/10570 [00:29<00:06, 300.31it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8694/10570 [00:29<00:06, 294.92it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8724/10570 [00:29<00:06, 292.78it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8754/10570 [00:29<00:06, 294.11it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8784/10570 [00:30<00:06, 277.74it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8812/10570 [00:30<00:06, 276.83it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8840/10570 [00:30<00:06, 271.86it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8872/10570 [00:30<00:05, 283.54it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8901/10570 [00:30<00:05, 284.03it/s]\u001b[A\n",
      " 84%|███████████████████████████████▎     | 8930/10570 [00:30<00:05, 284.39it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8959/10570 [00:30<00:05, 272.91it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8987/10570 [00:30<00:06, 256.05it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9013/10570 [00:30<00:06, 246.87it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9041/10570 [00:31<00:06, 253.84it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9071/10570 [00:31<00:05, 265.13it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9104/10570 [00:31<00:05, 282.74it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9137/10570 [00:31<00:04, 294.60it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9169/10570 [00:31<00:04, 299.90it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9201/10570 [00:31<00:04, 303.02it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9232/10570 [00:31<00:04, 289.25it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9263/10570 [00:31<00:04, 293.68it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9298/10570 [00:31<00:04, 308.33it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9329/10570 [00:31<00:04, 297.12it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9359/10570 [00:32<00:04, 293.83it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9389/10570 [00:32<00:04, 294.43it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9423/10570 [00:32<00:03, 306.44it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9458/10570 [00:32<00:03, 317.31it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9490/10570 [00:32<00:03, 306.42it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9523/10570 [00:32<00:03, 311.94it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9555/10570 [00:32<00:03, 307.41it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9587/10570 [00:32<00:03, 308.66it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9619/10570 [00:32<00:03, 310.14it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9651/10570 [00:33<00:03, 303.09it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9682/10570 [00:33<00:03, 294.88it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9716/10570 [00:33<00:02, 306.16it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9748/10570 [00:33<00:02, 309.90it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▏  | 9780/10570 [00:33<00:02, 310.41it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9812/10570 [00:33<00:02, 305.14it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9843/10570 [00:33<00:02, 285.63it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9873/10570 [00:33<00:02, 288.86it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9903/10570 [00:33<00:02, 290.27it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9935/10570 [00:33<00:02, 297.64it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9965/10570 [00:34<00:02, 291.10it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▉  | 9995/10570 [00:34<00:02, 286.62it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10025/10570 [00:34<00:01, 290.17it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10059/10570 [00:34<00:01, 302.00it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10090/10570 [00:34<00:01, 300.83it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10123/10570 [00:34<00:01, 308.29it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10155/10570 [00:34<00:01, 309.66it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10187/10570 [00:34<00:01, 304.92it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10222/10570 [00:34<00:01, 317.47it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10254/10570 [00:35<00:00, 316.52it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10286/10570 [00:35<00:00, 313.20it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10318/10570 [00:35<00:00, 301.29it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10349/10570 [00:35<00:00, 290.18it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10379/10570 [00:35<00:00, 287.11it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10409/10570 [00:35<00:00, 289.21it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10438/10570 [00:35<00:00, 281.18it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10467/10570 [00:35<00:00, 271.77it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10496/10570 [00:35<00:00, 276.35it/s]\u001b[A\n",
      "100%|███████████████████████████████████▊| 10526/10570 [00:35<00:00, 282.75it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:36<00:00, 292.64it/s]\u001b[A\n",
      "05/25/2022 21:56:21 - INFO - utils_qa - Saving predictions to ./squad_roberta_base/eval_predictions.json.\n",
      "05/25/2022 21:56:21 - INFO - utils_qa - Saving nbest_preds to ./squad_roberta_base/eval_nbest_predictions.json.\n",
      "05/25/2022 21:56:28 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1349/1349 [04:22<00:00,  5.14it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     2.0\n",
      "  eval_exact_match = 86.3576\n",
      "  eval_f1          = 92.3827\n",
      "  eval_samples     =   10790\n"
     ]
    }
   ],
   "source": [
    "!sh train_roberta.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

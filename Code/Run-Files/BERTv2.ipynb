{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d64b5bb-65b4-4d65-9070-0395515b7bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 28.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 39.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 17.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 40.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 21.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 32.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 31.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, pyarrow, multiprocess, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.6.0 multiprocess-0.70.12.2 pyarrow-8.0.0 responses-0.18.0 tokenizers-0.12.1 transformers-4.19.2 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "466d228b-5ad5-4c95-815f-51331c9918c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 19.3 MB/s eta 0:00:01    |█▉                              | 46.6 MB 24.8 MB/s eta 0:00:32     |██▏                             | 56.4 MB 19.7 MB/s eta 0:00:40MB 24.6 MB/s eta 0:00:27     |█████████████████████████▍      | 660.7 MB 21.0 MB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe55c00c-df29-4181-bae7-6c9148fc7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e4478a3-7876-46ce-bba5-1a77018d58ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/23/2022 12:29:35 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/23/2022 12:29:35 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./debug_squad/runs/May23_12-29-35_new1v3efzu,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./debug_squad/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./debug_squad/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpafto34eh\n",
      "Downloading builder script: 5.27kB [00:00, 3.50MB/s]                            \n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp7hdxxog1\n",
      "Downloading metadata: 2.36kB [00:00, 2.14MB/s]                                  \n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/23/2022 12:29:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/23/2022 12:29:35 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/23/2022 12:29:35 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/23/2022 12:29:35 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "05/23/2022 12:29:36 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp6ayfoe93\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 8.60MB [00:00, 86.0MB/s]                                      \u001b[A\n",
      "Downloading data: 17.5MB [00:00, 87.8MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 82.4MB/s]\u001b[A\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:00<00:00,  2.18it/s]05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpprd30njp\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 93.6MB/s]                                      \u001b[A\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:00<00:00,  3.48it/s]\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1441.34it/s]\n",
      "05/23/2022 12:29:36 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "05/23/2022 12:29:36 - INFO - datasets.builder - Generating train split\n",
      "05/23/2022 12:29:43 - INFO - datasets.builder - Generating validation split     \n",
      "05/23/2022 12:29:44 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 722.22it/s]\n",
      "[INFO|hub.py:583] 2022-05-23 12:29:44,181 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpaij68oxn\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 479kB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 12:29:44,243 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|hub.py:595] 2022-05-23 12:29:44,243 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 12:29:44,244 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 12:29:44,244 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 12:29:44,304 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpm41z_o_5\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 22.7kB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 12:29:44,374 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|hub.py:595] 2022-05-23 12:29:44,375 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 12:29:44,431 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 12:29:44,432 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 12:29:44,556 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpziewturj\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 7.44MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 12:29:44,653 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:595] 2022-05-23 12:29:44,653 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:583] 2022-05-23 12:29:44,712 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpa50zufus\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 10.5MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 12:29:44,822 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|hub.py:595] 2022-05-23 12:29:44,822 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 12:29:45,018 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 12:29:45,019 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 12:29:45,019 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 12:29:45,019 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 12:29:45,019 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 12:29:45,076 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 12:29:45,076 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 12:29:45,191 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmb5g8v49\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [00:18<00:00, 23.6MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 12:30:03,895 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|hub.py:595] 2022-05-23 12:30:03,895 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1953] 2022-05-23 12:30:03,895 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-23 12:30:05,452 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-23 12:30:05,452 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/23/2022 12:30:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-1d1db2aa4978ff8d.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:37<00:00,  2.34ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/23/2022 12:30:43 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-f9f3f6120696991b.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:35<00:00,  3.24s/ba]\n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpolxmfvxz\n",
      "Downloading builder script: 4.50kB [00:00, 2.97MB/s]                            \n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpnc8kgicg\n",
      "Downloading extra modules: 3.31kB [00:00, 3.40MB/s]                             \n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "05/23/2022 12:31:19 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-23 12:31:23,814 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-23 12:31:23,814 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1421] 2022-05-23 12:31:23,814 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1422] 2022-05-23 12:31:23,815 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-23 12:31:23,815 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-23 12:31:23,815 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-23 12:31:23,815 >>   Total optimization steps = 22131\n",
      "{'loss': 2.4273, 'learning_rate': 2.932221770367358e-05, 'epoch': 0.07}         \n",
      "  2%|▊                                    | 500/22131 [03:30<2:32:28,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 12:34:54,053 >> Saving model checkpoint to ./debug_squad/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:34:54,058 >> Configuration saved in ./debug_squad/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:34:57,820 >> Model weights saved in ./debug_squad/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:34:57,823 >> tokenizer config file saved in ./debug_squad/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:34:57,858 >> Special tokens file saved in ./debug_squad/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.4979, 'learning_rate': 2.864443540734716e-05, 'epoch': 0.14}         \n",
      "  5%|█▋                                  | 1000/22131 [07:11<2:28:00,  2.38it/s][INFO|trainer.py:2340] 2022-05-23 12:38:35,741 >> Saving model checkpoint to ./debug_squad/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:38:35,785 >> Configuration saved in ./debug_squad/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:38:40,034 >> Model weights saved in ./debug_squad/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:38:40,037 >> tokenizer config file saved in ./debug_squad/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:38:40,077 >> Special tokens file saved in ./debug_squad/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3922, 'learning_rate': 2.796665311102074e-05, 'epoch': 0.2}          \n",
      "  7%|██▍                                 | 1500/22131 [10:54<2:25:15,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 12:42:18,024 >> Saving model checkpoint to ./debug_squad/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:42:18,030 >> Configuration saved in ./debug_squad/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:42:21,596 >> Model weights saved in ./debug_squad/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:42:21,599 >> tokenizer config file saved in ./debug_squad/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:42:21,603 >> Special tokens file saved in ./debug_squad/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.3347, 'learning_rate': 2.728887081469432e-05, 'epoch': 0.27}         \n",
      "  9%|███▎                                | 2000/22131 [14:36<2:21:33,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 12:45:59,951 >> Saving model checkpoint to ./debug_squad/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:45:59,955 >> Configuration saved in ./debug_squad/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:46:03,546 >> Model weights saved in ./debug_squad/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:46:03,550 >> tokenizer config file saved in ./debug_squad/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:46:03,585 >> Special tokens file saved in ./debug_squad/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2433, 'learning_rate': 2.66110885183679e-05, 'epoch': 0.34}          \n",
      " 11%|████                                | 2500/22131 [18:17<2:17:44,  2.38it/s][INFO|trainer.py:2340] 2022-05-23 12:49:41,629 >> Saving model checkpoint to ./debug_squad/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:49:41,637 >> Configuration saved in ./debug_squad/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:49:45,151 >> Model weights saved in ./debug_squad/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:49:45,155 >> tokenizer config file saved in ./debug_squad/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:49:45,158 >> Special tokens file saved in ./debug_squad/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.2017, 'learning_rate': 2.593330622204148e-05, 'epoch': 0.41}         \n",
      " 14%|████▉                               | 3000/22131 [21:59<2:14:23,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 12:53:23,158 >> Saving model checkpoint to ./debug_squad/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:53:23,162 >> Configuration saved in ./debug_squad/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:53:26,794 >> Model weights saved in ./debug_squad/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:53:26,826 >> tokenizer config file saved in ./debug_squad/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:53:26,830 >> Special tokens file saved in ./debug_squad/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1978, 'learning_rate': 2.525552392571506e-05, 'epoch': 0.47}         \n",
      " 16%|█████▋                              | 3500/22131 [25:41<2:11:29,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 12:57:05,036 >> Saving model checkpoint to ./debug_squad/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 12:57:05,041 >> Configuration saved in ./debug_squad/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 12:57:08,645 >> Model weights saved in ./debug_squad/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 12:57:08,648 >> tokenizer config file saved in ./debug_squad/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 12:57:08,687 >> Special tokens file saved in ./debug_squad/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1431, 'learning_rate': 2.4577741629388644e-05, 'epoch': 0.54}        \n",
      " 18%|██████▌                             | 4000/22131 [29:23<2:07:22,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:00:46,917 >> Saving model checkpoint to ./debug_squad/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:00:46,922 >> Configuration saved in ./debug_squad/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:00:50,695 >> Model weights saved in ./debug_squad/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:00:50,700 >> tokenizer config file saved in ./debug_squad/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:00:50,732 >> Special tokens file saved in ./debug_squad/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.09, 'learning_rate': 2.389995933306222e-05, 'epoch': 0.61}           \n",
      " 20%|███████▎                            | 4500/22131 [33:04<2:03:58,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:04:28,695 >> Saving model checkpoint to ./debug_squad/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:04:28,699 >> Configuration saved in ./debug_squad/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:04:32,208 >> Model weights saved in ./debug_squad/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:04:32,215 >> tokenizer config file saved in ./debug_squad/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:04:32,218 >> Special tokens file saved in ./debug_squad/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.0868, 'learning_rate': 2.3222177036735802e-05, 'epoch': 0.68}        \n",
      " 23%|████████▏                           | 5000/22131 [36:46<2:00:40,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:08:10,537 >> Saving model checkpoint to ./debug_squad/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:08:10,589 >> Configuration saved in ./debug_squad/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:08:14,317 >> Model weights saved in ./debug_squad/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:08:14,323 >> tokenizer config file saved in ./debug_squad/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:08:14,357 >> Special tokens file saved in ./debug_squad/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.1249, 'learning_rate': 2.2544394740409383e-05, 'epoch': 0.75}        \n",
      " 25%|████████▉                           | 5500/22131 [40:28<1:57:01,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:11:52,383 >> Saving model checkpoint to ./debug_squad/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:11:52,388 >> Configuration saved in ./debug_squad/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:11:56,221 >> Model weights saved in ./debug_squad/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:11:56,268 >> tokenizer config file saved in ./debug_squad/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:11:56,281 >> Special tokens file saved in ./debug_squad/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0844, 'learning_rate': 2.1866612444082963e-05, 'epoch': 0.81}        \n",
      " 27%|█████████▊                          | 6000/22131 [44:10<1:53:27,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:15:34,333 >> Saving model checkpoint to ./debug_squad/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:15:34,341 >> Configuration saved in ./debug_squad/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:15:37,766 >> Model weights saved in ./debug_squad/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:15:37,793 >> tokenizer config file saved in ./debug_squad/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:15:37,798 >> Special tokens file saved in ./debug_squad/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0404, 'learning_rate': 2.118883014775654e-05, 'epoch': 0.88}         \n",
      " 29%|██████████▌                         | 6500/22131 [47:51<1:50:08,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 13:19:15,704 >> Saving model checkpoint to ./debug_squad/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:19:15,710 >> Configuration saved in ./debug_squad/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:19:19,248 >> Model weights saved in ./debug_squad/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:19:19,252 >> tokenizer config file saved in ./debug_squad/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:19:19,255 >> Special tokens file saved in ./debug_squad/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0671, 'learning_rate': 2.051104785143012e-05, 'epoch': 0.95}         \n",
      " 32%|███████████▍                        | 7000/22131 [51:34<1:46:43,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:22:58,401 >> Saving model checkpoint to ./debug_squad/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:22:58,446 >> Configuration saved in ./debug_squad/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:23:02,292 >> Model weights saved in ./debug_squad/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:23:02,331 >> tokenizer config file saved in ./debug_squad/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:23:02,334 >> Special tokens file saved in ./debug_squad/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.9706, 'learning_rate': 1.9833265555103702e-05, 'epoch': 1.02}        \n",
      " 34%|████████████▏                       | 7500/22131 [55:17<1:43:22,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:26:41,054 >> Saving model checkpoint to ./debug_squad/checkpoint-7500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:26:41,059 >> Configuration saved in ./debug_squad/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:26:44,812 >> Model weights saved in ./debug_squad/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:26:44,862 >> tokenizer config file saved in ./debug_squad/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:26:44,867 >> Special tokens file saved in ./debug_squad/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.7624, 'learning_rate': 1.915548325877728e-05, 'epoch': 1.08}         \n",
      " 36%|█████████████                       | 8000/22131 [58:59<1:40:36,  2.34it/s][INFO|trainer.py:2340] 2022-05-23 13:30:23,744 >> Saving model checkpoint to ./debug_squad/checkpoint-8000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:30:23,750 >> Configuration saved in ./debug_squad/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:30:27,915 >> Model weights saved in ./debug_squad/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:30:27,919 >> tokenizer config file saved in ./debug_squad/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:30:27,922 >> Special tokens file saved in ./debug_squad/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.7255, 'learning_rate': 1.847770096245086e-05, 'epoch': 1.15}         \n",
      " 38%|█████████████                     | 8500/22131 [1:02:44<1:36:13,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:34:08,018 >> Saving model checkpoint to ./debug_squad/checkpoint-8500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:34:08,024 >> Configuration saved in ./debug_squad/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:34:11,855 >> Model weights saved in ./debug_squad/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:34:11,859 >> tokenizer config file saved in ./debug_squad/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:34:11,862 >> Special tokens file saved in ./debug_squad/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.7202, 'learning_rate': 1.779991866612444e-05, 'epoch': 1.22}         \n",
      " 41%|█████████████▊                    | 9000/22131 [1:06:26<1:32:46,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:37:50,774 >> Saving model checkpoint to ./debug_squad/checkpoint-9000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:37:50,780 >> Configuration saved in ./debug_squad/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:37:54,238 >> Model weights saved in ./debug_squad/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:37:54,242 >> tokenizer config file saved in ./debug_squad/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:37:54,246 >> Special tokens file saved in ./debug_squad/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.7635, 'learning_rate': 1.7122136369798022e-05, 'epoch': 1.29}        \n",
      " 43%|██████████████▌                   | 9500/22131 [1:10:09<1:29:10,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:41:33,105 >> Saving model checkpoint to ./debug_squad/checkpoint-9500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:41:33,185 >> Configuration saved in ./debug_squad/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:41:36,688 >> Model weights saved in ./debug_squad/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:41:36,707 >> tokenizer config file saved in ./debug_squad/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:41:36,712 >> Special tokens file saved in ./debug_squad/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.7495, 'learning_rate': 1.64443540734716e-05, 'epoch': 1.36}          \n",
      " 45%|██████████████▉                  | 10000/22131 [1:13:52<1:25:32,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:45:15,917 >> Saving model checkpoint to ./debug_squad/checkpoint-10000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:45:15,922 >> Configuration saved in ./debug_squad/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:45:19,692 >> Model weights saved in ./debug_squad/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:45:19,699 >> tokenizer config file saved in ./debug_squad/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:45:19,702 >> Special tokens file saved in ./debug_squad/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.7618, 'learning_rate': 1.576657177714518e-05, 'epoch': 1.42}         \n",
      " 47%|███████████████▋                 | 10500/22131 [1:17:34<1:22:10,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:48:58,808 >> Saving model checkpoint to ./debug_squad/checkpoint-10500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:48:58,813 >> Configuration saved in ./debug_squad/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:49:02,894 >> Model weights saved in ./debug_squad/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:49:02,898 >> tokenizer config file saved in ./debug_squad/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:49:02,900 >> Special tokens file saved in ./debug_squad/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.7394, 'learning_rate': 1.508878948081876e-05, 'epoch': 1.49}         \n",
      " 50%|████████████████▍                | 11000/22131 [1:21:18<1:18:27,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:52:42,166 >> Saving model checkpoint to ./debug_squad/checkpoint-11000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:52:42,171 >> Configuration saved in ./debug_squad/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:52:45,697 >> Model weights saved in ./debug_squad/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:52:45,730 >> tokenizer config file saved in ./debug_squad/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:52:45,733 >> Special tokens file saved in ./debug_squad/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.7348, 'learning_rate': 1.4411007184492342e-05, 'epoch': 1.56}        \n",
      " 52%|█████████████████▏               | 11500/22131 [1:25:00<1:15:10,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 13:56:24,814 >> Saving model checkpoint to ./debug_squad/checkpoint-11500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 13:56:24,819 >> Configuration saved in ./debug_squad/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 13:56:29,126 >> Model weights saved in ./debug_squad/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 13:56:29,131 >> tokenizer config file saved in ./debug_squad/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 13:56:29,187 >> Special tokens file saved in ./debug_squad/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.73, 'learning_rate': 1.373322488816592e-05, 'epoch': 1.63}           \n",
      " 54%|█████████████████▉               | 12000/22131 [1:28:44<1:11:27,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:00:08,196 >> Saving model checkpoint to ./debug_squad/checkpoint-12000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:00:08,200 >> Configuration saved in ./debug_squad/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:00:11,775 >> Model weights saved in ./debug_squad/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:00:11,780 >> tokenizer config file saved in ./debug_squad/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:00:11,785 >> Special tokens file saved in ./debug_squad/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.7155, 'learning_rate': 1.3055442591839502e-05, 'epoch': 1.69}        \n",
      " 56%|██████████████████▋              | 12500/22131 [1:32:27<1:07:56,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:03:50,897 >> Saving model checkpoint to ./debug_squad/checkpoint-12500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:03:50,902 >> Configuration saved in ./debug_squad/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:03:55,000 >> Model weights saved in ./debug_squad/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:03:55,004 >> tokenizer config file saved in ./debug_squad/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:03:55,008 >> Special tokens file saved in ./debug_squad/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.7374, 'learning_rate': 1.237766029551308e-05, 'epoch': 1.76}         \n",
      " 59%|███████████████████▍             | 13000/22131 [1:36:09<1:04:20,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 14:07:33,315 >> Saving model checkpoint to ./debug_squad/checkpoint-13000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:07:33,320 >> Configuration saved in ./debug_squad/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:07:37,193 >> Model weights saved in ./debug_squad/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:07:37,196 >> tokenizer config file saved in ./debug_squad/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:07:37,200 >> Special tokens file saved in ./debug_squad/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.73, 'learning_rate': 1.1699877999186661e-05, 'epoch': 1.83}          \n",
      " 61%|████████████████████▏            | 13500/22131 [1:39:51<1:00:51,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:11:15,669 >> Saving model checkpoint to ./debug_squad/checkpoint-13500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:11:15,674 >> Configuration saved in ./debug_squad/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:11:19,670 >> Model weights saved in ./debug_squad/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:11:19,675 >> tokenizer config file saved in ./debug_squad/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:11:19,677 >> Special tokens file saved in ./debug_squad/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.7248, 'learning_rate': 1.1022095702860242e-05, 'epoch': 1.9}         \n",
      " 63%|██████████████████████▏            | 14000/22131 [1:43:34<57:24,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:14:58,461 >> Saving model checkpoint to ./debug_squad/checkpoint-14000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:14:58,465 >> Configuration saved in ./debug_squad/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:15:02,143 >> Model weights saved in ./debug_squad/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:15:02,180 >> tokenizer config file saved in ./debug_squad/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:15:02,184 >> Special tokens file saved in ./debug_squad/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.7307, 'learning_rate': 1.0344313406533821e-05, 'epoch': 1.97}        \n",
      " 66%|██████████████████████▉            | 14500/22131 [1:47:17<53:52,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:18:41,067 >> Saving model checkpoint to ./debug_squad/checkpoint-14500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:18:41,072 >> Configuration saved in ./debug_squad/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:18:45,203 >> Model weights saved in ./debug_squad/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:18:45,208 >> tokenizer config file saved in ./debug_squad/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:18:45,254 >> Special tokens file saved in ./debug_squad/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 0.5859, 'learning_rate': 9.666531110207402e-06, 'epoch': 2.03}         \n",
      " 68%|███████████████████████▋           | 15000/22131 [1:51:00<50:17,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:22:24,843 >> Saving model checkpoint to ./debug_squad/checkpoint-15000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:22:24,848 >> Configuration saved in ./debug_squad/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:22:28,620 >> Model weights saved in ./debug_squad/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:22:28,635 >> tokenizer config file saved in ./debug_squad/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:22:28,638 >> Special tokens file saved in ./debug_squad/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 0.5034, 'learning_rate': 8.988748813880981e-06, 'epoch': 2.1}          \n",
      " 70%|████████████████████████▌          | 15500/22131 [1:54:43<46:42,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 14:26:07,756 >> Saving model checkpoint to ./debug_squad/checkpoint-15500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:26:07,761 >> Configuration saved in ./debug_squad/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:26:11,484 >> Model weights saved in ./debug_squad/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:26:11,487 >> tokenizer config file saved in ./debug_squad/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:26:11,492 >> Special tokens file saved in ./debug_squad/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 0.4753, 'learning_rate': 8.310966517554562e-06, 'epoch': 2.17}         \n",
      " 72%|█████████████████████████▎         | 16000/22131 [1:58:27<43:27,  2.35it/s][INFO|trainer.py:2340] 2022-05-23 14:29:50,892 >> Saving model checkpoint to ./debug_squad/checkpoint-16000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:29:50,896 >> Configuration saved in ./debug_squad/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:29:54,609 >> Model weights saved in ./debug_squad/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:29:54,661 >> tokenizer config file saved in ./debug_squad/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:29:54,668 >> Special tokens file saved in ./debug_squad/checkpoint-16000/special_tokens_map.json\n",
      "{'loss': 0.4738, 'learning_rate': 7.633184221228141e-06, 'epoch': 2.24}         \n",
      " 75%|██████████████████████████         | 16500/22131 [2:02:09<39:50,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:33:33,311 >> Saving model checkpoint to ./debug_squad/checkpoint-16500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:33:33,316 >> Configuration saved in ./debug_squad/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:33:37,325 >> Model weights saved in ./debug_squad/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:33:37,331 >> tokenizer config file saved in ./debug_squad/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:33:37,333 >> Special tokens file saved in ./debug_squad/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 0.4631, 'learning_rate': 6.955401924901722e-06, 'epoch': 2.3}          \n",
      " 77%|██████████████████████████▉        | 17000/22131 [2:05:52<36:17,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:37:16,468 >> Saving model checkpoint to ./debug_squad/checkpoint-17000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:37:16,474 >> Configuration saved in ./debug_squad/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:37:20,173 >> Model weights saved in ./debug_squad/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:37:20,179 >> tokenizer config file saved in ./debug_squad/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:37:20,210 >> Special tokens file saved in ./debug_squad/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 0.4852, 'learning_rate': 6.277619628575302e-06, 'epoch': 2.37}         \n",
      " 79%|███████████████████████████▋       | 17500/22131 [2:09:35<32:39,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:40:59,456 >> Saving model checkpoint to ./debug_squad/checkpoint-17500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:40:59,461 >> Configuration saved in ./debug_squad/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:41:03,260 >> Model weights saved in ./debug_squad/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:41:03,263 >> tokenizer config file saved in ./debug_squad/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:41:03,267 >> Special tokens file saved in ./debug_squad/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 0.4687, 'learning_rate': 5.5998373322488825e-06, 'epoch': 2.44}        \n",
      " 81%|████████████████████████████▍      | 18000/22131 [2:13:17<29:11,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:44:41,881 >> Saving model checkpoint to ./debug_squad/checkpoint-18000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:44:41,888 >> Configuration saved in ./debug_squad/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:44:45,825 >> Model weights saved in ./debug_squad/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:44:45,828 >> tokenizer config file saved in ./debug_squad/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:44:45,832 >> Special tokens file saved in ./debug_squad/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 0.5071, 'learning_rate': 4.9220550359224616e-06, 'epoch': 2.51}        \n",
      " 84%|█████████████████████████████▎     | 18500/22131 [2:17:01<25:38,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:48:24,974 >> Saving model checkpoint to ./debug_squad/checkpoint-18500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:48:24,979 >> Configuration saved in ./debug_squad/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:48:28,872 >> Model weights saved in ./debug_squad/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:48:28,917 >> tokenizer config file saved in ./debug_squad/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:48:28,922 >> Special tokens file saved in ./debug_squad/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 0.4718, 'learning_rate': 4.2442727395960415e-06, 'epoch': 2.58}        \n",
      " 86%|██████████████████████████████     | 19000/22131 [2:20:44<22:02,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 14:52:07,938 >> Saving model checkpoint to ./debug_squad/checkpoint-19000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:52:07,943 >> Configuration saved in ./debug_squad/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:52:11,741 >> Model weights saved in ./debug_squad/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:52:11,745 >> tokenizer config file saved in ./debug_squad/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:52:11,749 >> Special tokens file saved in ./debug_squad/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 0.4661, 'learning_rate': 3.566490443269622e-06, 'epoch': 2.64}         \n",
      " 88%|██████████████████████████████▊    | 19500/22131 [2:24:26<18:29,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 14:55:50,352 >> Saving model checkpoint to ./debug_squad/checkpoint-19500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:55:50,357 >> Configuration saved in ./debug_squad/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:55:54,222 >> Model weights saved in ./debug_squad/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:55:54,249 >> tokenizer config file saved in ./debug_squad/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:55:54,253 >> Special tokens file saved in ./debug_squad/checkpoint-19500/special_tokens_map.json\n",
      "{'loss': 0.4725, 'learning_rate': 2.8887081469432018e-06, 'epoch': 2.71}        \n",
      " 90%|███████████████████████████████▋   | 20000/22131 [2:28:08<15:01,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 14:59:32,627 >> Saving model checkpoint to ./debug_squad/checkpoint-20000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 14:59:32,633 >> Configuration saved in ./debug_squad/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 14:59:36,676 >> Model weights saved in ./debug_squad/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 14:59:36,681 >> tokenizer config file saved in ./debug_squad/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 14:59:36,686 >> Special tokens file saved in ./debug_squad/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.4547, 'learning_rate': 2.210925850616782e-06, 'epoch': 2.78}         \n",
      " 93%|████████████████████████████████▍  | 20500/22131 [2:31:51<11:29,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 15:03:14,910 >> Saving model checkpoint to ./debug_squad/checkpoint-20500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 15:03:14,914 >> Configuration saved in ./debug_squad/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 15:03:19,000 >> Model weights saved in ./debug_squad/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 15:03:19,006 >> tokenizer config file saved in ./debug_squad/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 15:03:19,010 >> Special tokens file saved in ./debug_squad/checkpoint-20500/special_tokens_map.json\n",
      "{'loss': 0.4729, 'learning_rate': 1.533143554290362e-06, 'epoch': 2.85}         \n",
      " 95%|█████████████████████████████████▏ | 21000/22131 [2:35:33<07:57,  2.37it/s][INFO|trainer.py:2340] 2022-05-23 15:06:57,279 >> Saving model checkpoint to ./debug_squad/checkpoint-21000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 15:06:57,284 >> Configuration saved in ./debug_squad/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 15:07:01,065 >> Model weights saved in ./debug_squad/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 15:07:01,118 >> tokenizer config file saved in ./debug_squad/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 15:07:01,142 >> Special tokens file saved in ./debug_squad/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 0.4954, 'learning_rate': 8.55361257963942e-07, 'epoch': 2.91}          \n",
      " 97%|██████████████████████████████████ | 21500/22131 [2:39:16<04:26,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 15:10:40,042 >> Saving model checkpoint to ./debug_squad/checkpoint-21500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 15:10:40,047 >> Configuration saved in ./debug_squad/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 15:10:43,787 >> Model weights saved in ./debug_squad/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 15:10:43,822 >> tokenizer config file saved in ./debug_squad/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 15:10:43,827 >> Special tokens file saved in ./debug_squad/checkpoint-21500/special_tokens_map.json\n",
      "{'loss': 0.471, 'learning_rate': 1.7757896163752204e-07, 'epoch': 2.98}         \n",
      " 99%|██████████████████████████████████▊| 22000/22131 [2:42:58<00:55,  2.36it/s][INFO|trainer.py:2340] 2022-05-23 15:14:22,279 >> Saving model checkpoint to ./debug_squad/checkpoint-22000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 15:14:22,284 >> Configuration saved in ./debug_squad/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 15:14:26,000 >> Model weights saved in ./debug_squad/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 15:14:26,057 >> tokenizer config file saved in ./debug_squad/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 15:14:26,062 >> Special tokens file saved in ./debug_squad/checkpoint-22000/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 22131/22131 [2:44:03<00:00,  2.37it/s][INFO|trainer.py:1662] 2022-05-23 15:15:27,779 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 9843.9653, 'train_samples_per_second': 26.978, 'train_steps_per_second': 2.248, 'train_loss': 0.8274556010399987, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████| 22131/22131 [2:44:04<00:00,  2.25it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-23 15:15:27,968 >> Saving model checkpoint to ./debug_squad/\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 15:15:28,014 >> Configuration saved in ./debug_squad/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 15:15:31,667 >> Model weights saved in ./debug_squad/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 15:15:31,900 >> tokenizer config file saved in ./debug_squad/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 15:15:31,905 >> Special tokens file saved in ./debug_squad/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     0.8275\n",
      "  train_runtime            = 2:44:03.96\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     26.978\n",
      "  train_steps_per_second   =      2.248\n",
      "05/23/2022 15:15:32 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-23 15:15:32,033 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-23 15:15:32,038 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-23 15:15:32,038 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2595] 2022-05-23 15:15:32,038 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:13<00:00, 10.09it/s]05/23/2022 15:17:55 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 44/10570 [00:00<00:24, 430.78it/s]\u001b[A\n",
      "  1%|▎                                      | 91/10570 [00:00<00:23, 453.09it/s]\u001b[A\n",
      "  1%|▌                                     | 140/10570 [00:00<00:22, 466.50it/s]\u001b[A\n",
      "  2%|▋                                     | 187/10570 [00:00<00:22, 463.43it/s]\u001b[A\n",
      "  2%|▊                                     | 234/10570 [00:00<00:23, 436.80it/s]\u001b[A\n",
      "  3%|▉                                     | 278/10570 [00:00<00:28, 362.66it/s]\u001b[A\n",
      "  3%|█▏                                    | 320/10570 [00:00<00:27, 377.67it/s]\u001b[A\n",
      "  3%|█▎                                    | 366/10570 [00:00<00:25, 398.31it/s]\u001b[A\n",
      "  4%|█▍                                    | 410/10570 [00:00<00:24, 408.82it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:24<00:00, 10.09it/s]\u001b[A\n",
      "  5%|█▊                                    | 503/10570 [00:01<00:23, 435.60it/s]\u001b[A\n",
      "  5%|█▉                                    | 549/10570 [00:01<00:22, 441.07it/s]\u001b[A\n",
      "  6%|██▏                                   | 594/10570 [00:01<00:22, 441.18it/s]\u001b[A\n",
      "  6%|██▎                                   | 641/10570 [00:01<00:22, 449.18it/s]\u001b[A\n",
      "  7%|██▍                                   | 688/10570 [00:01<00:21, 454.81it/s]\u001b[A\n",
      "  7%|██▋                                   | 734/10570 [00:01<00:22, 445.30it/s]\u001b[A\n",
      "  7%|██▊                                   | 779/10570 [00:01<00:22, 437.08it/s]\u001b[A\n",
      "  8%|██▉                                   | 823/10570 [00:01<00:23, 420.11it/s]\u001b[A\n",
      "  8%|███                                   | 867/10570 [00:02<00:22, 424.87it/s]\u001b[A\n",
      "  9%|███▎                                  | 911/10570 [00:02<00:22, 427.38it/s]\u001b[A\n",
      "  9%|███▍                                  | 954/10570 [00:02<00:22, 427.20it/s]\u001b[A\n",
      "  9%|███▌                                  | 997/10570 [00:02<00:22, 420.97it/s]\u001b[A\n",
      " 10%|███▋                                 | 1040/10570 [00:02<00:22, 414.63it/s]\u001b[A\n",
      " 10%|███▊                                 | 1083/10570 [00:02<00:22, 418.59it/s]\u001b[A\n",
      " 11%|███▉                                 | 1125/10570 [00:02<00:22, 412.82it/s]\u001b[A\n",
      " 11%|████                                 | 1173/10570 [00:02<00:21, 431.22it/s]\u001b[A\n",
      " 12%|████▎                                | 1219/10570 [00:02<00:21, 437.18it/s]\u001b[A\n",
      " 12%|████▍                                | 1263/10570 [00:02<00:21, 436.15it/s]\u001b[A\n",
      " 12%|████▌                                | 1309/10570 [00:03<00:20, 441.40it/s]\u001b[A\n",
      " 13%|████▋                                | 1355/10570 [00:03<00:20, 444.63it/s]\u001b[A\n",
      " 13%|████▉                                | 1400/10570 [00:03<00:20, 439.58it/s]\u001b[A\n",
      " 14%|█████                                | 1445/10570 [00:03<00:20, 440.70it/s]\u001b[A\n",
      " 14%|█████▏                               | 1490/10570 [00:03<00:20, 436.81it/s]\u001b[A\n",
      " 15%|█████▍                               | 1536/10570 [00:03<00:20, 442.09it/s]\u001b[A\n",
      " 15%|█████▌                               | 1581/10570 [00:03<00:20, 442.59it/s]\u001b[A\n",
      " 15%|█████▋                               | 1627/10570 [00:03<00:20, 445.68it/s]\u001b[A\n",
      " 16%|█████▊                               | 1672/10570 [00:03<00:19, 446.78it/s]\u001b[A\n",
      " 16%|██████                               | 1717/10570 [00:03<00:20, 440.86it/s]\u001b[A\n",
      " 17%|██████▏                              | 1762/10570 [00:04<00:20, 436.13it/s]\u001b[A\n",
      " 17%|██████▎                              | 1808/10570 [00:04<00:19, 440.44it/s]\u001b[A\n",
      " 18%|██████▍                              | 1853/10570 [00:04<00:19, 436.05it/s]\u001b[A\n",
      " 18%|██████▋                              | 1899/10570 [00:04<00:19, 441.91it/s]\u001b[A\n",
      " 18%|██████▊                              | 1944/10570 [00:04<00:19, 440.84it/s]\u001b[A\n",
      " 19%|██████▉                              | 1992/10570 [00:04<00:19, 449.46it/s]\u001b[A\n",
      " 19%|███████▏                             | 2037/10570 [00:04<00:19, 445.93it/s]\u001b[A\n",
      " 20%|███████▎                             | 2085/10570 [00:04<00:18, 454.20it/s]\u001b[A\n",
      " 20%|███████▍                             | 2131/10570 [00:04<00:20, 419.75it/s]\u001b[A\n",
      " 21%|███████▌                             | 2175/10570 [00:05<00:19, 425.22it/s]\u001b[A\n",
      " 21%|███████▊                             | 2221/10570 [00:05<00:19, 433.58it/s]\u001b[A\n",
      " 21%|███████▉                             | 2265/10570 [00:05<00:19, 429.44it/s]\u001b[A\n",
      " 22%|████████                             | 2310/10570 [00:05<00:19, 433.37it/s]\u001b[A\n",
      " 22%|████████▏                            | 2354/10570 [00:05<00:19, 429.65it/s]\u001b[A\n",
      " 23%|████████▍                            | 2398/10570 [00:05<00:19, 426.95it/s]\u001b[A\n",
      " 23%|████████▌                            | 2442/10570 [00:05<00:18, 430.48it/s]\u001b[A\n",
      " 24%|████████▋                            | 2486/10570 [00:05<00:18, 432.25it/s]\u001b[A\n",
      " 24%|████████▊                            | 2530/10570 [00:05<00:19, 415.68it/s]\u001b[A\n",
      " 24%|█████████                            | 2572/10570 [00:05<00:19, 402.76it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2617/10570 [00:06<00:19, 413.60it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2663/10570 [00:06<00:18, 423.94it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2707/10570 [00:06<00:18, 427.88it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2752/10570 [00:06<00:18, 431.69it/s]\u001b[A\n",
      " 26%|█████████▊                           | 2800/10570 [00:06<00:17, 444.24it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2846/10570 [00:06<00:17, 446.43it/s]\u001b[A\n",
      " 27%|██████████                           | 2891/10570 [00:06<00:17, 437.28it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2935/10570 [00:06<00:17, 437.00it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2979/10570 [00:06<00:17, 434.33it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3023/10570 [00:07<00:17, 433.22it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3067/10570 [00:07<00:17, 432.45it/s]\u001b[A\n",
      " 29%|██████████▉                          | 3111/10570 [00:07<00:17, 430.33it/s]\u001b[A\n",
      " 30%|███████████                          | 3155/10570 [00:07<00:18, 411.57it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3197/10570 [00:07<00:18, 397.82it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3237/10570 [00:07<00:18, 390.92it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3280/10570 [00:07<00:18, 400.98it/s]\u001b[A\n",
      " 31%|███████████▋                         | 3323/10570 [00:07<00:17, 407.35it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3367/10570 [00:07<00:17, 414.44it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3412/10570 [00:07<00:16, 422.85it/s]\u001b[A\n",
      " 33%|████████████                         | 3456/10570 [00:08<00:16, 427.01it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3499/10570 [00:08<00:16, 427.53it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3543/10570 [00:08<00:16, 429.69it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3587/10570 [00:08<00:17, 401.84it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3628/10570 [00:08<00:17, 403.24it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3671/10570 [00:08<00:16, 409.08it/s]\u001b[A\n",
      " 35%|█████████████                        | 3715/10570 [00:08<00:16, 415.63it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3759/10570 [00:08<00:16, 421.41it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3803/10570 [00:08<00:15, 424.52it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3846/10570 [00:08<00:15, 424.58it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3889/10570 [00:09<00:15, 422.70it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3933/10570 [00:09<00:15, 425.98it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3976/10570 [00:09<00:15, 416.18it/s]\u001b[A\n",
      " 38%|██████████████                       | 4018/10570 [00:09<00:15, 414.79it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4061/10570 [00:09<00:15, 417.64it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4103/10570 [00:09<00:15, 416.45it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4145/10570 [00:09<00:16, 391.39it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4185/10570 [00:09<00:18, 336.27it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4221/10570 [00:10<00:19, 317.82it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4259/10570 [00:10<00:18, 332.78it/s]\u001b[A\n",
      " 41%|███████████████                      | 4294/10570 [00:10<00:25, 250.96it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4323/10570 [00:10<00:24, 251.24it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4363/10570 [00:10<00:21, 285.67it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4405/10570 [00:10<00:19, 318.30it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4449/10570 [00:10<00:17, 349.56it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4494/10570 [00:10<00:16, 375.31it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4534/10570 [00:10<00:15, 380.23it/s]\u001b[A\n",
      " 43%|████████████████                     | 4575/10570 [00:11<00:15, 386.99it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4615/10570 [00:11<00:16, 364.67it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4653/10570 [00:11<00:16, 351.19it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4690/10570 [00:11<00:16, 356.07it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4733/10570 [00:11<00:15, 376.81it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4772/10570 [00:11<00:15, 369.01it/s]\u001b[A\n",
      " 46%|████████████████▊                    | 4814/10570 [00:11<00:15, 382.42it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4853/10570 [00:11<00:15, 378.51it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4895/10570 [00:11<00:14, 389.89it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4939/10570 [00:12<00:13, 403.58it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4982/10570 [00:12<00:13, 408.80it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 5024/10570 [00:12<00:13, 406.55it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5069/10570 [00:12<00:13, 417.61it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5111/10570 [00:12<00:13, 403.98it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5154/10570 [00:12<00:13, 411.20it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5197/10570 [00:12<00:12, 415.22it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5241/10570 [00:12<00:12, 420.32it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5286/10570 [00:12<00:12, 427.15it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 5332/10570 [00:12<00:12, 434.91it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5376/10570 [00:13<00:12, 431.18it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5420/10570 [00:13<00:12, 426.42it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5463/10570 [00:13<00:12, 398.24it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5504/10570 [00:13<00:13, 380.43it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 5543/10570 [00:13<00:13, 379.11it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5584/10570 [00:13<00:12, 386.00it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5625/10570 [00:13<00:12, 391.23it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5665/10570 [00:13<00:12, 380.15it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5706/10570 [00:13<00:12, 387.14it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5749/10570 [00:14<00:12, 397.45it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5791/10570 [00:14<00:11, 403.22it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5834/10570 [00:14<00:11, 410.60it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5877/10570 [00:14<00:11, 415.89it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5919/10570 [00:14<00:11, 416.50it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5963/10570 [00:14<00:10, 420.84it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6006/10570 [00:14<00:10, 422.13it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6049/10570 [00:14<00:10, 421.64it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6092/10570 [00:14<00:10, 411.11it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6137/10570 [00:14<00:10, 420.20it/s]\u001b[A\n",
      " 58%|█████████████████████▋               | 6181/10570 [00:15<00:10, 423.29it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6224/10570 [00:15<00:10, 413.94it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6266/10570 [00:15<00:10, 393.57it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6308/10570 [00:15<00:10, 400.10it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6350/10570 [00:15<00:10, 405.14it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6391/10570 [00:15<00:10, 393.77it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6435/10570 [00:15<00:10, 406.51it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6479/10570 [00:15<00:09, 415.92it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6521/10570 [00:15<00:09, 416.28it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6564/10570 [00:16<00:09, 418.05it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6606/10570 [00:16<00:09, 411.30it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6651/10570 [00:16<00:09, 420.33it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6694/10570 [00:16<00:09, 419.04it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6736/10570 [00:16<00:09, 416.19it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6779/10570 [00:16<00:09, 419.01it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6821/10570 [00:16<00:09, 412.23it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6863/10570 [00:16<00:09, 393.51it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6903/10570 [00:16<00:09, 391.79it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6946/10570 [00:16<00:09, 401.25it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6989/10570 [00:17<00:08, 408.68it/s]\u001b[A\n",
      " 67%|████████████████████████▌            | 7034/10570 [00:17<00:08, 418.51it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7076/10570 [00:17<00:08, 417.65it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7121/10570 [00:17<00:08, 425.86it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7164/10570 [00:17<00:07, 426.67it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7207/10570 [00:17<00:07, 421.13it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7250/10570 [00:17<00:07, 420.66it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7293/10570 [00:17<00:08, 407.89it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7334/10570 [00:17<00:08, 392.42it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7374/10570 [00:18<00:08, 379.19it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7416/10570 [00:18<00:08, 389.45it/s]\u001b[A\n",
      " 71%|██████████████████████████           | 7459/10570 [00:18<00:07, 397.70it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7501/10570 [00:18<00:07, 401.79it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7545/10570 [00:18<00:07, 412.20it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7590/10570 [00:18<00:07, 420.88it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7634/10570 [00:18<00:06, 426.16it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7677/10570 [00:18<00:07, 413.00it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7720/10570 [00:18<00:06, 415.45it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7762/10570 [00:18<00:06, 406.44it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7805/10570 [00:19<00:06, 411.38it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7848/10570 [00:19<00:06, 414.17it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7891/10570 [00:19<00:06, 417.78it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7934/10570 [00:19<00:06, 421.09it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7977/10570 [00:19<00:06, 413.29it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8020/10570 [00:19<00:06, 415.97it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8062/10570 [00:19<00:06, 413.31it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8108/10570 [00:19<00:05, 424.80it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8151/10570 [00:19<00:05, 416.14it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8193/10570 [00:19<00:05, 406.62it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8234/10570 [00:20<00:05, 405.45it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8275/10570 [00:20<00:05, 395.11it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8315/10570 [00:20<00:05, 390.80it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8355/10570 [00:20<00:05, 391.32it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8398/10570 [00:20<00:05, 401.37it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8441/10570 [00:20<00:05, 408.43it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8485/10570 [00:20<00:05, 415.81it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8528/10570 [00:20<00:04, 419.09it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8571/10570 [00:20<00:04, 420.13it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8615/10570 [00:20<00:04, 423.91it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8658/10570 [00:21<00:04, 420.48it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8702/10570 [00:21<00:04, 426.20it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8745/10570 [00:21<00:04, 422.47it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8788/10570 [00:21<00:04, 413.94it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8830/10570 [00:21<00:04, 414.49it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8873/10570 [00:21<00:04, 417.78it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8916/10570 [00:21<00:03, 419.93it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8959/10570 [00:21<00:03, 422.50it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9002/10570 [00:21<00:03, 421.16it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9045/10570 [00:22<00:03, 417.49it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9087/10570 [00:22<00:03, 405.33it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9130/10570 [00:22<00:03, 410.15it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9172/10570 [00:22<00:03, 409.28it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9213/10570 [00:22<00:03, 407.41it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9256/10570 [00:22<00:03, 411.56it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9298/10570 [00:22<00:03, 412.55it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9340/10570 [00:22<00:02, 414.04it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9382/10570 [00:22<00:02, 411.30it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9424/10570 [00:22<00:02, 413.67it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9468/10570 [00:23<00:02, 421.41it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9511/10570 [00:23<00:02, 422.38it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9556/10570 [00:23<00:02, 429.38it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9599/10570 [00:23<00:02, 425.88it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9643/10570 [00:23<00:02, 428.20it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9688/10570 [00:23<00:02, 432.45it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9734/10570 [00:23<00:01, 439.10it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▏  | 9779/10570 [00:23<00:01, 440.85it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9824/10570 [00:23<00:01, 434.54it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9868/10570 [00:23<00:01, 421.00it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9911/10570 [00:24<00:01, 412.25it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9957/10570 [00:24<00:01, 424.01it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10000/10570 [00:24<00:01, 422.09it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10045/10570 [00:24<00:01, 428.95it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10089/10570 [00:24<00:01, 429.65it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10134/10570 [00:24<00:01, 434.31it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10178/10570 [00:24<00:00, 423.88it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10223/10570 [00:24<00:00, 429.28it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10266/10570 [00:24<00:00, 422.23it/s]\u001b[A\n",
      " 98%|███████████████████████████████████ | 10309/10570 [00:25<00:00, 423.15it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10352/10570 [00:25<00:00, 406.45it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10393/10570 [00:25<00:00, 398.03it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10433/10570 [00:25<00:00, 398.11it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10473/10570 [00:25<00:00, 389.45it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10517/10570 [00:25<00:00, 402.98it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:25<00:00, 409.74it/s]\u001b[A\n",
      "05/23/2022 15:18:21 - INFO - utils_qa - Saving predictions to ./debug_squad/eval_predictions.json.\n",
      "05/23/2022 15:18:21 - INFO - utils_qa - Saving nbest_preds to ./debug_squad/eval_nbest_predictions.json.\n",
      "05/23/2022 15:18:26 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:54<00:00,  7.74it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     3.0\n",
      "  eval_exact_match = 80.9839\n",
      "  eval_f1          = 88.5023\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

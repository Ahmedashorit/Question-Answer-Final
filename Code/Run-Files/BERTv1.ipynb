{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66609424-a597-4a7d-a4cd-bee4f2b3be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9704d9e3-e465-470d-9458-f5e7e930bfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==1.9.0 in /opt/conda/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e73a45b-94a1-4d7c-93cf-95a5910f41ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19265b6c-9561-44ba-b8d3-7bff6c8ebdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/29/2022 13:26:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/29/2022 13:26:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./debug_squad/runs/Apr29_13-26-34_n074trduss,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./debug_squad/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./debug_squad/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/29/2022 13:26:34 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\n",
      "04/29/2022 13:26:34 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "04/29/2022 13:26:34 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "04/29/2022 13:26:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "04/29/2022 13:26:34 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "04/29/2022 13:26:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 386.71it/s]\n",
      "[INFO|configuration_utils.py:654] 2022-04-29 13:26:34,939 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-04-29 13:26:34,940 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-29 13:26:35,228 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-04-29 13:26:35,229 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-29 13:26:35,397 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp5fi447r2\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 6.66MB/s]\n",
      "[INFO|hub.py:587] 2022-04-29 13:26:35,529 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:595] 2022-04-29 13:26:35,529 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:583] 2022-04-29 13:26:35,606 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt55pl9bq\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 10.3MB/s]\n",
      "[INFO|hub.py:587] 2022-04-29 13:26:35,796 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|hub.py:595] 2022-04-29 13:26:35,796 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-29 13:26:36,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-29 13:26:36,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-29 13:26:36,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-29 13:26:36,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-29 13:26:36,078 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:654] 2022-04-29 13:26:36,215 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-04-29 13:26:36,216 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-29 13:26:36,395 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpl39fphci\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [00:18<00:00, 23.5MB/s]\n",
      "[INFO|hub.py:587] 2022-04-29 13:26:55,239 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|hub.py:595] 2022-04-29 13:26:55,239 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1772] 2022-04-29 13:26:55,239 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2048] 2022-04-29 13:26:57,095 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2059] 2022-04-29 13:26:57,095 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]04/29/2022 13:26:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-267e2b6a8475ec93.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:47<00:00,  1.85ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]04/29/2022 13:27:45 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-034712f7dd307c41.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:38<00:00,  3.51s/ba]\n",
      "04/29/2022 13:28:23 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpwh_rvdyh\n",
      "Downloading builder script: 4.50kB [00:00, 2.04MB/s]                            \n",
      "04/29/2022 13:28:23 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/29/2022 13:28:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/29/2022 13:28:24 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpkg2ug8k9\n",
      "Downloading extra modules: 3.31kB [00:00, 2.16MB/s]                             \n",
      "04/29/2022 13:28:24 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "04/29/2022 13:28:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1290] 2022-04-29 13:28:28,895 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-29 13:28:28,895 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1292] 2022-04-29 13:28:28,895 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1293] 2022-04-29 13:28:28,895 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1294] 2022-04-29 13:28:28,895 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1295] 2022-04-29 13:28:28,895 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-29 13:28:28,895 >>   Total optimization steps = 14754\n",
      "{'loss': 2.4073, 'learning_rate': 2.8983326555510372e-05, 'epoch': 0.07}        \n",
      "  3%|█▎                                   | 500/14754 [05:29<2:38:49,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 13:33:58,189 >> Saving model checkpoint to ./debug_squad/checkpoint-500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 13:33:58,195 >> Configuration saved in ./debug_squad/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 13:35:34,361 >> Model weights saved in ./debug_squad/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 13:35:34,375 >> tokenizer config file saved in ./debug_squad/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 13:35:34,380 >> Special tokens file saved in ./debug_squad/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.5, 'learning_rate': 2.796665311102074e-05, 'epoch': 0.14}            \n",
      "  7%|██▍                                 | 1000/14754 [15:18<2:33:20,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 13:43:47,334 >> Saving model checkpoint to ./debug_squad/checkpoint-1000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 13:43:47,359 >> Configuration saved in ./debug_squad/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 13:44:57,771 >> Model weights saved in ./debug_squad/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 13:44:57,779 >> tokenizer config file saved in ./debug_squad/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 13:44:57,795 >> Special tokens file saved in ./debug_squad/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3903, 'learning_rate': 2.694997966653111e-05, 'epoch': 0.2}          \n",
      " 10%|███▋                                | 1500/14754 [24:10<2:28:11,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 13:52:39,498 >> Saving model checkpoint to ./debug_squad/checkpoint-1500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 13:52:39,504 >> Configuration saved in ./debug_squad/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 13:54:11,814 >> Model weights saved in ./debug_squad/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 13:54:11,831 >> tokenizer config file saved in ./debug_squad/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 13:54:11,840 >> Special tokens file saved in ./debug_squad/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.3209, 'learning_rate': 2.593330622204148e-05, 'epoch': 0.27}         \n",
      " 14%|████▉                               | 2000/14754 [33:40<2:21:44,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 14:02:09,721 >> Saving model checkpoint to ./debug_squad/checkpoint-2000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:02:09,732 >> Configuration saved in ./debug_squad/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:03:26,084 >> Model weights saved in ./debug_squad/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:03:26,094 >> tokenizer config file saved in ./debug_squad/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:03:26,117 >> Special tokens file saved in ./debug_squad/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2482, 'learning_rate': 2.491663277755185e-05, 'epoch': 0.34}         \n",
      " 17%|██████                              | 2500/14754 [44:19<2:16:44,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 14:12:48,307 >> Saving model checkpoint to ./debug_squad/checkpoint-2500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:12:48,331 >> Configuration saved in ./debug_squad/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:14:39,812 >> Model weights saved in ./debug_squad/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:14:39,831 >> tokenizer config file saved in ./debug_squad/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:14:39,860 >> Special tokens file saved in ./debug_squad/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1931, 'learning_rate': 2.389995933306222e-05, 'epoch': 0.41}         \n",
      " 20%|███████▎                            | 3000/14754 [56:48<2:10:34,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 14:25:17,653 >> Saving model checkpoint to ./debug_squad/checkpoint-3000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:25:17,690 >> Configuration saved in ./debug_squad/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:26:30,865 >> Model weights saved in ./debug_squad/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:26:30,870 >> tokenizer config file saved in ./debug_squad/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:26:30,897 >> Special tokens file saved in ./debug_squad/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1865, 'learning_rate': 2.288328588857259e-05, 'epoch': 0.47}         \n",
      " 24%|████████                          | 3500/14754 [1:04:53<2:05:39,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 14:33:22,990 >> Saving model checkpoint to ./debug_squad/checkpoint-3500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:33:23,003 >> Configuration saved in ./debug_squad/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:34:47,268 >> Model weights saved in ./debug_squad/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:34:47,274 >> tokenizer config file saved in ./debug_squad/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:34:47,299 >> Special tokens file saved in ./debug_squad/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1186, 'learning_rate': 2.1866612444082963e-05, 'epoch': 0.54}        \n",
      " 27%|█████████▏                        | 4000/14754 [1:16:26<2:00:28,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 14:44:54,993 >> Saving model checkpoint to ./debug_squad/checkpoint-4000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:44:55,000 >> Configuration saved in ./debug_squad/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:47:20,266 >> Model weights saved in ./debug_squad/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:47:20,276 >> tokenizer config file saved in ./debug_squad/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:47:20,294 >> Special tokens file saved in ./debug_squad/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0749, 'learning_rate': 2.084993899959333e-05, 'epoch': 0.61}         \n",
      " 31%|██████████▎                       | 4500/14754 [1:29:13<1:53:46,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 14:57:42,039 >> Saving model checkpoint to ./debug_squad/checkpoint-4500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 14:57:42,063 >> Configuration saved in ./debug_squad/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 14:58:38,161 >> Model weights saved in ./debug_squad/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 14:58:38,170 >> tokenizer config file saved in ./debug_squad/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 14:58:38,175 >> Special tokens file saved in ./debug_squad/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.0861, 'learning_rate': 1.9833265555103702e-05, 'epoch': 0.68}        \n",
      " 34%|███████████▌                      | 5000/14754 [1:38:02<1:48:58,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 15:06:31,543 >> Saving model checkpoint to ./debug_squad/checkpoint-5000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:06:31,551 >> Configuration saved in ./debug_squad/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:07:09,827 >> Model weights saved in ./debug_squad/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:07:09,850 >> tokenizer config file saved in ./debug_squad/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:07:09,857 >> Special tokens file saved in ./debug_squad/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.1132, 'learning_rate': 1.8816592110614073e-05, 'epoch': 0.75}        \n",
      " 37%|████████████▋                     | 5500/14754 [1:45:25<1:43:12,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 15:13:54,526 >> Saving model checkpoint to ./debug_squad/checkpoint-5500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:13:54,531 >> Configuration saved in ./debug_squad/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:14:36,461 >> Model weights saved in ./debug_squad/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:14:36,480 >> tokenizer config file saved in ./debug_squad/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:14:36,483 >> Special tokens file saved in ./debug_squad/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0727, 'learning_rate': 1.779991866612444e-05, 'epoch': 0.81}         \n",
      " 41%|█████████████▊                    | 6000/14754 [1:53:09<1:37:30,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 15:21:38,922 >> Saving model checkpoint to ./debug_squad/checkpoint-6000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:21:38,934 >> Configuration saved in ./debug_squad/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:22:18,051 >> Model weights saved in ./debug_squad/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:22:18,060 >> tokenizer config file saved in ./debug_squad/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:22:18,074 >> Special tokens file saved in ./debug_squad/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0314, 'learning_rate': 1.6783245221634812e-05, 'epoch': 0.88}        \n",
      " 44%|██████████████▉                   | 6500/14754 [2:00:30<1:32:02,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 15:28:59,413 >> Saving model checkpoint to ./debug_squad/checkpoint-6500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:28:59,419 >> Configuration saved in ./debug_squad/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:29:35,076 >> Model weights saved in ./debug_squad/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:29:35,098 >> tokenizer config file saved in ./debug_squad/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:29:35,100 >> Special tokens file saved in ./debug_squad/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0472, 'learning_rate': 1.576657177714518e-05, 'epoch': 0.95}         \n",
      " 47%|████████████████▏                 | 7000/14754 [2:07:46<1:26:24,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 15:36:15,689 >> Saving model checkpoint to ./debug_squad/checkpoint-7000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:36:15,697 >> Configuration saved in ./debug_squad/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:36:51,482 >> Model weights saved in ./debug_squad/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:36:51,488 >> tokenizer config file saved in ./debug_squad/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:36:51,508 >> Special tokens file saved in ./debug_squad/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.9536, 'learning_rate': 1.4749898332655551e-05, 'epoch': 1.02}        \n",
      " 51%|█████████████████▎                | 7500/14754 [2:15:05<1:20:59,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 15:43:34,180 >> Saving model checkpoint to ./debug_squad/checkpoint-7500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:43:34,209 >> Configuration saved in ./debug_squad/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:44:09,675 >> Model weights saved in ./debug_squad/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:44:09,680 >> tokenizer config file saved in ./debug_squad/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:44:09,705 >> Special tokens file saved in ./debug_squad/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.7505, 'learning_rate': 1.373322488816592e-05, 'epoch': 1.08}         \n",
      " 54%|██████████████████▍               | 8000/14754 [2:22:21<1:15:27,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 15:50:50,133 >> Saving model checkpoint to ./debug_squad/checkpoint-8000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:50:50,139 >> Configuration saved in ./debug_squad/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:51:25,971 >> Model weights saved in ./debug_squad/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:51:26,000 >> tokenizer config file saved in ./debug_squad/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:51:26,011 >> Special tokens file saved in ./debug_squad/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.732, 'learning_rate': 1.271655144367629e-05, 'epoch': 1.15}          \n",
      " 58%|███████████████████▌              | 8500/14754 [2:29:38<1:09:40,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 15:58:07,911 >> Saving model checkpoint to ./debug_squad/checkpoint-8500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 15:58:07,918 >> Configuration saved in ./debug_squad/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 15:58:47,522 >> Model weights saved in ./debug_squad/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 15:58:47,529 >> tokenizer config file saved in ./debug_squad/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 15:58:47,539 >> Special tokens file saved in ./debug_squad/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.7208, 'learning_rate': 1.1699877999186661e-05, 'epoch': 1.22}        \n",
      " 61%|████████████████████▋             | 9000/14754 [2:37:02<1:04:10,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:05:31,854 >> Saving model checkpoint to ./debug_squad/checkpoint-9000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:05:31,861 >> Configuration saved in ./debug_squad/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:06:08,235 >> Model weights saved in ./debug_squad/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:06:08,242 >> tokenizer config file saved in ./debug_squad/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:06:08,259 >> Special tokens file saved in ./debug_squad/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.7448, 'learning_rate': 1.0683204554697033e-05, 'epoch': 1.29}        \n",
      " 64%|███████████████████████▏            | 9500/14754 [2:44:49<58:41,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:13:18,187 >> Saving model checkpoint to ./debug_squad/checkpoint-9500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:13:18,212 >> Configuration saved in ./debug_squad/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:13:54,156 >> Model weights saved in ./debug_squad/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:13:54,160 >> tokenizer config file saved in ./debug_squad/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:13:54,182 >> Special tokens file saved in ./debug_squad/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.7271, 'learning_rate': 9.666531110207402e-06, 'epoch': 1.36}         \n",
      " 68%|███████████████████████▋           | 10000/14754 [2:52:07<53:00,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:20:36,145 >> Saving model checkpoint to ./debug_squad/checkpoint-10000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:20:36,171 >> Configuration saved in ./debug_squad/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:21:10,440 >> Model weights saved in ./debug_squad/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:21:10,444 >> tokenizer config file saved in ./debug_squad/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:21:10,467 >> Special tokens file saved in ./debug_squad/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.7464, 'learning_rate': 8.649857665717772e-06, 'epoch': 1.42}         \n",
      " 71%|████████████████████████▉          | 10500/14754 [2:59:22<47:33,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:27:51,396 >> Saving model checkpoint to ./debug_squad/checkpoint-10500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:27:51,423 >> Configuration saved in ./debug_squad/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:28:30,798 >> Model weights saved in ./debug_squad/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:28:30,805 >> tokenizer config file saved in ./debug_squad/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:28:30,808 >> Special tokens file saved in ./debug_squad/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.6985, 'learning_rate': 7.633184221228141e-06, 'epoch': 1.49}         \n",
      " 75%|██████████████████████████         | 11000/14754 [3:06:47<42:06,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:35:16,550 >> Saving model checkpoint to ./debug_squad/checkpoint-11000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:35:16,579 >> Configuration saved in ./debug_squad/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:35:52,543 >> Model weights saved in ./debug_squad/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:35:52,552 >> tokenizer config file saved in ./debug_squad/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:35:52,573 >> Special tokens file saved in ./debug_squad/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.7167, 'learning_rate': 6.616510776738511e-06, 'epoch': 1.56}         \n",
      " 78%|███████████████████████████▎       | 11500/14754 [3:14:10<36:15,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 16:42:39,330 >> Saving model checkpoint to ./debug_squad/checkpoint-11500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:42:39,345 >> Configuration saved in ./debug_squad/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:43:16,006 >> Model weights saved in ./debug_squad/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:43:16,012 >> tokenizer config file saved in ./debug_squad/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:43:16,030 >> Special tokens file saved in ./debug_squad/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.7123, 'learning_rate': 5.5998373322488825e-06, 'epoch': 1.63}        \n",
      " 81%|████████████████████████████▍      | 12000/14754 [3:21:32<30:44,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:50:01,099 >> Saving model checkpoint to ./debug_squad/checkpoint-12000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:50:01,106 >> Configuration saved in ./debug_squad/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:50:37,585 >> Model weights saved in ./debug_squad/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:50:37,612 >> tokenizer config file saved in ./debug_squad/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:50:37,633 >> Special tokens file saved in ./debug_squad/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.6785, 'learning_rate': 4.583163887759252e-06, 'epoch': 1.69}         \n",
      " 85%|█████████████████████████████▋     | 12500/14754 [3:28:54<25:15,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 16:57:23,735 >> Saving model checkpoint to ./debug_squad/checkpoint-12500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 16:57:23,741 >> Configuration saved in ./debug_squad/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 16:58:00,553 >> Model weights saved in ./debug_squad/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 16:58:00,560 >> tokenizer config file saved in ./debug_squad/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 16:58:00,585 >> Special tokens file saved in ./debug_squad/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.7045, 'learning_rate': 3.566490443269622e-06, 'epoch': 1.76}         \n",
      " 88%|██████████████████████████████▊    | 13000/14754 [3:36:17<19:35,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 17:04:46,115 >> Saving model checkpoint to ./debug_squad/checkpoint-13000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 17:04:46,140 >> Configuration saved in ./debug_squad/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 17:05:25,137 >> Model weights saved in ./debug_squad/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 17:05:25,144 >> tokenizer config file saved in ./debug_squad/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 17:05:25,149 >> Special tokens file saved in ./debug_squad/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.7032, 'learning_rate': 2.549816998779992e-06, 'epoch': 1.83}         \n",
      " 92%|████████████████████████████████   | 13500/14754 [3:43:42<14:02,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 17:12:11,068 >> Saving model checkpoint to ./debug_squad/checkpoint-13500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 17:12:11,070 >> Configuration saved in ./debug_squad/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 17:12:12,085 >> Model weights saved in ./debug_squad/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 17:12:12,087 >> tokenizer config file saved in ./debug_squad/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 17:12:12,088 >> Special tokens file saved in ./debug_squad/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.6924, 'learning_rate': 1.533143554290362e-06, 'epoch': 1.9}          \n",
      " 95%|█████████████████████████████████▏ | 14000/14754 [3:49:19<08:24,  1.49it/s][INFO|trainer.py:2166] 2022-04-29 17:17:48,796 >> Saving model checkpoint to ./debug_squad/checkpoint-14000\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 17:17:48,815 >> Configuration saved in ./debug_squad/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 17:17:49,834 >> Model weights saved in ./debug_squad/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 17:17:49,836 >> tokenizer config file saved in ./debug_squad/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 17:17:49,836 >> Special tokens file saved in ./debug_squad/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.6908, 'learning_rate': 5.164701098007319e-07, 'epoch': 1.97}         \n",
      " 98%|██████████████████████████████████▍| 14500/14754 [3:54:57<02:49,  1.50it/s][INFO|trainer.py:2166] 2022-04-29 17:23:26,064 >> Saving model checkpoint to ./debug_squad/checkpoint-14500\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 17:23:26,067 >> Configuration saved in ./debug_squad/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 17:23:27,327 >> Model weights saved in ./debug_squad/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 17:23:27,352 >> tokenizer config file saved in ./debug_squad/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 17:23:27,375 >> Special tokens file saved in ./debug_squad/checkpoint-14500/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 14754/14754 [3:57:49<00:00,  1.49it/s][INFO|trainer.py:1530] 2022-04-29 17:26:18,802 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 14269.9075, 'train_samples_per_second': 12.407, 'train_steps_per_second': 1.034, 'train_loss': 0.9864320159039233, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████| 14754/14754 [3:57:49<00:00,  1.03it/s]\n",
      "[INFO|trainer.py:2166] 2022-04-29 17:26:18,945 >> Saving model checkpoint to ./debug_squad/\n",
      "[INFO|configuration_utils.py:441] 2022-04-29 17:26:18,948 >> Configuration saved in ./debug_squad/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-29 17:26:19,835 >> Model weights saved in ./debug_squad/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-29 17:26:19,837 >> tokenizer config file saved in ./debug_squad/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-29 17:26:19,858 >> Special tokens file saved in ./debug_squad/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.9864\n",
      "  train_runtime            = 3:57:49.90\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     12.407\n",
      "  train_steps_per_second   =      1.034\n",
      "04/29/2022 17:26:19 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:566] 2022-04-29 17:26:19,950 >> The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2416] 2022-04-29 17:26:19,952 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2022-04-29 17:26:19,952 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2421] 2022-04-29 17:26:19,952 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1348/1348 [03:31<00:00,  6.38it/s]04/29/2022 17:30:07 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 36/10570 [00:00<00:29, 355.00it/s]\u001b[A\n",
      "  1%|▎                                      | 74/10570 [00:00<00:28, 368.46it/s]\u001b[A\n",
      "  1%|▍                                     | 111/10570 [00:00<00:28, 361.01it/s]\u001b[A\n",
      "  1%|▌                                     | 148/10570 [00:00<00:32, 319.97it/s]\u001b[A\n",
      "  2%|▋                                     | 183/10570 [00:00<00:31, 329.58it/s]\u001b[A\n",
      "  2%|▊                                     | 220/10570 [00:00<00:30, 339.71it/s]\u001b[A\n",
      "  2%|▉                                     | 255/10570 [00:00<00:34, 300.30it/s]\u001b[A\n",
      "  3%|█                                     | 286/10570 [00:00<00:40, 253.07it/s]\u001b[A\n",
      "  3%|█▏                                    | 319/10570 [00:01<00:37, 270.90it/s]\u001b[A\n",
      "  3%|█▎                                    | 352/10570 [00:01<00:35, 286.04it/s]\u001b[A\n",
      "  4%|█▍                                    | 386/10570 [00:01<00:34, 298.63it/s]\u001b[A\n",
      "  4%|█▌                                    | 418/10570 [00:01<00:33, 304.39it/s]\u001b[A\n",
      "  4%|█▋                                    | 454/10570 [00:01<00:31, 317.93it/s]\u001b[A\n",
      "  5%|█▊                                    | 488/10570 [00:01<00:31, 322.44it/s]\u001b[A\n",
      "  5%|█▊                                    | 521/10570 [00:01<00:31, 323.65it/s]\u001b[A\n",
      "  5%|█▉                                    | 555/10570 [00:01<00:30, 324.28it/s]\u001b[A\n",
      "  6%|██                                    | 589/10570 [00:01<00:30, 326.59it/s]\u001b[A\n",
      "  6%|██▏                                   | 622/10570 [00:01<00:30, 324.64it/s]\u001b[A\n",
      "  6%|██▎                                   | 656/10570 [00:02<00:30, 328.04it/s]\u001b[A\n",
      "  7%|██▍                                   | 692/10570 [00:02<00:29, 335.82it/s]\u001b[A\n",
      "  7%|██▌                                   | 727/10570 [00:02<00:29, 337.42it/s]\u001b[A\n",
      "  7%|██▋                                   | 761/10570 [00:02<00:29, 329.59it/s]\u001b[A\n",
      "  8%|██▊                                   | 795/10570 [00:02<00:30, 324.65it/s]\u001b[A\n",
      "  8%|██▉                                   | 828/10570 [00:02<00:30, 315.49it/s]\u001b[A\n",
      "  8%|███                                   | 867/10570 [00:02<00:28, 335.04it/s]\u001b[A\n",
      "  9%|███▏                                  | 904/10570 [00:02<00:28, 345.02it/s]\u001b[A\n",
      "  9%|███▍                                  | 939/10570 [00:02<00:29, 331.99it/s]\u001b[A\n",
      "  9%|███▍                                  | 973/10570 [00:03<00:29, 320.59it/s]\u001b[A\n",
      " 10%|███▌                                 | 1006/10570 [00:03<00:29, 323.16it/s]\u001b[A\n",
      " 10%|███▋                                 | 1039/10570 [00:03<00:30, 307.97it/s]\u001b[A\n",
      " 10%|███▋                                 | 1071/10570 [00:03<00:31, 297.95it/s]\u001b[A\n",
      " 10%|███▊                                 | 1101/10570 [00:03<00:34, 276.60it/s]\u001b[A\n",
      " 11%|███▉                                 | 1133/10570 [00:03<00:32, 286.72it/s]\u001b[A\n",
      " 11%|████                                 | 1163/10570 [00:03<00:32, 289.73it/s]\u001b[A\n",
      " 11%|████▏                                | 1193/10570 [00:03<00:32, 292.02it/s]\u001b[A\n",
      " 12%|████▎                                | 1223/10570 [00:03<00:32, 288.80it/s]\u001b[A\n",
      " 12%|████▍                                | 1253/10570 [00:04<00:34, 272.80it/s]\u001b[A\n",
      " 12%|████▍                                | 1281/10570 [00:04<00:34, 267.95it/s]\u001b[A\n",
      " 12%|████▌                                | 1308/10570 [00:04<00:35, 262.64it/s]\u001b[A\n",
      " 13%|████▋                                | 1335/10570 [00:04<00:35, 261.77it/s]\u001b[A\n",
      " 13%|████▊                                | 1362/10570 [00:04<00:37, 247.35it/s]\u001b[A\n",
      " 13%|████▊                                | 1388/10570 [00:04<00:36, 248.52it/s]\u001b[A\n",
      " 13%|████▉                                | 1413/10570 [00:04<00:36, 247.97it/s]\u001b[A\n",
      " 14%|█████                                | 1440/10570 [00:04<00:35, 254.20it/s]\u001b[A\n",
      " 14%|█████▏                               | 1467/10570 [00:04<00:35, 257.30it/s]\u001b[A\n",
      " 14%|█████▏                               | 1493/10570 [00:05<00:35, 257.09it/s]\u001b[A\n",
      " 14%|█████▎                               | 1525/10570 [00:05<00:33, 272.95it/s]\u001b[A\n",
      " 15%|█████▍                               | 1555/10570 [00:05<00:32, 280.22it/s]\u001b[A\n",
      " 15%|█████▌                               | 1584/10570 [00:05<00:31, 281.55it/s]\u001b[A\n",
      " 15%|█████▋                               | 1616/10570 [00:05<00:30, 290.53it/s]\u001b[A\n",
      " 16%|█████▊                               | 1646/10570 [00:05<00:30, 288.96it/s]\u001b[A\n",
      " 16%|█████▊                               | 1677/10570 [00:05<00:30, 293.32it/s]\u001b[A\n",
      " 16%|█████▉                               | 1708/10570 [00:05<00:29, 297.02it/s]\u001b[A\n",
      " 16%|██████                               | 1738/10570 [00:05<00:31, 283.42it/s]\u001b[A\n",
      " 17%|██████▏                              | 1767/10570 [00:05<00:31, 280.89it/s]\u001b[A\n",
      " 17%|██████▎                              | 1796/10570 [00:06<00:31, 279.92it/s]\u001b[A\n",
      " 17%|██████▍                              | 1825/10570 [00:06<00:33, 259.26it/s]\u001b[A\n",
      " 18%|██████▍                              | 1853/10570 [00:06<00:33, 262.92it/s]\u001b[A\n",
      " 18%|██████▌                              | 1883/10570 [00:06<00:31, 272.56it/s]\u001b[A\n",
      " 18%|██████▋                              | 1911/10570 [00:06<00:32, 265.12it/s]\u001b[A\n",
      " 18%|██████▊                              | 1939/10570 [00:06<00:32, 268.30it/s]\u001b[A\n",
      " 19%|██████▉                              | 1967/10570 [00:06<00:31, 269.33it/s]\u001b[A\n",
      " 19%|██████▉                              | 1996/10570 [00:06<00:31, 274.74it/s]\u001b[A\n",
      " 19%|███████                              | 2028/10570 [00:06<00:29, 286.56it/s]\u001b[A\n",
      " 19%|███████▏                             | 2057/10570 [00:07<00:30, 276.24it/s]\u001b[A\n",
      " 20%|███████▎                             | 2089/10570 [00:07<00:29, 287.03it/s]\u001b[A\n",
      " 20%|███████▍                             | 2118/10570 [00:07<00:30, 278.73it/s]\u001b[A\n",
      " 20%|███████▌                             | 2146/10570 [00:07<00:32, 257.62it/s]\u001b[A\n",
      " 21%|███████▌                             | 2173/10570 [00:07<00:34, 246.00it/s]\u001b[A\n",
      " 21%|███████▋                             | 2203/10570 [00:07<00:32, 260.01it/s]\u001b[A\n",
      " 21%|███████▊                             | 2235/10570 [00:07<00:30, 276.11it/s]\u001b[A\n",
      " 21%|███████▉                             | 2266/10570 [00:07<00:29, 283.53it/s]\u001b[A\n",
      " 22%|████████                             | 2297/10570 [00:07<00:28, 289.08it/s]\u001b[A\n",
      " 22%|████████▏                            | 2327/10570 [00:07<00:29, 280.98it/s]\u001b[A\n",
      " 22%|████████▏                            | 2356/10570 [00:08<00:30, 271.53it/s]\u001b[A\n",
      " 23%|████████▎                            | 2384/10570 [00:08<00:31, 256.14it/s]\u001b[A\n",
      " 23%|████████▍                            | 2415/10570 [00:08<00:30, 268.55it/s]\u001b[A\n",
      " 23%|████████▌                            | 2446/10570 [00:08<00:29, 279.21it/s]\u001b[A\n",
      " 23%|████████▋                            | 2475/10570 [00:08<00:29, 277.22it/s]\u001b[A\n",
      " 24%|████████▊                            | 2503/10570 [00:08<00:29, 273.20it/s]\u001b[A\n",
      " 24%|████████▊                            | 2534/10570 [00:08<00:28, 281.76it/s]\u001b[A\n",
      " 24%|████████▉                            | 2563/10570 [00:08<00:29, 269.95it/s]\u001b[A\n",
      " 25%|█████████                            | 2591/10570 [00:08<00:30, 260.95it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2620/10570 [00:09<00:29, 267.98it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2647/10570 [00:09<00:29, 266.84it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2676/10570 [00:09<00:29, 271.95it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2708/10570 [00:09<00:27, 285.32it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2737/10570 [00:09<00:27, 280.79it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2769/10570 [00:09<00:26, 289.83it/s]\u001b[A\n",
      " 26%|█████████▊                           | 2801/10570 [00:09<00:26, 296.73it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2832/10570 [00:09<00:25, 299.35it/s]\u001b[A\n",
      " 27%|██████████                           | 2864/10570 [00:09<00:25, 304.34it/s]\u001b[A\n",
      " 27%|██████████▏                          | 2895/10570 [00:10<00:26, 288.85it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2925/10570 [00:10<00:26, 286.25it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2957/10570 [00:10<00:25, 293.25it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2987/10570 [00:10<00:26, 282.74it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3016/10570 [00:10<00:26, 280.36it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3045/10570 [00:10<00:27, 273.49it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3075/10570 [00:10<00:26, 279.92it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3106/10570 [00:10<00:26, 284.62it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3135/10570 [00:10<00:26, 281.65it/s]\u001b[A\n",
      " 30%|███████████                          | 3164/10570 [00:10<00:26, 277.99it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3192/10570 [00:11<00:26, 275.98it/s]\u001b[A\n",
      " 30%|███████████▎                         | 3220/10570 [00:11<00:28, 260.49it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3247/10570 [00:11<00:28, 253.39it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3273/10570 [00:11<00:28, 251.69it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3299/10570 [00:11<00:29, 250.13it/s]\u001b[A\n",
      " 32%|███████████▋                         | 3331/10570 [00:11<00:27, 267.35it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3361/10570 [00:11<00:26, 276.26it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3389/10570 [00:11<00:26, 272.05it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3418/10570 [00:11<00:25, 276.54it/s]\u001b[A\n",
      " 33%|████████████                         | 3446/10570 [00:12<00:25, 276.44it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3474/10570 [00:12<00:26, 269.31it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3502/10570 [00:12<00:26, 271.08it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3532/10570 [00:12<00:25, 279.17it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3560/10570 [00:12<00:25, 271.73it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3588/10570 [00:12<00:25, 271.57it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3617/10570 [00:12<00:25, 275.35it/s]\u001b[A\n",
      " 34%|████████████▊                        | 3646/10570 [00:12<00:24, 278.78it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3677/10570 [00:12<00:24, 287.13it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3706/10570 [00:12<00:24, 282.03it/s]\u001b[A\n",
      " 35%|█████████████                        | 3737/10570 [00:13<00:23, 290.19it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3767/10570 [00:13<00:23, 289.87it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3797/10570 [00:13<00:24, 281.24it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3826/10570 [00:13<00:23, 281.07it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3855/10570 [00:13<00:24, 278.59it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3883/10570 [00:13<00:25, 267.35it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3910/10570 [00:13<00:24, 267.45it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3937/10570 [00:13<00:24, 266.51it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3969/10570 [00:13<00:23, 279.82it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3999/10570 [00:14<00:23, 285.02it/s]\u001b[A\n",
      " 38%|██████████████                       | 4028/10570 [00:14<00:24, 268.12it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4056/10570 [00:14<00:24, 267.61it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4083/10570 [00:14<00:24, 265.78it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4110/10570 [00:14<00:24, 264.15it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4137/10570 [00:14<00:25, 250.41it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4163/10570 [00:14<00:29, 215.90it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4186/10570 [00:14<00:31, 201.38it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4207/10570 [00:15<00:33, 189.54it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4234/10570 [00:15<00:30, 208.82it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4260/10570 [00:15<00:28, 219.46it/s]\u001b[A\n",
      " 41%|██████████████▉                      | 4283/10570 [00:15<00:38, 161.56it/s]\u001b[A\n",
      " 41%|███████████████                      | 4302/10570 [00:15<00:40, 156.31it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4327/10570 [00:15<00:35, 176.69it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4356/10570 [00:15<00:30, 203.44it/s]\u001b[A\n",
      " 42%|███████████████▎                     | 4387/10570 [00:15<00:26, 229.41it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4418/10570 [00:16<00:24, 248.83it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4445/10570 [00:16<00:24, 252.18it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4472/10570 [00:16<00:24, 248.46it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4498/10570 [00:16<00:24, 243.89it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4523/10570 [00:16<00:24, 243.75it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4548/10570 [00:16<00:24, 243.05it/s]\u001b[A\n",
      " 43%|████████████████                     | 4576/10570 [00:16<00:23, 252.00it/s]\u001b[A\n",
      " 44%|████████████████                     | 4602/10570 [00:16<00:24, 242.40it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4633/10570 [00:16<00:22, 260.16it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4660/10570 [00:16<00:23, 248.38it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4686/10570 [00:17<00:23, 250.78it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4714/10570 [00:17<00:22, 257.63it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4740/10570 [00:17<00:23, 249.79it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4766/10570 [00:17<00:24, 239.71it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4791/10570 [00:17<00:23, 241.57it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4821/10570 [00:17<00:22, 257.09it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4848/10570 [00:17<00:21, 260.39it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4875/10570 [00:17<00:22, 250.85it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4905/10570 [00:17<00:21, 263.60it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4935/10570 [00:18<00:20, 273.76it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4967/10570 [00:18<00:19, 285.81it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4996/10570 [00:18<00:20, 274.55it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 5024/10570 [00:18<00:20, 265.06it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5052/10570 [00:18<00:20, 268.19it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5082/10570 [00:18<00:19, 276.40it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5110/10570 [00:18<00:20, 267.40it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 5140/10570 [00:18<00:19, 275.98it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5171/10570 [00:18<00:18, 284.42it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5200/10570 [00:19<00:19, 281.63it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5229/10570 [00:19<00:19, 271.73it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5259/10570 [00:19<00:19, 278.23it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5290/10570 [00:19<00:18, 285.14it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5319/10570 [00:19<00:18, 285.21it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5348/10570 [00:19<00:18, 282.42it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5377/10570 [00:19<00:24, 210.87it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5402/10570 [00:19<00:23, 217.77it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5427/10570 [00:19<00:22, 223.89it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5451/10570 [00:20<00:23, 222.21it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5476/10570 [00:20<00:22, 229.46it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5500/10570 [00:20<00:23, 219.95it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5528/10570 [00:20<00:21, 235.61it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5553/10570 [00:20<00:21, 238.17it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5579/10570 [00:20<00:20, 242.14it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5605/10570 [00:20<00:20, 246.83it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5630/10570 [00:20<00:20, 237.72it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5655/10570 [00:20<00:20, 235.49it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5679/10570 [00:21<00:22, 220.84it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5704/10570 [00:21<00:21, 227.96it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5731/10570 [00:21<00:20, 238.74it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 5756/10570 [00:21<00:20, 235.99it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5780/10570 [00:21<00:20, 232.99it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5807/10570 [00:21<00:19, 241.30it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5832/10570 [00:21<00:19, 241.99it/s]\u001b[A\n",
      " 55%|████████████████████▌                | 5862/10570 [00:21<00:18, 257.94it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5894/10570 [00:21<00:17, 273.80it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5922/10570 [00:21<00:16, 273.46it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5950/10570 [00:22<00:17, 261.18it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5977/10570 [00:22<00:17, 258.60it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6003/10570 [00:22<00:18, 252.82it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6030/10570 [00:22<00:17, 255.57it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6057/10570 [00:22<00:17, 255.44it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6083/10570 [00:22<00:17, 256.68it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6112/10570 [00:22<00:16, 265.91it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6142/10570 [00:22<00:16, 272.78it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6170/10570 [00:22<00:16, 266.22it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6197/10570 [00:23<00:16, 263.16it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6224/10570 [00:23<00:18, 239.56it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6249/10570 [00:23<00:19, 225.53it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6280/10570 [00:23<00:17, 246.90it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6310/10570 [00:23<00:16, 259.25it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6337/10570 [00:23<00:16, 254.79it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6363/10570 [00:23<00:16, 253.27it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6389/10570 [00:23<00:17, 243.56it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6417/10570 [00:23<00:16, 253.23it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6445/10570 [00:24<00:15, 258.45it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6472/10570 [00:24<00:15, 258.92it/s]\u001b[A\n",
      " 61%|██████████████████████▊              | 6500/10570 [00:24<00:15, 264.46it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6527/10570 [00:24<00:15, 258.77it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6558/10570 [00:24<00:14, 272.22it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6588/10570 [00:24<00:14, 280.16it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6619/10570 [00:24<00:13, 287.83it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6648/10570 [00:24<00:13, 282.06it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6677/10570 [00:24<00:14, 275.94it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6705/10570 [00:24<00:14, 275.17it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6735/10570 [00:25<00:13, 281.02it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6765/10570 [00:25<00:13, 284.88it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6795/10570 [00:25<00:13, 286.59it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6824/10570 [00:25<00:13, 277.41it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6852/10570 [00:25<00:13, 276.64it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6880/10570 [00:25<00:13, 275.16it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6908/10570 [00:25<00:13, 270.71it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6938/10570 [00:25<00:13, 277.23it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6966/10570 [00:25<00:13, 268.03it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6994/10570 [00:26<00:13, 270.47it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7022/10570 [00:26<00:13, 259.45it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7049/10570 [00:26<00:13, 256.03it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7075/10570 [00:26<00:13, 252.21it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7102/10570 [00:26<00:13, 255.45it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7128/10570 [00:26<00:13, 252.84it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7156/10570 [00:26<00:13, 260.24it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7183/10570 [00:26<00:12, 261.59it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7210/10570 [00:26<00:13, 253.55it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7236/10570 [00:27<00:13, 238.19it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7262/10570 [00:27<00:13, 243.81it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7290/10570 [00:27<00:12, 253.96it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7319/10570 [00:27<00:12, 262.59it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7346/10570 [00:27<00:13, 239.35it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7375/10570 [00:27<00:12, 252.50it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7405/10570 [00:27<00:11, 263.99it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7437/10570 [00:27<00:11, 279.53it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7467/10570 [00:27<00:10, 285.02it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7496/10570 [00:27<00:10, 285.45it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7525/10570 [00:28<00:11, 275.59it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7553/10570 [00:28<00:11, 269.05it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7581/10570 [00:28<00:11, 269.72it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7609/10570 [00:28<00:11, 262.60it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7636/10570 [00:28<00:11, 245.02it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7661/10570 [00:28<00:12, 238.67it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7690/10570 [00:28<00:11, 249.39it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7719/10570 [00:28<00:11, 258.51it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7746/10570 [00:28<00:10, 258.43it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7774/10570 [00:29<00:10, 261.63it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7804/10570 [00:29<00:10, 270.80it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7832/10570 [00:29<00:10, 259.50it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7859/10570 [00:29<00:10, 255.90it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7890/10570 [00:29<00:09, 269.95it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7920/10570 [00:29<00:09, 278.43it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7950/10570 [00:29<00:09, 284.54it/s]\u001b[A\n",
      " 76%|███████████████████████████▉         | 7981/10570 [00:29<00:08, 289.51it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8011/10570 [00:29<00:08, 288.16it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8040/10570 [00:30<00:09, 277.96it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8068/10570 [00:30<00:09, 261.66it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8101/10570 [00:30<00:08, 278.97it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8134/10570 [00:30<00:08, 290.65it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8165/10570 [00:30<00:08, 293.78it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8195/10570 [00:30<00:08, 287.28it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8225/10570 [00:30<00:08, 289.99it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8257/10570 [00:30<00:07, 296.76it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 8287/10570 [00:30<00:08, 277.24it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8316/10570 [00:31<00:08, 266.65it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8343/10570 [00:31<00:08, 263.42it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8371/10570 [00:31<00:08, 266.71it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8398/10570 [00:31<00:08, 264.75it/s]\u001b[A\n",
      " 80%|█████████████████████████████▍       | 8425/10570 [00:31<00:08, 254.84it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8453/10570 [00:31<00:08, 260.33it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8482/10570 [00:31<00:07, 268.11it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8511/10570 [00:31<00:07, 271.90it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8543/10570 [00:31<00:07, 283.77it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8572/10570 [00:31<00:07, 283.35it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8602/10570 [00:32<00:06, 285.90it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8631/10570 [00:32<00:06, 285.36it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8660/10570 [00:32<00:06, 274.85it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8688/10570 [00:32<00:07, 260.29it/s]\u001b[A\n",
      " 82%|██████████████████████████████▌      | 8715/10570 [00:32<00:07, 255.32it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8741/10570 [00:32<00:07, 247.44it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8768/10570 [00:32<00:07, 251.44it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8796/10570 [00:32<00:06, 253.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████▉      | 8824/10570 [00:32<00:06, 259.73it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8851/10570 [00:33<00:06, 253.05it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8877/10570 [00:33<00:06, 253.23it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8907/10570 [00:33<00:06, 265.50it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8934/10570 [00:33<00:06, 259.15it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8960/10570 [00:33<00:06, 250.95it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8987/10570 [00:33<00:06, 254.34it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9015/10570 [00:33<00:05, 261.47it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9043/10570 [00:33<00:05, 265.63it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9073/10570 [00:33<00:05, 273.08it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9101/10570 [00:33<00:05, 273.21it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9132/10570 [00:34<00:05, 281.59it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9162/10570 [00:34<00:04, 284.67it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9191/10570 [00:34<00:04, 282.37it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9220/10570 [00:34<00:04, 282.54it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9249/10570 [00:34<00:04, 271.09it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9277/10570 [00:34<00:04, 271.86it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9306/10570 [00:34<00:04, 273.96it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9334/10570 [00:34<00:04, 259.22it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9363/10570 [00:34<00:04, 267.76it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9392/10570 [00:35<00:04, 273.94it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9421/10570 [00:35<00:04, 276.12it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9449/10570 [00:35<00:04, 270.73it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9480/10570 [00:35<00:03, 281.45it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9511/10570 [00:35<00:03, 288.18it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9540/10570 [00:35<00:03, 284.19it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▍   | 9570/10570 [00:35<00:03, 288.45it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9602/10570 [00:35<00:03, 295.26it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9634/10570 [00:35<00:03, 300.84it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9666/10570 [00:35<00:02, 305.11it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9697/10570 [00:36<00:02, 303.60it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9728/10570 [00:36<00:02, 295.68it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9758/10570 [00:36<00:02, 282.27it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9788/10570 [00:36<00:02, 286.20it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9817/10570 [00:36<00:02, 282.98it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9846/10570 [00:36<00:02, 262.05it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9876/10570 [00:36<00:02, 270.29it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9904/10570 [00:36<00:02, 264.57it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9932/10570 [00:36<00:02, 268.80it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9961/10570 [00:37<00:02, 274.41it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▉  | 9993/10570 [00:37<00:02, 286.74it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10022/10570 [00:37<00:01, 285.45it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10052/10570 [00:37<00:01, 285.59it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10081/10570 [00:37<00:01, 276.11it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10109/10570 [00:37<00:01, 272.23it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10137/10570 [00:37<00:01, 273.09it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10165/10570 [00:37<00:01, 271.67it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10194/10570 [00:37<00:01, 275.52it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10222/10570 [00:37<00:01, 269.40it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10250/10570 [00:38<00:01, 271.61it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10281/10570 [00:38<00:01, 279.74it/s]\u001b[A\n",
      " 98%|███████████████████████████████████ | 10310/10570 [00:38<00:00, 260.47it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10339/10570 [00:38<00:00, 266.57it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10367/10570 [00:38<00:00, 270.34it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10395/10570 [00:38<00:00, 264.37it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▍| 10422/10570 [00:38<00:00, 248.84it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10452/10570 [00:38<00:00, 262.76it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10482/10570 [00:38<00:00, 271.75it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10510/10570 [00:39<00:00, 263.24it/s]\u001b[A\n",
      "100%|███████████████████████████████████▉| 10537/10570 [00:39<00:00, 255.89it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:39<00:00, 268.85it/s]\u001b[A\n",
      "04/29/2022 17:30:46 - INFO - utils_qa - Saving predictions to ./debug_squad/eval_predictions.json.\n",
      "04/29/2022 17:30:46 - INFO - utils_qa - Saving nbest_preds to ./debug_squad/eval_nbest_predictions.json.\n",
      "04/29/2022 17:30:52 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1348/1348 [04:32<00:00,  4.94it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     2.0\n",
      "  eval_exact_match = 80.9745\n",
      "  eval_f1          = 88.4023\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee801b75-95b4-4760-aead-725c8ec33b86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

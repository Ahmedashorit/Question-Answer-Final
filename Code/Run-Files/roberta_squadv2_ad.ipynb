{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efc68a64-c9ff-4c94-b052-3d51def92ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==1.9.0 in /opt/conda/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f28ae041-287b-4ed6-a600-0ff84af79b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03ad2d76-949b-4809-b827-d6395dc9ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a74913b9-2d34-4078-9d87-0df15b6dfb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/26/2022 20:13:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "05/26/2022 20:13:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_roberta_base_uncased_advanced/runs/May26_20-13-32_ny1g7f1a1u,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_roberta_base_uncased_advanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_roberta_base_uncased_advanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/26/2022 20:13:32 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/26/2022 20:13:32 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/26/2022 20:13:32 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "05/26/2022 20:13:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/26/2022 20:13:32 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "05/26/2022 20:13:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 143.20it/s]\n",
      "[INFO|configuration_utils.py:659] 2022-05-26 20:13:32,804 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-26 20:13:32,805 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:371] 2022-05-26 20:13:32,870 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:659] 2022-05-26 20:13:32,933 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-26 20:13:32,934 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-26 20:13:33,367 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:659] 2022-05-26 20:13:33,426 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:708] 2022-05-26 20:13:33,427 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1953] 2022-05-26 20:13:33,574 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-26 20:13:36,132 >> Some weights of the model checkpoint at roberta-base were not used when initializing Advanced_RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing Advanced_RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Advanced_RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-26 20:13:36,132 >> Some weights of Advanced_RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/26/2022 20:13:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-ae3947db143ee90e.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:37<00:00,  2.37ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/26/2022 20:14:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-3dd8be6d7ad90896.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:34<00:00,  3.17s/ba]\n",
      "[INFO|trainer.py:502] 2022-05-26 20:14:53,716 >> Using amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-26 20:14:53,958 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-26 20:14:53,958 >>   Num examples = 88568\n",
      "[INFO|trainer.py:1421] 2022-05-26 20:14:53,958 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1422] 2022-05-26 20:14:53,958 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-26 20:14:53,958 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-26 20:14:53,958 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-26 20:14:53,958 >>   Total optimization steps = 14762\n",
      "  0%|                                                 | 0/14762 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.8082, 'learning_rate': 2.8994038748137112e-05, 'epoch': 0.07}        \n",
      "  3%|█▎                                     | 500/14762 [01:53<54:09,  4.39it/s][INFO|trainer.py:2340] 2022-05-26 20:16:47,436 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:16:47,437 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:16:48,729 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:16:48,730 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:16:48,765 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.2423, 'learning_rate': 2.7977916271507926e-05, 'epoch': 0.14}        \n",
      "  7%|██▌                                   | 1000/14762 [03:51<51:56,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 20:18:45,008 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:18:45,043 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:18:46,334 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:18:46,335 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:18:46,336 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/special_tokens_map.json\n",
      "  7%|██▍                                 | 1005/14762 [03:56<2:00:27,  1.90it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.1561, 'learning_rate': 2.6963826039832e-05, 'epoch': 0.2}            \n",
      " 10%|███▊                                  | 1500/14762 [05:48<50:08,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:20:42,716 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:20:42,717 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:20:44,107 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:20:44,109 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:20:44,147 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.0942, 'learning_rate': 2.594770356320282e-05, 'epoch': 0.27}         \n",
      " 14%|█████▏                                | 2000/14762 [07:46<48:10,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 20:22:40,468 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:22:40,501 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:22:41,993 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:22:42,085 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:22:42,092 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/special_tokens_map.json\n",
      " 17%|██████▍                               | 2487/14762 [09:41<46:25,  4.41it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.0397, 'learning_rate': 2.4933613331526897e-05, 'epoch': 0.34}        \n",
      " 17%|██████▍                               | 2500/14762 [09:44<46:22,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:24:38,647 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:24:38,681 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:24:39,812 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:24:39,832 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:24:39,866 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.0193, 'learning_rate': 2.391749085489771e-05, 'epoch': 0.41}         \n",
      " 20%|███████▋                              | 3000/14762 [11:42<45:08,  4.34it/s][INFO|trainer.py:2340] 2022-05-26 20:26:36,094 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:26:36,095 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:26:37,308 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:26:37,309 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:26:37,354 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.001, 'learning_rate': 2.290136837826853e-05, 'epoch': 0.47}          \n",
      " 24%|█████████                             | 3500/14762 [13:40<42:38,  4.40it/s][INFO|trainer.py:2340] 2022-05-26 20:28:34,853 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:28:34,887 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:28:36,139 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:28:36,173 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:28:36,174 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 0.937, 'learning_rate': 2.1885245901639347e-05, 'epoch': 0.54}         \n",
      " 27%|██████████▎                           | 4000/14762 [15:39<40:36,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 20:30:33,727 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:30:33,765 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:30:34,961 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:30:34,962 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:30:34,963 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 0.9712, 'learning_rate': 2.086912342501016e-05, 'epoch': 0.61}         \n",
      " 30%|███████████▌                          | 4500/14762 [17:37<39:01,  4.38it/s][INFO|trainer.py:2340] 2022-05-26 20:32:31,810 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:32:31,811 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:32:33,164 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:32:33,165 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:32:33,206 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 0.9044, 'learning_rate': 1.985300094838098e-05, 'epoch': 0.68}         \n",
      " 34%|████████████▊                         | 5000/14762 [19:36<36:53,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:34:30,613 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:34:30,615 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:34:32,008 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:34:32,010 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:34:32,065 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/special_tokens_map.json\n",
      " 35%|█████████████▎                        | 5161/14762 [20:18<36:21,  4.40it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.9292, 'learning_rate': 1.8838910716705054e-05, 'epoch': 0.75}        \n",
      " 37%|██████████████▏                       | 5500/14762 [21:36<35:00,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:36:30,087 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:36:30,121 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:36:31,310 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:36:31,312 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:36:31,346 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 0.8986, 'learning_rate': 1.782278824007587e-05, 'epoch': 0.81}         \n",
      " 41%|███████████████▍                      | 6000/14762 [23:34<33:07,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:38:28,667 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:38:28,670 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:38:29,995 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:38:29,996 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:38:29,997 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/special_tokens_map.json\n",
      " 41%|███████████████▋                      | 6094/14762 [24:00<33:10,  4.36it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.901, 'learning_rate': 1.6808698008399946e-05, 'epoch': 0.88}         \n",
      " 44%|████████████████▋                     | 6500/14762 [25:33<31:25,  4.38it/s][INFO|trainer.py:2340] 2022-05-26 20:40:27,455 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:40:27,456 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:40:28,719 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:40:28,720 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:40:28,720 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 0.9034, 'learning_rate': 1.5792575531770764e-05, 'epoch': 0.95}        \n",
      " 47%|██████████████████                    | 7000/14762 [27:31<29:27,  4.39it/s][INFO|trainer.py:2340] 2022-05-26 20:42:25,266 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:42:25,267 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:42:26,565 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:42:26,594 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:42:26,596 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.8357, 'learning_rate': 1.477645305514158e-05, 'epoch': 1.02}         \n",
      " 51%|███████████████████▎                  | 7500/14762 [29:28<27:25,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:44:22,762 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-7500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:44:22,764 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:44:24,074 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:44:24,075 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:44:24,109 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.6838, 'learning_rate': 1.3760330578512398e-05, 'epoch': 1.08}        \n",
      " 54%|████████████████████▌                 | 8000/14762 [31:26<25:35,  4.40it/s][INFO|trainer.py:2340] 2022-05-26 20:46:20,334 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-8000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:46:20,336 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:46:21,505 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:46:21,506 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:46:21,550 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.6776, 'learning_rate': 1.2744208101883214e-05, 'epoch': 1.15}        \n",
      " 58%|█████████████████████▉                | 8500/14762 [33:23<23:40,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:48:17,821 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-8500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:48:17,860 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:48:19,163 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:48:19,165 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:48:19,166 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.6571, 'learning_rate': 1.1728085625254032e-05, 'epoch': 1.22}        \n",
      " 61%|███████████████████████▏              | 9000/14762 [35:21<21:44,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 20:50:15,698 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-9000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:50:15,733 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:50:17,060 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:50:17,062 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:50:17,062 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.6833, 'learning_rate': 1.0711963148624848e-05, 'epoch': 1.29}        \n",
      " 64%|████████████████████████▍             | 9500/14762 [37:19<20:03,  4.37it/s][INFO|trainer.py:2340] 2022-05-26 20:52:13,548 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-9500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:52:13,551 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:52:14,856 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:52:14,858 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:52:14,859 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.6553, 'learning_rate': 9.695840671995665e-06, 'epoch': 1.35}         \n",
      " 68%|█████████████████████████            | 10000/14762 [39:17<17:59,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:54:11,513 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-10000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:54:11,549 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:54:12,908 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:54:12,909 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:54:12,909 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.6825, 'learning_rate': 8.679718195366482e-06, 'epoch': 1.42}         \n",
      " 71%|██████████████████████████▎          | 10500/14762 [41:15<16:05,  4.41it/s][INFO|trainer.py:2340] 2022-05-26 20:56:09,156 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-10500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:56:09,158 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:56:10,530 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:56:10,531 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:56:10,532 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.6315, 'learning_rate': 7.663595718737299e-06, 'epoch': 1.49}         \n",
      " 75%|███████████████████████████▌         | 11000/14762 [43:12<14:15,  4.40it/s][INFO|trainer.py:2340] 2022-05-26 20:58:06,697 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-11000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 20:58:06,698 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 20:58:07,994 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 20:58:07,996 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 20:58:08,035 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.659, 'learning_rate': 6.647473242108115e-06, 'epoch': 1.56}          \n",
      " 78%|████████████████████████████▊        | 11500/14762 [45:10<12:18,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 21:00:04,134 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-11500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:00:04,171 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:00:05,469 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:00:05,475 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:00:05,510 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-11500/special_tokens_map.json\n",
      " 80%|█████████████████████████████▋       | 11868/14762 [46:37<10:57,  4.40it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.6362, 'learning_rate': 5.633383010432191e-06, 'epoch': 1.63}         \n",
      " 81%|██████████████████████████████       | 12000/14762 [47:07<10:31,  4.38it/s][INFO|trainer.py:2340] 2022-05-26 21:02:01,750 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-12000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:02:01,752 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:02:02,987 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:02:02,988 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:02:02,989 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.6344, 'learning_rate': 4.617260533803008e-06, 'epoch': 1.69}         \n",
      " 85%|███████████████████████████████▎     | 12500/14762 [49:05<08:32,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 21:03:59,101 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-12500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:03:59,102 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:04:00,339 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:04:00,340 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:04:00,374 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.6629, 'learning_rate': 3.6011380571738247e-06, 'epoch': 1.76}        \n",
      " 88%|████████████████████████████████▌    | 13000/14762 [51:02<06:40,  4.40it/s][INFO|trainer.py:2340] 2022-05-26 21:05:56,590 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-13000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:05:56,591 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:05:57,981 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:05:57,985 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:05:57,985 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.6173, 'learning_rate': 2.5850155805446418e-06, 'epoch': 1.83}        \n",
      " 91%|█████████████████████████████████▊   | 13500/14762 [53:00<04:45,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 21:07:54,616 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-13500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:07:54,618 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:07:55,855 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:07:55,856 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:07:55,857 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.65, 'learning_rate': 1.5688931039154586e-06, 'epoch': 1.9}           \n",
      " 95%|███████████████████████████████████  | 14000/14762 [54:58<02:52,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 21:09:52,733 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-14000\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:09:52,735 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:09:54,271 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:09:54,343 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:09:54,390 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.612, 'learning_rate': 5.527706272862755e-07, 'epoch': 1.96}          \n",
      " 98%|████████████████████████████████████▎| 14500/14762 [56:56<00:59,  4.42it/s][INFO|trainer.py:2340] 2022-05-26 21:11:50,974 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-14500\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:11:50,975 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:11:52,246 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:11:52,250 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:11:52,286 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-14500/special_tokens_map.json\n",
      "100%|█████████████████████████████████████| 14762/14762 [58:00<00:00,  4.79it/s][INFO|trainer.py:1662] 2022-05-26 21:12:54,413 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3480.4549, 'train_samples_per_second': 50.894, 'train_steps_per_second': 4.241, 'train_loss': 0.8507382597417434, 'epoch': 2.0}\n",
      "100%|█████████████████████████████████████| 14762/14762 [58:00<00:00,  4.24it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-26 21:12:54,415 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced\n",
      "[INFO|configuration_utils.py:446] 2022-05-26 21:12:54,467 >> Configuration saved in ./squad_roberta_base_uncased_advanced/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-26 21:12:55,689 >> Model weights saved in ./squad_roberta_base_uncased_advanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-26 21:12:55,690 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-26 21:12:55,736 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.8507\n",
      "  train_runtime            = 0:58:00.45\n",
      "  train_samples            =      88568\n",
      "  train_samples_per_second =     50.894\n",
      "  train_steps_per_second   =      4.241\n",
      "05/26/2022 21:12:55 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-26 21:12:55,938 >> The following columns in the evaluation set don't have a corresponding argument in `Advanced_RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `Advanced_RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-26 21:12:55,947 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-26 21:12:55,947 >>   Num examples = 10790\n",
      "[INFO|trainer.py:2595] 2022-05-26 21:12:55,947 >>   Batch size = 12\n",
      "100%|████████████████████████████████████████▉| 898/900 [01:02<00:00, 14.38it/s]05/26/2022 21:14:06 - INFO - utils_qa - Post-processing 10570 example predictions split into 10790 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 48/10570 [00:00<00:22, 477.00it/s]\u001b[A\n",
      "  1%|▎                                     | 102/10570 [00:00<00:20, 511.50it/s]\u001b[A\n",
      "  1%|▌                                     | 157/10570 [00:00<00:19, 526.63it/s]\u001b[A\n",
      "  2%|▊                                     | 210/10570 [00:00<00:19, 525.04it/s]\u001b[A\n",
      "  2%|▉                                     | 263/10570 [00:00<00:23, 444.45it/s]\u001b[A\n",
      "  3%|█                                     | 310/10570 [00:00<00:23, 434.77it/s]\u001b[A\n",
      "  3%|█▎                                    | 367/10570 [00:00<00:21, 472.17it/s]\u001b[A\n",
      "  4%|█▍                                    | 416/10570 [00:00<00:21, 468.62it/s]\u001b[A\n",
      "  4%|█▋                                    | 472/10570 [00:00<00:20, 494.29it/s]\u001b[A\n",
      "  5%|█▉                                    | 527/10570 [00:01<00:19, 508.65it/s]\u001b[A\n",
      "  5%|██                                    | 580/10570 [00:01<00:19, 513.88it/s]\u001b[A\n",
      "  6%|██▎                                   | 632/10570 [00:01<00:19, 513.85it/s]\u001b[A\n",
      "  6%|██▍                                   | 684/10570 [00:01<00:19, 513.35it/s]\u001b[A\n",
      "  7%|██▋                                   | 736/10570 [00:01<00:19, 501.55it/s]\u001b[A\n",
      "  7%|██▊                                   | 787/10570 [00:01<00:20, 487.44it/s]\u001b[A\n",
      "  8%|███                                   | 836/10570 [00:01<00:21, 455.68it/s]\u001b[A\n",
      "  8%|███▏                                  | 888/10570 [00:01<00:20, 470.78it/s]\u001b[A\n",
      "  9%|███▎                                  | 936/10570 [00:01<00:20, 461.69it/s]\u001b[A\n",
      "  9%|███▌                                  | 985/10570 [00:02<00:20, 467.95it/s]\u001b[A\n",
      " 10%|███▌                                 | 1033/10570 [00:02<00:20, 465.78it/s]\u001b[A\n",
      " 10%|███▊                                 | 1081/10570 [00:02<00:20, 466.13it/s]\u001b[A\n",
      " 11%|███▉                                 | 1128/10570 [00:02<00:20, 466.18it/s]\u001b[A\n",
      " 11%|████▏                                | 1182/10570 [00:02<00:19, 486.78it/s]\u001b[A\n",
      " 12%|████▎                                | 1235/10570 [00:02<00:18, 499.29it/s]\u001b[A\n",
      " 12%|████▌                                | 1286/10570 [00:02<00:18, 496.70it/s]\u001b[A\n",
      " 13%|████▋                                | 1341/10570 [00:02<00:18, 510.14it/s]\u001b[A\n",
      " 13%|████▉                                | 1393/10570 [00:02<00:18, 509.12it/s]\u001b[A\n",
      " 14%|█████                                | 1445/10570 [00:02<00:17, 509.88it/s]\u001b[A\n",
      " 14%|█████▏                               | 1498/10570 [00:03<00:17, 513.12it/s]\u001b[A\n",
      " 15%|█████▍                               | 1551/10570 [00:03<00:17, 517.30it/s]\u001b[A\n",
      " 15%|█████▌                               | 1603/10570 [00:03<00:17, 516.08it/s]\u001b[A\n",
      " 16%|█████▊                               | 1655/10570 [00:03<00:17, 516.90it/s]\u001b[A\n",
      " 16%|█████▉                               | 1710/10570 [00:03<00:16, 526.38it/s]\u001b[A\n",
      " 17%|██████▏                              | 1764/10570 [00:03<00:16, 528.65it/s]\u001b[A\n",
      " 17%|██████▎                              | 1817/10570 [00:03<00:16, 522.52it/s]\u001b[A\n",
      " 18%|██████▌                              | 1870/10570 [00:03<00:16, 519.53it/s]\u001b[A\n",
      " 18%|██████▋                              | 1926/10570 [00:03<00:16, 530.44it/s]\u001b[A\n",
      " 19%|██████▉                              | 1980/10570 [00:03<00:16, 529.39it/s]\u001b[A\n",
      " 19%|███████                              | 2033/10570 [00:04<00:16, 522.31it/s]\u001b[A\n",
      " 20%|███████▎                             | 2089/10570 [00:04<00:15, 531.10it/s]\u001b[A\n",
      " 20%|███████▌                             | 2143/10570 [00:04<00:17, 494.93it/s]\u001b[A\n",
      " 21%|███████▋                             | 2194/10570 [00:04<00:16, 497.79it/s]\u001b[A\n",
      " 21%|███████▊                             | 2248/10570 [00:04<00:16, 509.38it/s]\u001b[A\n",
      " 22%|████████                             | 2301/10570 [00:04<00:16, 513.04it/s]\u001b[A\n",
      " 22%|████████▏                            | 2354/10570 [00:04<00:15, 516.70it/s]\u001b[A\n",
      " 23%|████████▍                            | 2406/10570 [00:04<00:16, 507.40it/s]\u001b[A\n",
      " 23%|████████▌                            | 2457/10570 [00:04<00:16, 499.63it/s]\u001b[A\n",
      " 24%|████████▊                            | 2508/10570 [00:05<00:16, 491.74it/s]\u001b[A\n",
      " 24%|████████▉                            | 2558/10570 [00:05<00:16, 483.34it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2609/10570 [00:05<00:16, 489.51it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2662/10570 [00:05<00:15, 500.22it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2713/10570 [00:05<00:15, 501.20it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2768/10570 [00:05<00:15, 514.65it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2822/10570 [00:05<00:14, 521.19it/s]\u001b[A\n",
      " 27%|██████████                           | 2875/10570 [00:05<00:14, 515.73it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2927/10570 [00:05<00:15, 507.81it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2978/10570 [00:05<00:15, 504.48it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3029/10570 [00:06<00:15, 499.36it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3079/10570 [00:06<00:15, 480.63it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3128/10570 [00:06<00:15, 473.81it/s]\u001b[A\n",
      " 30%|███████████                          | 3176/10570 [00:06<00:15, 473.72it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3225/10570 [00:06<00:15, 475.31it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3273/10570 [00:06<00:15, 469.03it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 900/900 [01:17<00:00, 14.38it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3374/10570 [00:06<00:14, 483.61it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3423/10570 [00:06<00:14, 483.25it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3472/10570 [00:06<00:14, 485.22it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3521/10570 [00:07<00:14, 480.23it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3570/10570 [00:07<00:14, 477.80it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3618/10570 [00:07<00:14, 467.92it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3665/10570 [00:07<00:14, 463.42it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3713/10570 [00:07<00:14, 468.01it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3761/10570 [00:07<00:14, 468.86it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3808/10570 [00:07<00:14, 467.60it/s]\u001b[A\n",
      " 36%|█████████████▌                       | 3858/10570 [00:07<00:14, 476.82it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3906/10570 [00:07<00:13, 476.89it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3956/10570 [00:08<00:13, 482.65it/s]\u001b[A\n",
      " 38%|██████████████                       | 4005/10570 [00:08<00:13, 481.18it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4054/10570 [00:08<00:13, 482.22it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4103/10570 [00:08<00:13, 472.70it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4151/10570 [00:08<00:15, 425.21it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4195/10570 [00:08<00:16, 385.57it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4235/10570 [00:08<00:16, 379.53it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4274/10570 [00:08<00:18, 332.65it/s]\u001b[A\n",
      " 41%|███████████████                      | 4309/10570 [00:09<00:21, 292.12it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4358/10570 [00:09<00:18, 337.71it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4407/10570 [00:09<00:16, 374.79it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4458/10570 [00:09<00:14, 410.29it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4507/10570 [00:09<00:14, 432.05it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4552/10570 [00:09<00:13, 433.63it/s]\u001b[A\n",
      " 43%|████████████████                     | 4597/10570 [00:09<00:13, 434.51it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4642/10570 [00:09<00:13, 433.35it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4686/10570 [00:09<00:13, 430.37it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4736/10570 [00:09<00:12, 449.92it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4782/10570 [00:10<00:13, 433.70it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4833/10570 [00:10<00:12, 453.74it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4879/10570 [00:10<00:12, 440.60it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4929/10570 [00:10<00:12, 456.23it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4978/10570 [00:10<00:12, 464.07it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 5025/10570 [00:10<00:12, 453.77it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5074/10570 [00:10<00:11, 463.18it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5122/10570 [00:10<00:11, 464.56it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5169/10570 [00:10<00:11, 459.34it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5216/10570 [00:11<00:11, 457.40it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5262/10570 [00:11<00:11, 457.19it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5314/10570 [00:11<00:11, 472.93it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5363/10570 [00:11<00:10, 476.92it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5413/10570 [00:11<00:10, 482.92it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5462/10570 [00:11<00:11, 457.83it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5509/10570 [00:11<00:11, 437.41it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5555/10570 [00:11<00:11, 443.00it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5601/10570 [00:11<00:11, 445.12it/s]\u001b[A\n",
      " 53%|███████████████████▊                 | 5646/10570 [00:11<00:11, 439.02it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5691/10570 [00:12<00:11, 433.53it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5737/10570 [00:12<00:11, 438.49it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5781/10570 [00:12<00:10, 437.08it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5827/10570 [00:12<00:10, 441.99it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5876/10570 [00:12<00:10, 455.01it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5925/10570 [00:12<00:10, 463.81it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5975/10570 [00:12<00:09, 471.48it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6023/10570 [00:12<00:09, 471.55it/s]\u001b[A\n",
      " 57%|█████████████████████▎               | 6071/10570 [00:12<00:09, 463.86it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6119/10570 [00:12<00:09, 466.76it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6168/10570 [00:13<00:09, 471.88it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6216/10570 [00:13<00:09, 471.02it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6264/10570 [00:13<00:09, 451.80it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6315/10570 [00:13<00:09, 466.68it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6364/10570 [00:13<00:08, 472.08it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6412/10570 [00:13<00:09, 457.63it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6464/10570 [00:13<00:08, 473.86it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6512/10570 [00:13<00:08, 474.92it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6560/10570 [00:13<00:08, 476.11it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6608/10570 [00:14<00:08, 476.42it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6658/10570 [00:14<00:08, 479.88it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6707/10570 [00:14<00:08, 473.49it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6755/10570 [00:14<00:08, 475.18it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6804/10570 [00:14<00:07, 476.92it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6852/10570 [00:14<00:07, 476.08it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6900/10570 [00:14<00:07, 471.17it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6949/10570 [00:14<00:07, 476.49it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6997/10570 [00:14<00:07, 470.17it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7045/10570 [00:14<00:07, 460.58it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7092/10570 [00:15<00:07, 449.24it/s]\u001b[A\n",
      " 68%|████████████████████████▉            | 7138/10570 [00:15<00:07, 449.43it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7185/10570 [00:15<00:07, 454.03it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7231/10570 [00:15<00:07, 439.27it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7278/10570 [00:15<00:07, 445.90it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7328/10570 [00:15<00:07, 459.07it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7375/10570 [00:15<00:07, 437.16it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7423/10570 [00:15<00:07, 447.77it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7471/10570 [00:15<00:06, 454.22it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7522/10570 [00:16<00:06, 468.90it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7572/10570 [00:16<00:06, 475.95it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7623/10570 [00:16<00:06, 484.12it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7672/10570 [00:16<00:06, 468.12it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7723/10570 [00:16<00:05, 479.36it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7772/10570 [00:16<00:06, 463.98it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7821/10570 [00:16<00:05, 469.15it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7870/10570 [00:16<00:05, 472.78it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7920/10570 [00:16<00:05, 478.52it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7968/10570 [00:16<00:05, 477.53it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8016/10570 [00:17<00:05, 476.25it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8064/10570 [00:17<00:05, 458.95it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8115/10570 [00:17<00:05, 473.41it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8163/10570 [00:17<00:05, 454.01it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8209/10570 [00:17<00:05, 444.07it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8255/10570 [00:17<00:05, 448.12it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8300/10570 [00:17<00:05, 440.59it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8349/10570 [00:17<00:04, 452.92it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8399/10570 [00:17<00:04, 465.69it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8449/10570 [00:18<00:04, 474.53it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8498/10570 [00:18<00:04, 478.80it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8548/10570 [00:18<00:04, 482.57it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8597/10570 [00:18<00:04, 475.96it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8645/10570 [00:18<00:04, 476.77it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8694/10570 [00:18<00:03, 480.66it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8743/10570 [00:18<00:03, 480.12it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8792/10570 [00:18<00:03, 471.95it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8840/10570 [00:18<00:03, 464.03it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8887/10570 [00:18<00:03, 463.10it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8934/10570 [00:19<00:03, 461.09it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8981/10570 [00:19<00:03, 457.78it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9027/10570 [00:19<00:03, 457.32it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9073/10570 [00:19<00:03, 457.57it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9122/10570 [00:19<00:03, 465.34it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9170/10570 [00:19<00:02, 467.39it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9218/10570 [00:19<00:02, 470.24it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9268/10570 [00:19<00:02, 478.02it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9316/10570 [00:19<00:02, 471.17it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9364/10570 [00:19<00:02, 473.27it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9412/10570 [00:20<00:02, 467.80it/s]\u001b[A\n",
      " 90%|█████████████████████████████████    | 9461/10570 [00:20<00:02, 473.27it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9509/10570 [00:20<00:02, 471.74it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9560/10570 [00:20<00:02, 480.42it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9609/10570 [00:20<00:02, 480.26it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9660/10570 [00:20<00:01, 487.26it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9713/10570 [00:20<00:01, 497.24it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9763/10570 [00:20<00:01, 486.05it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9812/10570 [00:20<00:01, 481.30it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9861/10570 [00:20<00:01, 475.22it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9909/10570 [00:21<00:01, 469.39it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9959/10570 [00:21<00:01, 478.09it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10007/10570 [00:21<00:01, 477.06it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10056/10570 [00:21<00:01, 480.04it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10105/10570 [00:21<00:00, 472.52it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10153/10570 [00:21<00:00, 459.35it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10200/10570 [00:21<00:00, 460.38it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10249/10570 [00:21<00:00, 468.49it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10296/10570 [00:21<00:00, 468.89it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10345/10570 [00:22<00:00, 472.23it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10393/10570 [00:22<00:00, 460.21it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10440/10570 [00:22<00:00, 454.66it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10486/10570 [00:22<00:00, 453.49it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:22<00:00, 469.89it/s]\u001b[A\n",
      "05/26/2022 21:14:29 - INFO - utils_qa - Saving predictions to ./squad_roberta_base_uncased_advanced/eval_predictions.json.\n",
      "05/26/2022 21:14:29 - INFO - utils_qa - Saving nbest_preds to ./squad_roberta_base_uncased_advanced/eval_nbest_predictions.json.\n",
      "05/26/2022 21:14:33 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████| 900/900 [01:37<00:00,  9.20it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     2.0\n",
      "  eval_exact_match = 85.7048\n",
      "  eval_f1          = 92.0786\n",
      "  eval_samples     =   10790\n"
     ]
    }
   ],
   "source": [
    "!sh train_roberta_squadv1_advanced.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

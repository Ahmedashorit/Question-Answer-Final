{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5e1999a-d269-47db-9f66-4b906d3ef0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b16a0e8d-77cb-4e14-b408-e8afaf83b10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==1.9.0 in /opt/conda/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8168454-df69-4a8a-84d2-c537923b3023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c30051e-f651-44c0-89df-0357f1a2b1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/23/2022 20:12:57 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "05/23/2022 20:12:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./debug_squad/runs/May23_20-12-56_ntxansm7ab,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./debug_squad/,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./debug_squad/,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpk91j0z5f\n",
      "Downloading builder script: 5.27kB [00:00, 5.47MB/s]                            \n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgkri9xmy\n",
      "Downloading metadata: 2.36kB [00:00, 2.12MB/s]                                  \n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/23/2022 20:12:57 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/23/2022 20:12:57 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/23/2022 20:12:57 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/23/2022 20:12:57 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "05/23/2022 20:12:57 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmphcc275gg\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data: 8.81MB [00:00, 88.1MB/s]                                      \u001b[A\n",
      "Downloading data: 17.7MB [00:00, 88.7MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 83.4MB/s]\u001b[A\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.13s/it]05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpuhqg820v\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 94.1MB/s]                                      \u001b[A\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.50it/s]\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1805.55it/s]\n",
      "05/23/2022 20:12:58 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "05/23/2022 20:12:58 - INFO - datasets.builder - Generating train split\n",
      "05/23/2022 20:13:05 - INFO - datasets.builder - Generating validation split     \n",
      "05/23/2022 20:13:06 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 585.02it/s]\n",
      "[INFO|hub.py:583] 2022-05-23 20:13:06,102 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpoq1m9bib\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 661kB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 20:13:06,173 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|hub.py:595] 2022-05-23 20:13:06,173 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 20:13:06,174 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 20:13:06,174 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 20:13:06,233 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpuy2pzefz\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 26.6kB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 20:13:06,321 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|hub.py:595] 2022-05-23 20:13:06,321 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 20:13:06,380 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 20:13:06,381 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 20:13:06,528 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmprdr9qx8m\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 7.27MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 20:13:06,625 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:595] 2022-05-23 20:13:06,626 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:583] 2022-05-23 20:13:06,685 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps3p31yig\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 5.88MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 20:13:06,828 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|hub.py:595] 2022-05-23 20:13:06,829 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 20:13:07,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 20:13:07,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 20:13:07,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 20:13:07,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-23 20:13:07,012 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-23 20:13:07,071 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-23 20:13:07,071 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-23 20:13:07,191 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpok63y923\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [00:18<00:00, 23.3MB/s]\n",
      "[INFO|hub.py:587] 2022-05-23 20:13:26,237 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|hub.py:595] 2022-05-23 20:13:26,237 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1953] 2022-05-23 20:13:26,237 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-23 20:13:27,703 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-23 20:13:27,703 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/23/2022 20:13:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-1d1db2aa4978ff8d.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:35<00:00,  2.45ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/23/2022 20:14:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-f9f3f6120696991b.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:34<00:00,  3.12s/ba]\n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp3h4v7kdm\n",
      "Downloading builder script: 4.50kB [00:00, 3.86MB/s]                            \n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpquek0vhr\n",
      "Downloading extra modules: 3.31kB [00:00, 3.10MB/s]                             \n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "05/23/2022 20:14:38 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-23 20:14:43,082 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-23 20:14:43,083 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1421] 2022-05-23 20:14:43,083 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1422] 2022-05-23 20:14:43,083 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-23 20:14:43,083 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-23 20:14:43,083 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-23 20:14:43,083 >>   Total optimization steps = 7377\n",
      "{'loss': 2.4314, 'learning_rate': 2.796665311102074e-05, 'epoch': 0.07}         \n",
      "  7%|██▋                                     | 500/7377 [03:27<47:39,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:18:10,199 >> Saving model checkpoint to ./debug_squad/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:18:10,233 >> Configuration saved in ./debug_squad/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:18:14,079 >> Model weights saved in ./debug_squad/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:18:14,088 >> tokenizer config file saved in ./debug_squad/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:18:14,114 >> Special tokens file saved in ./debug_squad/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.4946, 'learning_rate': 2.593330622204148e-05, 'epoch': 0.14}         \n",
      " 14%|█████▎                                 | 1000/7377 [07:07<44:08,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:21:50,249 >> Saving model checkpoint to ./debug_squad/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:21:50,284 >> Configuration saved in ./debug_squad/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:21:54,119 >> Model weights saved in ./debug_squad/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:21:54,155 >> tokenizer config file saved in ./debug_squad/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:21:54,164 >> Special tokens file saved in ./debug_squad/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3888, 'learning_rate': 2.389995933306222e-05, 'epoch': 0.2}          \n",
      " 20%|███████▉                               | 1500/7377 [10:45<40:58,  2.39it/s][INFO|trainer.py:2340] 2022-05-23 20:25:28,405 >> Saving model checkpoint to ./debug_squad/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:25:28,462 >> Configuration saved in ./debug_squad/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:25:32,254 >> Model weights saved in ./debug_squad/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:25:32,257 >> tokenizer config file saved in ./debug_squad/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:25:32,290 >> Special tokens file saved in ./debug_squad/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.3311, 'learning_rate': 2.1866612444082963e-05, 'epoch': 0.27}        \n",
      " 27%|██████████▌                            | 2000/7377 [14:23<37:26,  2.39it/s][INFO|trainer.py:2340] 2022-05-23 20:29:06,826 >> Saving model checkpoint to ./debug_squad/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:29:06,837 >> Configuration saved in ./debug_squad/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:29:10,184 >> Model weights saved in ./debug_squad/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:29:10,189 >> tokenizer config file saved in ./debug_squad/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:29:10,196 >> Special tokens file saved in ./debug_squad/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2343, 'learning_rate': 1.9833265555103702e-05, 'epoch': 0.34}        \n",
      " 34%|█████████████▏                         | 2500/7377 [18:02<33:43,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:32:45,559 >> Saving model checkpoint to ./debug_squad/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:32:45,569 >> Configuration saved in ./debug_squad/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:32:49,374 >> Model weights saved in ./debug_squad/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:32:49,377 >> tokenizer config file saved in ./debug_squad/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:32:49,380 >> Special tokens file saved in ./debug_squad/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1892, 'learning_rate': 1.779991866612444e-05, 'epoch': 0.41}         \n",
      " 41%|███████████████▊                       | 3000/7377 [21:41<30:20,  2.40it/s][INFO|trainer.py:2340] 2022-05-23 20:36:24,614 >> Saving model checkpoint to ./debug_squad/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:36:24,648 >> Configuration saved in ./debug_squad/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:36:28,006 >> Model weights saved in ./debug_squad/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:36:28,009 >> tokenizer config file saved in ./debug_squad/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:36:28,041 >> Special tokens file saved in ./debug_squad/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.19, 'learning_rate': 1.576657177714518e-05, 'epoch': 0.47}           \n",
      " 47%|██████████████████▌                    | 3500/7377 [25:19<26:54,  2.40it/s][INFO|trainer.py:2340] 2022-05-23 20:40:02,185 >> Saving model checkpoint to ./debug_squad/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:40:02,191 >> Configuration saved in ./debug_squad/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:40:05,898 >> Model weights saved in ./debug_squad/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:40:05,902 >> tokenizer config file saved in ./debug_squad/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:40:05,934 >> Special tokens file saved in ./debug_squad/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1318, 'learning_rate': 1.373322488816592e-05, 'epoch': 0.54}         \n",
      " 54%|█████████████████████▏                 | 4000/7377 [28:57<23:23,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:43:40,629 >> Saving model checkpoint to ./debug_squad/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:43:40,637 >> Configuration saved in ./debug_squad/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:43:44,380 >> Model weights saved in ./debug_squad/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:43:44,385 >> tokenizer config file saved in ./debug_squad/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:43:44,387 >> Special tokens file saved in ./debug_squad/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0678, 'learning_rate': 1.1699877999186661e-05, 'epoch': 0.61}        \n",
      " 61%|███████████████████████▊               | 4500/7377 [32:35<19:55,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:47:19,062 >> Saving model checkpoint to ./debug_squad/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:47:19,067 >> Configuration saved in ./debug_squad/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:47:22,535 >> Model weights saved in ./debug_squad/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:47:22,540 >> tokenizer config file saved in ./debug_squad/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:47:22,543 >> Special tokens file saved in ./debug_squad/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.0696, 'learning_rate': 9.666531110207402e-06, 'epoch': 0.68}         \n",
      " 68%|██████████████████████████▍            | 5000/7377 [36:13<16:27,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:50:56,959 >> Saving model checkpoint to ./debug_squad/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:50:56,965 >> Configuration saved in ./debug_squad/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:51:00,236 >> Model weights saved in ./debug_squad/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:51:00,241 >> tokenizer config file saved in ./debug_squad/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:51:00,272 >> Special tokens file saved in ./debug_squad/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.0999, 'learning_rate': 7.633184221228141e-06, 'epoch': 0.75}         \n",
      " 75%|█████████████████████████████          | 5500/7377 [39:51<13:00,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:54:34,627 >> Saving model checkpoint to ./debug_squad/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:54:34,663 >> Configuration saved in ./debug_squad/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:54:37,901 >> Model weights saved in ./debug_squad/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:54:37,905 >> tokenizer config file saved in ./debug_squad/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:54:37,937 >> Special tokens file saved in ./debug_squad/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0582, 'learning_rate': 5.5998373322488825e-06, 'epoch': 0.81}        \n",
      " 81%|███████████████████████████████▋       | 6000/7377 [43:28<09:32,  2.41it/s][INFO|trainer.py:2340] 2022-05-23 20:58:11,821 >> Saving model checkpoint to ./debug_squad/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 20:58:11,883 >> Configuration saved in ./debug_squad/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 20:58:16,341 >> Model weights saved in ./debug_squad/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 20:58:16,345 >> tokenizer config file saved in ./debug_squad/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 20:58:16,399 >> Special tokens file saved in ./debug_squad/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0164, 'learning_rate': 3.566490443269622e-06, 'epoch': 0.88}         \n",
      " 88%|██████████████████████████████████▎    | 6500/7377 [47:09<06:07,  2.39it/s][INFO|trainer.py:2340] 2022-05-23 21:01:53,244 >> Saving model checkpoint to ./debug_squad/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 21:01:53,253 >> Configuration saved in ./debug_squad/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 21:01:57,412 >> Model weights saved in ./debug_squad/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 21:01:57,416 >> tokenizer config file saved in ./debug_squad/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 21:01:57,424 >> Special tokens file saved in ./debug_squad/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0406, 'learning_rate': 1.533143554290362e-06, 'epoch': 0.95}         \n",
      " 95%|█████████████████████████████████████  | 7000/7377 [50:50<02:36,  2.40it/s][INFO|trainer.py:2340] 2022-05-23 21:05:33,377 >> Saving model checkpoint to ./debug_squad/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 21:05:33,388 >> Configuration saved in ./debug_squad/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 21:05:37,177 >> Model weights saved in ./debug_squad/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 21:05:37,194 >> tokenizer config file saved in ./debug_squad/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 21:05:37,198 >> Special tokens file saved in ./debug_squad/checkpoint-7000/special_tokens_map.json\n",
      "100%|███████████████████████████████████████| 7377/7377 [53:37<00:00,  2.39it/s][INFO|trainer.py:1662] 2022-05-23 21:08:20,680 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 3217.5984, 'train_samples_per_second': 27.512, 'train_steps_per_second': 2.293, 'train_loss': 1.2546662026439335, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████████| 7377/7377 [53:37<00:00,  2.29it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-23 21:08:20,970 >> Saving model checkpoint to ./debug_squad/\n",
      "[INFO|configuration_utils.py:446] 2022-05-23 21:08:20,985 >> Configuration saved in ./debug_squad/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-23 21:08:24,731 >> Model weights saved in ./debug_squad/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-23 21:08:24,752 >> tokenizer config file saved in ./debug_squad/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-23 21:08:24,756 >> Special tokens file saved in ./debug_squad/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.2547\n",
      "  train_runtime            = 0:53:37.59\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     27.512\n",
      "  train_steps_per_second   =      2.293\n",
      "05/23/2022 21:08:24 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-23 21:08:24,864 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-23 21:08:24,868 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-23 21:08:24,868 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2595] 2022-05-23 21:08:24,868 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:11<00:00, 10.29it/s]05/23/2022 21:10:45 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 42/10570 [00:00<00:25, 415.89it/s]\u001b[A\n",
      "  1%|▎                                      | 90/10570 [00:00<00:23, 453.20it/s]\u001b[A\n",
      "  1%|▌                                     | 142/10570 [00:00<00:21, 479.62it/s]\u001b[A\n",
      "  2%|▋                                     | 190/10570 [00:00<00:21, 478.72it/s]\u001b[A\n",
      "  2%|▊                                     | 238/10570 [00:00<00:23, 445.23it/s]\u001b[A\n",
      "  3%|█                                     | 283/10570 [00:00<00:27, 371.68it/s]\u001b[A\n",
      "  3%|█▏                                    | 331/10570 [00:00<00:25, 400.50it/s]\u001b[A\n",
      "  4%|█▎                                    | 382/10570 [00:00<00:23, 430.30it/s]\u001b[A\n",
      "  4%|█▌                                    | 428/10570 [00:00<00:23, 438.29it/s]\u001b[A\n",
      "  4%|█▋                                    | 475/10570 [00:01<00:22, 447.25it/s]\u001b[A\n",
      "  5%|█▉                                    | 523/10570 [00:01<00:22, 456.50it/s]\u001b[A\n",
      "  5%|██                                    | 571/10570 [00:01<00:21, 461.05it/s]\u001b[A\n",
      "  6%|██▏                                   | 619/10570 [00:01<00:21, 465.03it/s]\u001b[A\n",
      "  6%|██▍                                   | 667/10570 [00:01<00:21, 468.80it/s]\u001b[A\n",
      "  7%|██▌                                   | 715/10570 [00:01<00:20, 469.75it/s]\u001b[A\n",
      "  7%|██▋                                   | 763/10570 [00:01<00:21, 457.59it/s]\u001b[A\n",
      "  8%|██▉                                   | 809/10570 [00:01<00:22, 439.86it/s]\u001b[A\n",
      "  8%|███                                   | 854/10570 [00:01<00:22, 441.16it/s]\u001b[A\n",
      "  9%|███▏                                  | 901/10570 [00:02<00:21, 449.04it/s]\u001b[A\n",
      "  9%|███▍                                  | 947/10570 [00:02<00:21, 443.63it/s]\u001b[A\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:22<00:00, 10.29it/s]\u001b[A\n",
      " 10%|███▋                                 | 1037/10570 [00:02<00:21, 438.41it/s]\u001b[A\n",
      " 10%|███▊                                 | 1082/10570 [00:02<00:21, 440.22it/s]\u001b[A\n",
      " 11%|███▉                                 | 1127/10570 [00:02<00:21, 435.12it/s]\u001b[A\n",
      " 11%|████                                 | 1174/10570 [00:02<00:21, 445.01it/s]\u001b[A\n",
      " 12%|████▎                                | 1219/10570 [00:02<00:21, 441.32it/s]\u001b[A\n",
      " 12%|████▍                                | 1264/10570 [00:02<00:21, 433.75it/s]\u001b[A\n",
      " 12%|████▌                                | 1312/10570 [00:02<00:20, 446.80it/s]\u001b[A\n",
      " 13%|████▊                                | 1360/10570 [00:03<00:20, 455.07it/s]\u001b[A\n",
      " 13%|████▉                                | 1406/10570 [00:03<00:20, 443.86it/s]\u001b[A\n",
      " 14%|█████                                | 1451/10570 [00:03<00:20, 444.87it/s]\u001b[A\n",
      " 14%|█████▏                               | 1499/10570 [00:03<00:20, 452.47it/s]\u001b[A\n",
      " 15%|█████▍                               | 1547/10570 [00:03<00:19, 458.06it/s]\u001b[A\n",
      " 15%|█████▌                               | 1595/10570 [00:03<00:19, 462.90it/s]\u001b[A\n",
      " 16%|█████▊                               | 1643/10570 [00:03<00:19, 467.32it/s]\u001b[A\n",
      " 16%|█████▉                               | 1694/10570 [00:03<00:18, 479.50it/s]\u001b[A\n",
      " 16%|██████                               | 1742/10570 [00:03<00:18, 476.28it/s]\u001b[A\n",
      " 17%|██████▎                              | 1791/10570 [00:03<00:18, 478.93it/s]\u001b[A\n",
      " 17%|██████▍                              | 1839/10570 [00:04<00:18, 474.30it/s]\u001b[A\n",
      " 18%|██████▌                              | 1888/10570 [00:04<00:18, 478.49it/s]\u001b[A\n",
      " 18%|██████▊                              | 1938/10570 [00:04<00:17, 482.67it/s]\u001b[A\n",
      " 19%|██████▉                              | 1988/10570 [00:04<00:17, 485.40it/s]\u001b[A\n",
      " 19%|███████▏                             | 2037/10570 [00:04<00:17, 476.89it/s]\u001b[A\n",
      " 20%|███████▎                             | 2087/10570 [00:04<00:17, 482.17it/s]\u001b[A\n",
      " 20%|███████▍                             | 2136/10570 [00:04<00:18, 445.38it/s]\u001b[A\n",
      " 21%|███████▋                             | 2182/10570 [00:04<00:18, 446.45it/s]\u001b[A\n",
      " 21%|███████▊                             | 2232/10570 [00:04<00:18, 459.41it/s]\u001b[A\n",
      " 22%|███████▉                             | 2279/10570 [00:05<00:18, 457.00it/s]\u001b[A\n",
      " 22%|████████▏                            | 2327/10570 [00:05<00:17, 461.52it/s]\u001b[A\n",
      " 22%|████████▎                            | 2374/10570 [00:05<00:17, 461.52it/s]\u001b[A\n",
      " 23%|████████▍                            | 2421/10570 [00:05<00:17, 456.21it/s]\u001b[A\n",
      " 23%|████████▋                            | 2467/10570 [00:05<00:17, 452.34it/s]\u001b[A\n",
      " 24%|████████▊                            | 2513/10570 [00:05<00:18, 435.04it/s]\u001b[A\n",
      " 24%|████████▉                            | 2557/10570 [00:05<00:18, 429.33it/s]\u001b[A\n",
      " 25%|█████████                            | 2602/10570 [00:05<00:18, 434.53it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2651/10570 [00:05<00:17, 450.12it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2697/10570 [00:05<00:17, 452.98it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2745/10570 [00:06<00:16, 460.93it/s]\u001b[A\n",
      " 26%|█████████▊                           | 2797/10570 [00:06<00:16, 475.61it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2845/10570 [00:06<00:16, 473.33it/s]\u001b[A\n",
      " 27%|██████████▏                          | 2893/10570 [00:06<00:16, 464.61it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2940/10570 [00:06<00:16, 464.72it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2987/10570 [00:06<00:16, 460.26it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3034/10570 [00:06<00:16, 462.59it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3081/10570 [00:06<00:16, 457.50it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3127/10570 [00:06<00:16, 457.44it/s]\u001b[A\n",
      " 30%|███████████                          | 3173/10570 [00:06<00:16, 451.25it/s]\u001b[A\n",
      " 30%|███████████▎                         | 3219/10570 [00:07<00:16, 448.60it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3264/10570 [00:07<00:16, 444.23it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3310/10570 [00:07<00:16, 447.55it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3357/10570 [00:07<00:15, 451.49it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3403/10570 [00:07<00:15, 452.52it/s]\u001b[A\n",
      " 33%|████████████                         | 3449/10570 [00:07<00:15, 451.56it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3495/10570 [00:07<00:15, 447.62it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3542/10570 [00:07<00:15, 451.76it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3588/10570 [00:07<00:15, 447.37it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3634/10570 [00:08<00:15, 449.82it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3679/10570 [00:08<00:15, 449.14it/s]\u001b[A\n",
      " 35%|█████████████                        | 3724/10570 [00:08<00:15, 449.16it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3770/10570 [00:08<00:15, 450.14it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3816/10570 [00:08<00:15, 448.45it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3862/10570 [00:08<00:14, 451.22it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3908/10570 [00:08<00:14, 450.38it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3954/10570 [00:08<00:14, 450.81it/s]\u001b[A\n",
      " 38%|██████████████                       | 4000/10570 [00:08<00:14, 451.84it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4046/10570 [00:08<00:14, 452.67it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4093/10570 [00:09<00:14, 454.78it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4139/10570 [00:09<00:14, 430.83it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4183/10570 [00:09<00:17, 357.03it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4221/10570 [00:09<00:19, 329.20it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4259/10570 [00:09<00:18, 341.27it/s]\u001b[A\n",
      " 41%|███████████████                      | 4295/10570 [00:09<00:22, 273.03it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4326/10570 [00:09<00:22, 272.45it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4372/10570 [00:09<00:19, 315.93it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4417/10570 [00:10<00:17, 348.67it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4463/10570 [00:10<00:16, 377.36it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4505/10570 [00:10<00:15, 388.01it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4546/10570 [00:10<00:15, 390.42it/s]\u001b[A\n",
      " 43%|████████████████                     | 4589/10570 [00:10<00:14, 401.19it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4630/10570 [00:10<00:15, 393.59it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4670/10570 [00:10<00:15, 382.38it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4716/10570 [00:10<00:14, 402.70it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4757/10570 [00:10<00:15, 382.06it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4796/10570 [00:11<00:15, 377.69it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4843/10570 [00:11<00:14, 401.68it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4884/10570 [00:11<00:14, 387.67it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4929/10570 [00:11<00:13, 403.36it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4973/10570 [00:11<00:13, 411.29it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5015/10570 [00:11<00:13, 407.97it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5059/10570 [00:11<00:13, 416.07it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5101/10570 [00:11<00:13, 416.47it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5145/10570 [00:11<00:12, 421.30it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5188/10570 [00:11<00:12, 420.20it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5233/10570 [00:12<00:12, 427.99it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5277/10570 [00:12<00:12, 429.68it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 5323/10570 [00:12<00:12, 436.09it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5367/10570 [00:12<00:11, 435.99it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5412/10570 [00:12<00:11, 438.31it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5456/10570 [00:12<00:12, 415.24it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5498/10570 [00:12<00:12, 395.88it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 5538/10570 [00:12<00:13, 386.42it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5580/10570 [00:12<00:12, 395.62it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5623/10570 [00:13<00:12, 402.84it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5664/10570 [00:13<00:12, 396.22it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5709/10570 [00:13<00:11, 411.32it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 5751/10570 [00:13<00:11, 412.86it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5793/10570 [00:13<00:11, 412.34it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5837/10570 [00:13<00:11, 418.94it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5880/10570 [00:13<00:11, 421.83it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5923/10570 [00:13<00:11, 421.28it/s]\u001b[A\n",
      " 56%|████████████████████▉                | 5967/10570 [00:13<00:10, 425.76it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6010/10570 [00:13<00:10, 423.00it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6053/10570 [00:14<00:10, 421.00it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6096/10570 [00:14<00:10, 414.38it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6140/10570 [00:14<00:10, 421.83it/s]\u001b[A\n",
      " 58%|█████████████████████▋               | 6183/10570 [00:14<00:10, 417.54it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6225/10570 [00:14<00:10, 416.11it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6267/10570 [00:14<00:10, 410.51it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6313/10570 [00:14<00:10, 423.28it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6357/10570 [00:14<00:09, 426.81it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6400/10570 [00:14<00:10, 415.02it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6447/10570 [00:14<00:09, 429.03it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6492/10570 [00:15<00:09, 433.66it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6537/10570 [00:15<00:09, 436.00it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6581/10570 [00:15<00:09, 435.49it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6625/10570 [00:15<00:09, 435.71it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6671/10570 [00:15<00:08, 440.38it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6716/10570 [00:15<00:09, 425.54it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6761/10570 [00:15<00:08, 430.83it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6805/10570 [00:15<00:08, 428.41it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6848/10570 [00:15<00:08, 423.51it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6892/10570 [00:16<00:08, 426.17it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6935/10570 [00:16<00:08, 426.91it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6978/10570 [00:16<00:08, 419.46it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7020/10570 [00:16<00:08, 418.91it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7062/10570 [00:16<00:08, 410.78it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7104/10570 [00:16<00:08, 405.79it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7146/10570 [00:16<00:08, 407.60it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7187/10570 [00:16<00:08, 403.68it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7228/10570 [00:16<00:08, 394.20it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7268/10570 [00:16<00:08, 389.35it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7308/10570 [00:17<00:08, 390.73it/s]\u001b[A\n",
      " 70%|█████████████████████████▋           | 7348/10570 [00:17<00:09, 355.28it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7388/10570 [00:17<00:08, 364.68it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7429/10570 [00:17<00:08, 376.62it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7468/10570 [00:17<00:08, 376.68it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7509/10570 [00:17<00:07, 383.87it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7549/10570 [00:17<00:07, 387.11it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7590/10570 [00:17<00:07, 391.52it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7630/10570 [00:17<00:07, 393.66it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7670/10570 [00:18<00:07, 376.01it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7710/10570 [00:18<00:07, 379.93it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7750/10570 [00:18<00:07, 384.05it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7792/10570 [00:18<00:07, 392.37it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7834/10570 [00:18<00:06, 399.24it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7878/10570 [00:18<00:06, 410.07it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7924/10570 [00:18<00:06, 422.69it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7967/10570 [00:18<00:06, 423.84it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8010/10570 [00:18<00:06, 420.50it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8053/10570 [00:18<00:06, 418.45it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8099/10570 [00:19<00:05, 429.34it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8143/10570 [00:19<00:05, 429.69it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 8186/10570 [00:19<00:05, 420.41it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8229/10570 [00:19<00:05, 416.11it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8271/10570 [00:19<00:05, 412.98it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8313/10570 [00:19<00:05, 404.56it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8357/10570 [00:19<00:05, 412.22it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8402/10570 [00:19<00:05, 422.05it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8448/10570 [00:19<00:04, 432.61it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8494/10570 [00:19<00:04, 438.49it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8540/10570 [00:20<00:04, 442.48it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8585/10570 [00:20<00:04, 430.21it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8630/10570 [00:20<00:04, 433.58it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8674/10570 [00:20<00:04, 427.36it/s]\u001b[A\n",
      " 82%|██████████████████████████████▌      | 8718/10570 [00:20<00:04, 427.82it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8761/10570 [00:20<00:04, 422.19it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8804/10570 [00:20<00:04, 418.23it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8847/10570 [00:20<00:04, 421.53it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8891/10570 [00:20<00:03, 424.99it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8935/10570 [00:21<00:03, 428.93it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8979/10570 [00:21<00:03, 430.09it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9023/10570 [00:21<00:03, 429.56it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9066/10570 [00:21<00:03, 427.80it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9109/10570 [00:21<00:03, 424.96it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9152/10570 [00:21<00:03, 415.40it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9196/10570 [00:21<00:03, 420.12it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9240/10570 [00:21<00:03, 425.37it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9284/10570 [00:21<00:03, 427.01it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9327/10570 [00:21<00:02, 425.48it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9371/10570 [00:22<00:02, 428.71it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9416/10570 [00:22<00:02, 432.37it/s]\u001b[A\n",
      " 90%|█████████████████████████████████    | 9461/10570 [00:22<00:02, 436.27it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9505/10570 [00:22<00:02, 430.14it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9553/10570 [00:22<00:02, 442.74it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9598/10570 [00:22<00:02, 435.08it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9645/10570 [00:22<00:02, 443.31it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9692/10570 [00:22<00:01, 448.69it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9739/10570 [00:22<00:01, 454.52it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9785/10570 [00:22<00:01, 450.73it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9831/10570 [00:23<00:01, 448.06it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9876/10570 [00:23<00:01, 444.73it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9921/10570 [00:23<00:01, 444.18it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9967/10570 [00:23<00:01, 445.48it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10012/10570 [00:23<00:01, 443.18it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10058/10570 [00:23<00:01, 446.36it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10103/10570 [00:23<00:01, 441.34it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10149/10570 [00:23<00:00, 443.94it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10194/10570 [00:23<00:00, 433.05it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10240/10570 [00:24<00:00, 438.84it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10285/10570 [00:24<00:00, 439.78it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10330/10570 [00:24<00:00, 438.54it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10375/10570 [00:24<00:00, 439.70it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▍| 10419/10570 [00:24<00:00, 437.98it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10463/10570 [00:24<00:00, 432.97it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10508/10570 [00:24<00:00, 437.52it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:24<00:00, 426.91it/s]\u001b[A\n",
      "05/23/2022 21:11:10 - INFO - utils_qa - Saving predictions to ./debug_squad/eval_predictions.json.\n",
      "05/23/2022 21:11:10 - INFO - utils_qa - Saving nbest_preds to ./debug_squad/eval_nbest_predictions.json.\n",
      "05/23/2022 21:11:15 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1348/1348 [02:50<00:00,  7.92it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match = 80.1798\n",
      "  eval_f1          = 87.5745\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

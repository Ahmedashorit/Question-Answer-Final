{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60006a9-672a-4f45-a693-346ddc9a3604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 24.8 MB/s eta 0:00:01��██▎     | 682.7 MB 24.6 MB/s eta 0:00:07�█▋ | 796.6 MB 24.8 MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dd655ae-5213-4a4d-8b01-a9049d0943cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 19.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 31.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 53.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 29.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 29.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 35.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 39.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 29.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.1.0 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.5.1 multiprocess-0.70.12.2 responses-0.18.0 tokenizers-0.12.1 transformers-4.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44215626-4aa5-4cf9-bead-6967fcfc3c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89ed1e3-f13e-4996-a82e-22f3abb3e4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28/2022 08:33:25 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/28/2022 08:33:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_roberta_base/runs/Apr28_08-33-25_nhodxf86t7,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_roberta_base,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=3,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_roberta_base,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/28/2022 08:33:25 - INFO - __main__ - Checkpoint detected, resuming training at ./squad_roberta_base/checkpoint-21000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
      "04/28/2022 08:33:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpmpuaz7a_\n",
      "Downloading builder script: 5.27kB [00:00, 3.65MB/s]                            \n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpp0svecpp\n",
      "Downloading metadata: 2.36kB [00:00, 1.80MB/s]                                  \n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/28/2022 08:33:26 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\n",
      "04/28/2022 08:33:26 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "04/28/2022 08:33:26 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "04/28/2022 08:33:26 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpcsuf0of1\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  81%|████████████████▏   | 6.55M/8.12M [00:00<00:00, 65.5MB/s]\u001b[A\n",
      "Downloading data: 13.1MB [00:00, 62.2MB/s]                                      \u001b[A\n",
      "Downloading data: 19.3MB [00:00, 62.1MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 63.1MB/s]\u001b[A\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:00<00:00,  1.76it/s]04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvtyvwbkf\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 72.3MB/s]                                      \u001b[A\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:00<00:00,  2.84it/s]\n",
      "04/28/2022 08:33:26 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "04/28/2022 08:33:27 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1691.93it/s]\n",
      "04/28/2022 08:33:27 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "04/28/2022 08:33:27 - INFO - datasets.builder - Generating train split\n",
      "04/28/2022 08:33:34 - INFO - datasets.builder - Generating validation split     \n",
      "04/28/2022 08:33:35 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 501.44it/s]\n",
      "[INFO|hub.py:583] 2022-04-28 08:33:35,968 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp_k1ynpie\n",
      "Downloading: 100%|██████████████████████████████| 481/481 [00:00<00:00, 280kB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 08:33:36,054 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|hub.py:595] 2022-04-28 08:33:36,054 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 08:33:36,055 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 08:33:36,056 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-04-28 08:33:36,126 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 08:33:36,194 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 08:33:36,195 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-28 08:33:36,322 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpxwbp7kuw\n",
      "Downloading: 100%|███████████████████████████| 878k/878k [00:00<00:00, 15.2MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 08:33:36,501 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:595] 2022-04-28 08:33:36,501 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:583] 2022-04-28 08:33:36,564 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvgvm3nws\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 7.47MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 08:33:36,700 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:595] 2022-04-28 08:33:36,701 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:583] 2022-04-28 08:33:36,765 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6dn_qc40\n",
      "Downloading: 100%|█████████████████████████| 1.29M/1.29M [00:00<00:00, 15.9MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 08:33:36,943 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|hub.py:595] 2022-04-28 08:33:36,943 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,155 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,155 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,156 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,156 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,156 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 08:33:37,156 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 08:33:37,218 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 08:33:37,219 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-28 08:33:37,412 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2pprllh1\n",
      "Downloading: 100%|███████████████████████████| 478M/478M [00:21<00:00, 23.3MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 08:33:58,994 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|hub.py:595] 2022-04-28 08:33:58,994 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1772] 2022-04-28 08:33:58,994 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:2048] 2022-04-28 08:34:00,704 >> Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2059] 2022-04-28 08:34:00,704 >> Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]04/28/2022 08:34:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-8ef90d432692b3f8.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:46<00:00,  1.88ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]04/28/2022 08:34:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-0431a0b7eb7e2996.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:40<00:00,  3.68s/ba]\n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpdcrt4r42\n",
      "Downloading builder script: 4.50kB [00:00, 2.90MB/s]                            \n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpuulz6kan\n",
      "Downloading extra modules: 3.31kB [00:00, 1.93MB/s]                             \n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "04/28/2022 08:35:28 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "[INFO|trainer.py:1172] 2022-04-28 08:35:33,485 >> Loading model from ./squad_roberta_base/checkpoint-21000).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1290] 2022-04-28 08:35:36,847 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-28 08:35:36,847 >>   Num examples = 88568\n",
      "[INFO|trainer.py:1292] 2022-04-28 08:35:36,848 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1293] 2022-04-28 08:35:36,848 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:1294] 2022-04-28 08:35:36,848 >>   Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "[INFO|trainer.py:1295] 2022-04-28 08:35:36,848 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-28 08:35:36,848 >>   Total optimization steps = 29523\n",
      "{'loss': 0.9447, 'learning_rate': 2.9491921552687736e-05, 'epoch': 0.02}        \n",
      "  2%|▋                                    | 500/29523 [01:53<1:49:34,  4.41it/s][INFO|trainer.py:2166] 2022-04-28 08:37:30,211 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:37:30,214 >> Configuration saved in ./squad_roberta_base/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:37:31,172 >> Model weights saved in ./squad_roberta_base/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:37:31,183 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:37:31,193 >> Special tokens file saved in ./squad_roberta_base/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.8145, 'learning_rate': 2.8983843105375472e-05, 'epoch': 0.03}        \n",
      "  3%|█▏                                  | 1000/29523 [03:52<1:51:06,  4.28it/s][INFO|trainer.py:2166] 2022-04-28 08:39:29,778 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-1000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:39:29,785 >> Configuration saved in ./squad_roberta_base/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:39:33,982 >> Model weights saved in ./squad_roberta_base/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:39:33,992 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:39:33,999 >> Special tokens file saved in ./squad_roberta_base/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.7937, 'learning_rate': 2.8475764658063208e-05, 'epoch': 0.05}        \n",
      "  5%|█▊                                  | 1500/29523 [06:01<1:49:18,  4.27it/s][INFO|trainer.py:2166] 2022-04-28 08:41:38,584 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-1500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:41:38,593 >> Configuration saved in ./squad_roberta_base/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:41:42,745 >> Model weights saved in ./squad_roberta_base/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:41:42,752 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:41:42,760 >> Special tokens file saved in ./squad_roberta_base/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 0.7967, 'learning_rate': 2.796768621075094e-05, 'epoch': 0.07}         \n",
      "  7%|██▍                                 | 2000/29523 [08:10<1:47:25,  4.27it/s][INFO|trainer.py:2166] 2022-04-28 08:43:47,227 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-2000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:43:47,234 >> Configuration saved in ./squad_roberta_base/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:43:51,373 >> Model weights saved in ./squad_roberta_base/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:43:51,379 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:43:51,387 >> Special tokens file saved in ./squad_roberta_base/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 0.7664, 'learning_rate': 2.7459607763438676e-05, 'epoch': 0.08}        \n",
      "  8%|███                                 | 2500/29523 [10:18<1:45:22,  4.27it/s][INFO|trainer.py:2166] 2022-04-28 08:45:55,617 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-2500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:45:55,625 >> Configuration saved in ./squad_roberta_base/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:46:00,155 >> Model weights saved in ./squad_roberta_base/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:46:00,167 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:46:00,176 >> Special tokens file saved in ./squad_roberta_base/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 0.7425, 'learning_rate': 2.695152931612641e-05, 'epoch': 0.1}          \n",
      " 10%|███▋                                | 3000/29523 [12:28<1:42:26,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 08:48:05,332 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-3000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:48:05,338 >> Configuration saved in ./squad_roberta_base/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:48:09,894 >> Model weights saved in ./squad_roberta_base/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:48:09,900 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:48:09,906 >> Special tokens file saved in ./squad_roberta_base/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 0.751, 'learning_rate': 2.6443450868814147e-05, 'epoch': 0.12}         \n",
      " 12%|████▎                               | 3500/29523 [14:38<1:41:05,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 08:50:15,178 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-3500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:50:15,185 >> Configuration saved in ./squad_roberta_base/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:50:19,695 >> Model weights saved in ./squad_roberta_base/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:50:19,701 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:50:19,714 >> Special tokens file saved in ./squad_roberta_base/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 0.7389, 'learning_rate': 2.593537242150188e-05, 'epoch': 0.14}         \n",
      " 14%|████▉                               | 4000/29523 [16:46<1:38:38,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 08:52:23,732 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-4000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:52:23,738 >> Configuration saved in ./squad_roberta_base/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:52:28,156 >> Model weights saved in ./squad_roberta_base/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:52:28,162 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:52:28,169 >> Special tokens file saved in ./squad_roberta_base/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 0.7271, 'learning_rate': 2.5427293974189615e-05, 'epoch': 0.15}        \n",
      " 15%|█████▍                              | 4500/29523 [18:55<1:36:44,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 08:54:32,294 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-4500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:54:32,300 >> Configuration saved in ./squad_roberta_base/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:54:36,878 >> Model weights saved in ./squad_roberta_base/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:54:36,890 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:54:36,906 >> Special tokens file saved in ./squad_roberta_base/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 0.7377, 'learning_rate': 2.491921552687735e-05, 'epoch': 0.17}         \n",
      " 17%|██████                              | 5000/29523 [21:04<1:34:39,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 08:56:41,698 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-5000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:56:41,703 >> Configuration saved in ./squad_roberta_base/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:56:45,677 >> Model weights saved in ./squad_roberta_base/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:56:45,686 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:56:45,692 >> Special tokens file saved in ./squad_roberta_base/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 0.7591, 'learning_rate': 2.4411137079565086e-05, 'epoch': 0.19}        \n",
      " 19%|██████▋                             | 5500/29523 [23:13<1:33:15,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 08:58:50,208 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-5500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 08:58:50,214 >> Configuration saved in ./squad_roberta_base/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 08:58:54,355 >> Model weights saved in ./squad_roberta_base/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 08:58:54,364 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 08:58:54,374 >> Special tokens file saved in ./squad_roberta_base/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 0.7138, 'learning_rate': 2.390305863225282e-05, 'epoch': 0.2}          \n",
      " 20%|███████▎                            | 6000/29523 [25:21<1:31:13,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:00:58,431 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-6000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:00:58,507 >> Configuration saved in ./squad_roberta_base/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:01:02,786 >> Model weights saved in ./squad_roberta_base/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:01:02,796 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:01:02,805 >> Special tokens file saved in ./squad_roberta_base/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 0.7389, 'learning_rate': 2.3394980184940554e-05, 'epoch': 0.22}        \n",
      " 22%|███████▉                            | 6500/29523 [27:30<1:28:32,  4.33it/s][INFO|trainer.py:2166] 2022-04-28 09:03:07,363 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-6500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:03:07,370 >> Configuration saved in ./squad_roberta_base/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:03:13,580 >> Model weights saved in ./squad_roberta_base/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:03:13,589 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:03:13,596 >> Special tokens file saved in ./squad_roberta_base/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 0.7309, 'learning_rate': 2.2886901737628294e-05, 'epoch': 0.24}        \n",
      " 24%|████████▌                           | 7000/29523 [29:41<1:27:37,  4.28it/s][INFO|trainer.py:2166] 2022-04-28 09:05:17,921 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-7000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:05:17,928 >> Configuration saved in ./squad_roberta_base/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:05:21,960 >> Model weights saved in ./squad_roberta_base/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:05:21,969 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:05:21,975 >> Special tokens file saved in ./squad_roberta_base/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.7775, 'learning_rate': 2.2378823290316026e-05, 'epoch': 0.25}        \n",
      " 25%|█████████▏                          | 7500/29523 [31:49<1:25:27,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 09:07:26,041 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-7500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:07:26,048 >> Configuration saved in ./squad_roberta_base/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:07:30,404 >> Model weights saved in ./squad_roberta_base/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:07:30,494 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:07:30,501 >> Special tokens file saved in ./squad_roberta_base/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.7281, 'learning_rate': 2.187074484300376e-05, 'epoch': 0.27}         \n",
      " 27%|█████████▊                          | 8000/29523 [33:57<1:23:16,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:09:34,150 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-8000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:09:34,180 >> Configuration saved in ./squad_roberta_base/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:09:39,753 >> Model weights saved in ./squad_roberta_base/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:09:39,765 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:09:39,774 >> Special tokens file saved in ./squad_roberta_base/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.7232, 'learning_rate': 2.1362666395691494e-05, 'epoch': 0.29}        \n",
      " 29%|██████████▎                         | 8500/29523 [36:08<1:21:30,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:11:45,661 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-8500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:11:45,663 >> Configuration saved in ./squad_roberta_base/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:11:46,630 >> Model weights saved in ./squad_roberta_base/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:11:46,636 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:11:46,648 >> Special tokens file saved in ./squad_roberta_base/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 0.7208, 'learning_rate': 2.0854587948379233e-05, 'epoch': 0.3}         \n",
      " 30%|██████████▉                         | 9000/29523 [38:08<1:19:21,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:13:45,282 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-9000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:13:45,293 >> Configuration saved in ./squad_roberta_base/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:13:50,045 >> Model weights saved in ./squad_roberta_base/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:13:50,053 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:13:50,065 >> Special tokens file saved in ./squad_roberta_base/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.7482, 'learning_rate': 2.0346509501066965e-05, 'epoch': 0.32}        \n",
      " 32%|███████████▌                        | 9500/29523 [40:17<1:17:30,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:15:54,721 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-9500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:15:54,730 >> Configuration saved in ./squad_roberta_base/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:15:59,680 >> Model weights saved in ./squad_roberta_base/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:15:59,689 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:15:59,695 >> Special tokens file saved in ./squad_roberta_base/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.6428, 'learning_rate': 1.98384310537547e-05, 'epoch': 0.34}          \n",
      " 34%|███████████▊                       | 10000/29523 [42:27<1:15:30,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:18:04,673 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-10000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:18:04,679 >> Configuration saved in ./squad_roberta_base/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:18:10,530 >> Model weights saved in ./squad_roberta_base/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:18:10,539 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:18:10,544 >> Special tokens file saved in ./squad_roberta_base/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.7739, 'learning_rate': 1.9330352606442433e-05, 'epoch': 0.36}        \n",
      " 36%|████████████▍                      | 10500/29523 [44:38<1:13:03,  4.34it/s][INFO|trainer.py:2166] 2022-04-28 09:20:15,512 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-10500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:20:15,519 >> Configuration saved in ./squad_roberta_base/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:20:20,510 >> Model weights saved in ./squad_roberta_base/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:20:20,522 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:20:20,528 >> Special tokens file saved in ./squad_roberta_base/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.6989, 'learning_rate': 1.8822274159130172e-05, 'epoch': 0.37}        \n",
      " 37%|█████████████                      | 11000/29523 [46:49<1:11:50,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:22:26,363 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-11000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:22:26,369 >> Configuration saved in ./squad_roberta_base/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:22:31,351 >> Model weights saved in ./squad_roberta_base/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:22:31,358 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:22:31,371 >> Special tokens file saved in ./squad_roberta_base/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.7174, 'learning_rate': 1.8314195711817904e-05, 'epoch': 0.39}        \n",
      " 39%|█████████████▋                     | 11500/29523 [48:59<1:09:55,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:24:36,358 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-11500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:24:36,366 >> Configuration saved in ./squad_roberta_base/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:24:41,513 >> Model weights saved in ./squad_roberta_base/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:24:41,523 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:24:41,528 >> Special tokens file saved in ./squad_roberta_base/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.6692, 'learning_rate': 1.780611726450564e-05, 'epoch': 0.41}         \n",
      " 41%|██████████████▏                    | 12000/29523 [51:11<1:07:49,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:26:48,413 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-12000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:26:48,419 >> Configuration saved in ./squad_roberta_base/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:26:53,024 >> Model weights saved in ./squad_roberta_base/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:26:53,171 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:26:53,180 >> Special tokens file saved in ./squad_roberta_base/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.6961, 'learning_rate': 1.7298038817193372e-05, 'epoch': 0.42}        \n",
      " 42%|██████████████▊                    | 12500/29523 [53:21<1:06:03,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 09:28:57,903 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-12500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:28:57,911 >> Configuration saved in ./squad_roberta_base/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:29:03,136 >> Model weights saved in ./squad_roberta_base/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:29:03,147 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:29:03,153 >> Special tokens file saved in ./squad_roberta_base/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.6891, 'learning_rate': 1.678996036988111e-05, 'epoch': 0.44}         \n",
      " 44%|███████████████▍                   | 13000/29523 [55:31<1:03:54,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:31:07,967 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-13000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:31:07,977 >> Configuration saved in ./squad_roberta_base/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:31:12,501 >> Model weights saved in ./squad_roberta_base/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:31:12,510 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:31:12,519 >> Special tokens file saved in ./squad_roberta_base/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.71, 'learning_rate': 1.6281881922568847e-05, 'epoch': 0.46}          \n",
      " 46%|████████████████                   | 13500/29523 [57:40<1:02:02,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:33:17,815 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-13500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:33:17,821 >> Configuration saved in ./squad_roberta_base/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:33:22,751 >> Model weights saved in ./squad_roberta_base/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:33:22,757 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:33:22,764 >> Special tokens file saved in ./squad_roberta_base/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.7306, 'learning_rate': 1.577380347525658e-05, 'epoch': 0.47}         \n",
      " 47%|████████████████▌                  | 14000/29523 [59:51<1:00:06,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:35:28,173 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-14000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:35:28,179 >> Configuration saved in ./squad_roberta_base/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:35:32,388 >> Model weights saved in ./squad_roberta_base/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:35:32,470 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:35:32,476 >> Special tokens file saved in ./squad_roberta_base/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.6666, 'learning_rate': 1.5265725027944315e-05, 'epoch': 0.49}        \n",
      " 49%|█████████████████▏                 | 14500/29523 [1:02:01<58:00,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 09:37:38,515 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-14500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:37:38,521 >> Configuration saved in ./squad_roberta_base/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:37:43,477 >> Model weights saved in ./squad_roberta_base/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:37:43,483 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:37:43,491 >> Special tokens file saved in ./squad_roberta_base/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 0.6947, 'learning_rate': 1.4757646580632049e-05, 'epoch': 0.51}        \n",
      " 51%|█████████████████▊                 | 15000/29523 [1:04:11<56:06,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:39:48,642 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-15000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:39:48,649 >> Configuration saved in ./squad_roberta_base/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:39:53,254 >> Model weights saved in ./squad_roberta_base/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:39:53,329 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:39:53,334 >> Special tokens file saved in ./squad_roberta_base/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 0.7007, 'learning_rate': 1.4249568133319785e-05, 'epoch': 0.53}        \n",
      " 53%|██████████████████▍                | 15500/29523 [1:06:22<54:09,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 09:41:59,108 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-15500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:41:59,114 >> Configuration saved in ./squad_roberta_base/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:42:03,792 >> Model weights saved in ./squad_roberta_base/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:42:03,801 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:42:03,812 >> Special tokens file saved in ./squad_roberta_base/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 0.6556, 'learning_rate': 1.374148968600752e-05, 'epoch': 0.54}         \n",
      " 54%|██████████████████▉                | 16000/29523 [1:08:31<52:19,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:44:08,237 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-16000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:44:08,244 >> Configuration saved in ./squad_roberta_base/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:44:13,078 >> Model weights saved in ./squad_roberta_base/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:44:13,085 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:44:13,155 >> Special tokens file saved in ./squad_roberta_base/checkpoint-16000/special_tokens_map.json\n",
      "{'loss': 0.6544, 'learning_rate': 1.3233411238695256e-05, 'epoch': 0.56}        \n",
      " 56%|███████████████████▌               | 16500/29523 [1:10:40<49:55,  4.35it/s][INFO|trainer.py:2166] 2022-04-28 09:46:16,974 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-16500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:46:16,983 >> Configuration saved in ./squad_roberta_base/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:46:21,823 >> Model weights saved in ./squad_roberta_base/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:46:21,829 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:46:21,834 >> Special tokens file saved in ./squad_roberta_base/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 0.6774, 'learning_rate': 1.272533279138299e-05, 'epoch': 0.58}         \n",
      " 58%|████████████████████▏              | 17000/29523 [1:12:49<48:35,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 09:48:26,278 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-17000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:48:26,280 >> Configuration saved in ./squad_roberta_base/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:48:27,312 >> Model weights saved in ./squad_roberta_base/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:48:27,316 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:48:27,319 >> Special tokens file saved in ./squad_roberta_base/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 0.7818, 'learning_rate': 1.2217254344070726e-05, 'epoch': 0.59}        \n",
      " 59%|████████████████████▋              | 17500/29523 [1:14:49<46:44,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 09:50:26,382 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-17500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:50:26,389 >> Configuration saved in ./squad_roberta_base/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:50:31,316 >> Model weights saved in ./squad_roberta_base/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:50:31,324 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:50:31,332 >> Special tokens file saved in ./squad_roberta_base/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 0.7475, 'learning_rate': 1.170917589675846e-05, 'epoch': 0.61}         \n",
      " 61%|█████████████████████▎             | 18000/29523 [1:16:59<44:28,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 09:52:36,568 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-18000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:52:36,576 >> Configuration saved in ./squad_roberta_base/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:52:40,995 >> Model weights saved in ./squad_roberta_base/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:52:41,002 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:52:41,007 >> Special tokens file saved in ./squad_roberta_base/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 0.6589, 'learning_rate': 1.1201097449446196e-05, 'epoch': 0.63}        \n",
      " 63%|█████████████████████▉             | 18500/29523 [1:19:08<42:36,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 09:54:44,998 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-18500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:54:45,006 >> Configuration saved in ./squad_roberta_base/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:54:49,732 >> Model weights saved in ./squad_roberta_base/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:54:49,740 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:54:49,749 >> Special tokens file saved in ./squad_roberta_base/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 0.7161, 'learning_rate': 1.069301900213393e-05, 'epoch': 0.64}         \n",
      " 64%|██████████████████████▌            | 19000/29523 [1:21:18<40:38,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 09:56:54,937 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-19000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:56:54,944 >> Configuration saved in ./squad_roberta_base/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:57:02,958 >> Model weights saved in ./squad_roberta_base/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:57:02,968 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:57:02,977 >> Special tokens file saved in ./squad_roberta_base/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 0.675, 'learning_rate': 1.0184940554821665e-05, 'epoch': 0.66}         \n",
      " 66%|███████████████████████            | 19500/29523 [1:23:34<38:58,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 09:59:11,694 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-19500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 09:59:11,703 >> Configuration saved in ./squad_roberta_base/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 09:59:16,168 >> Model weights saved in ./squad_roberta_base/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 09:59:16,177 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 09:59:16,182 >> Special tokens file saved in ./squad_roberta_base/checkpoint-19500/special_tokens_map.json\n",
      "{'loss': 0.731, 'learning_rate': 9.6768621075094e-06, 'epoch': 0.68}            \n",
      " 68%|███████████████████████▋           | 20000/29523 [1:25:43<36:43,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 10:01:20,590 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-20000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:01:20,603 >> Configuration saved in ./squad_roberta_base/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:01:24,874 >> Model weights saved in ./squad_roberta_base/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:01:24,882 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:01:24,889 >> Special tokens file saved in ./squad_roberta_base/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.7438, 'learning_rate': 9.168783660197135e-06, 'epoch': 0.69}         \n",
      " 69%|████████████████████████▎          | 20500/29523 [1:27:51<34:59,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 10:03:28,700 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-20500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:03:28,705 >> Configuration saved in ./squad_roberta_base/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:03:33,631 >> Model weights saved in ./squad_roberta_base/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:03:33,641 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:03:33,649 >> Special tokens file saved in ./squad_roberta_base/checkpoint-20500/special_tokens_map.json\n",
      "{'loss': 0.7183, 'learning_rate': 8.660705212884869e-06, 'epoch': 0.71}         \n",
      " 71%|████████████████████████▉          | 21000/29523 [1:30:00<33:00,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 10:05:37,791 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-21000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:05:37,802 >> Configuration saved in ./squad_roberta_base/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:05:43,445 >> Model weights saved in ./squad_roberta_base/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:05:43,476 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:05:43,487 >> Special tokens file saved in ./squad_roberta_base/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 1.083, 'learning_rate': 8.152626765572605e-06, 'epoch': 0.73}          \n",
      " 73%|█████████████████████████▍         | 21500/29523 [1:32:14<31:04,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 10:07:50,931 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-21500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:07:50,937 >> Configuration saved in ./squad_roberta_base/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:07:55,161 >> Model weights saved in ./squad_roberta_base/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:07:55,165 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:07:55,171 >> Special tokens file saved in ./squad_roberta_base/checkpoint-21500/special_tokens_map.json\n",
      "{'loss': 1.0897, 'learning_rate': 7.64454831826034e-06, 'epoch': 0.75}          \n",
      " 75%|██████████████████████████         | 22000/29523 [1:34:23<29:07,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 10:09:59,920 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-22000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:09:59,930 >> Configuration saved in ./squad_roberta_base/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:10:04,598 >> Model weights saved in ./squad_roberta_base/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:10:04,605 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:10:04,613 >> Special tokens file saved in ./squad_roberta_base/checkpoint-22000/special_tokens_map.json\n",
      "{'loss': 0.9922, 'learning_rate': 7.136469870948074e-06, 'epoch': 0.76}         \n",
      " 76%|██████████████████████████▋        | 22500/29523 [1:36:32<27:11,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 10:12:09,167 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-22500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:12:09,174 >> Configuration saved in ./squad_roberta_base/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:12:14,070 >> Model weights saved in ./squad_roberta_base/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:12:14,073 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:12:14,080 >> Special tokens file saved in ./squad_roberta_base/checkpoint-22500/special_tokens_map.json\n",
      "{'loss': 0.9532, 'learning_rate': 6.628391423635809e-06, 'epoch': 0.78}         \n",
      " 78%|███████████████████████████▎       | 23000/29523 [1:38:42<25:11,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 10:14:18,998 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-23000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:14:19,003 >> Configuration saved in ./squad_roberta_base/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:14:23,429 >> Model weights saved in ./squad_roberta_base/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:14:23,435 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:14:23,441 >> Special tokens file saved in ./squad_roberta_base/checkpoint-23000/special_tokens_map.json\n",
      "{'loss': 0.9796, 'learning_rate': 6.120312976323545e-06, 'epoch': 0.8}          \n",
      " 80%|███████████████████████████▊       | 23500/29523 [1:40:50<23:23,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 10:16:27,421 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-23500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:16:27,428 >> Configuration saved in ./squad_roberta_base/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:16:31,596 >> Model weights saved in ./squad_roberta_base/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:16:31,603 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:16:31,611 >> Special tokens file saved in ./squad_roberta_base/checkpoint-23500/special_tokens_map.json\n",
      "{'loss': 1.001, 'learning_rate': 5.6122345290112795e-06, 'epoch': 0.81}         \n",
      " 81%|████████████████████████████▍      | 24000/29523 [1:43:02<21:36,  4.26it/s][INFO|trainer.py:2166] 2022-04-28 10:18:39,859 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-24000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:18:39,866 >> Configuration saved in ./squad_roberta_base/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:18:44,427 >> Model weights saved in ./squad_roberta_base/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:18:44,433 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:18:44,437 >> Special tokens file saved in ./squad_roberta_base/checkpoint-24000/special_tokens_map.json\n",
      "{'loss': 0.9846, 'learning_rate': 5.104156081699014e-06, 'epoch': 0.83}         \n",
      " 83%|█████████████████████████████      | 24500/29523 [1:45:11<19:25,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 10:20:48,246 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-24500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:20:48,252 >> Configuration saved in ./squad_roberta_base/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:20:52,697 >> Model weights saved in ./squad_roberta_base/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:20:52,707 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:20:52,712 >> Special tokens file saved in ./squad_roberta_base/checkpoint-24500/special_tokens_map.json\n",
      "{'loss': 0.9818, 'learning_rate': 4.596077634386749e-06, 'epoch': 0.85}         \n",
      " 85%|█████████████████████████████▋     | 25000/29523 [1:47:19<17:30,  4.31it/s][INFO|trainer.py:2166] 2022-04-28 10:22:56,862 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-25000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:22:56,868 >> Configuration saved in ./squad_roberta_base/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:23:01,156 >> Model weights saved in ./squad_roberta_base/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:23:01,168 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:23:01,174 >> Special tokens file saved in ./squad_roberta_base/checkpoint-25000/special_tokens_map.json\n",
      "{'loss': 0.9587, 'learning_rate': 4.087999187074484e-06, 'epoch': 0.86}         \n",
      " 86%|██████████████████████████████▏    | 25500/29523 [1:49:28<15:53,  4.22it/s][INFO|trainer.py:2166] 2022-04-28 10:25:04,991 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-25500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:25:04,999 >> Configuration saved in ./squad_roberta_base/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:25:09,584 >> Model weights saved in ./squad_roberta_base/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:25:09,588 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:25:09,592 >> Special tokens file saved in ./squad_roberta_base/checkpoint-25500/special_tokens_map.json\n",
      "{'loss': 0.9675, 'learning_rate': 3.5799207397622197e-06, 'epoch': 0.88}        \n",
      " 88%|██████████████████████████████▊    | 26000/29523 [1:51:37<13:50,  4.24it/s][INFO|trainer.py:2166] 2022-04-28 10:27:14,617 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-26000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:27:14,624 >> Configuration saved in ./squad_roberta_base/checkpoint-26000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:27:19,373 >> Model weights saved in ./squad_roberta_base/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:27:19,377 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:27:19,381 >> Special tokens file saved in ./squad_roberta_base/checkpoint-26000/special_tokens_map.json\n",
      "{'loss': 0.9773, 'learning_rate': 3.0718422924499546e-06, 'epoch': 0.9}         \n",
      " 90%|███████████████████████████████▍   | 26500/29523 [1:53:46<11:43,  4.30it/s][INFO|trainer.py:2166] 2022-04-28 10:29:23,598 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-26500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:29:23,620 >> Configuration saved in ./squad_roberta_base/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:29:27,749 >> Model weights saved in ./squad_roberta_base/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:29:27,753 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:29:27,760 >> Special tokens file saved in ./squad_roberta_base/checkpoint-26500/special_tokens_map.json\n",
      "{'loss': 0.972, 'learning_rate': 2.5637638451376894e-06, 'epoch': 0.91}         \n",
      " 91%|████████████████████████████████   | 27000/29523 [1:55:54<09:51,  4.26it/s][INFO|trainer.py:2166] 2022-04-28 10:31:31,549 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-27000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:31:31,555 >> Configuration saved in ./squad_roberta_base/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:31:35,703 >> Model weights saved in ./squad_roberta_base/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:31:35,707 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:31:35,712 >> Special tokens file saved in ./squad_roberta_base/checkpoint-27000/special_tokens_map.json\n",
      "{'loss': 0.9944, 'learning_rate': 2.0556853978254246e-06, 'epoch': 0.93}        \n",
      " 93%|████████████████████████████████▌  | 27500/29523 [1:58:02<07:51,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 10:33:39,323 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-27500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:33:39,330 >> Configuration saved in ./squad_roberta_base/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:33:43,720 >> Model weights saved in ./squad_roberta_base/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:33:43,727 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:33:43,732 >> Special tokens file saved in ./squad_roberta_base/checkpoint-27500/special_tokens_map.json\n",
      "{'loss': 0.9271, 'learning_rate': 1.5476069505131593e-06, 'epoch': 0.95}        \n",
      " 95%|█████████████████████████████████▏ | 28000/29523 [2:00:10<05:52,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 10:35:47,721 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-28000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:35:47,727 >> Configuration saved in ./squad_roberta_base/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:35:50,541 >> Model weights saved in ./squad_roberta_base/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:35:50,543 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:35:50,544 >> Special tokens file saved in ./squad_roberta_base/checkpoint-28000/special_tokens_map.json\n",
      "{'loss': 0.981, 'learning_rate': 1.039528503200894e-06, 'epoch': 0.97}          \n",
      " 97%|█████████████████████████████████▊ | 28500/29523 [2:02:12<03:56,  4.32it/s][INFO|trainer.py:2166] 2022-04-28 10:37:48,981 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-28500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:37:48,983 >> Configuration saved in ./squad_roberta_base/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:37:49,937 >> Model weights saved in ./squad_roberta_base/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:37:49,939 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:37:49,939 >> Special tokens file saved in ./squad_roberta_base/checkpoint-28500/special_tokens_map.json\n",
      "{'loss': 0.9473, 'learning_rate': 5.314500558886293e-07, 'epoch': 0.98}         \n",
      " 98%|██████████████████████████████████▍| 29000/29523 [2:04:11<02:01,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 10:39:48,554 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-29000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:39:48,555 >> Configuration saved in ./squad_roberta_base/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:39:49,533 >> Model weights saved in ./squad_roberta_base/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:39:49,535 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:39:49,536 >> Special tokens file saved in ./squad_roberta_base/checkpoint-29000/special_tokens_map.json\n",
      "{'loss': 0.9154, 'learning_rate': 2.337160857636419e-08, 'epoch': 1.0}          \n",
      "100%|██████████████████████████████████▉| 29500/29523 [2:06:11<00:05,  4.29it/s][INFO|trainer.py:2166] 2022-04-28 10:41:48,039 >> Saving model checkpoint to ./squad_roberta_base/checkpoint-29500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:41:48,041 >> Configuration saved in ./squad_roberta_base/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:41:48,982 >> Model weights saved in ./squad_roberta_base/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:41:48,983 >> tokenizer config file saved in ./squad_roberta_base/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:41:48,984 >> Special tokens file saved in ./squad_roberta_base/checkpoint-29500/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 29523/29523 [2:06:19<00:00,  4.65it/s][INFO|trainer.py:1530] 2022-04-28 10:41:56,402 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 7579.5562, 'train_samples_per_second': 11.685, 'train_steps_per_second': 3.895, 'train_loss': 0.8021655186556003, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████| 29523/29523 [2:06:19<00:00,  3.90it/s]\n",
      "[INFO|trainer.py:2166] 2022-04-28 10:41:56,407 >> Saving model checkpoint to ./squad_roberta_base\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 10:41:56,409 >> Configuration saved in ./squad_roberta_base/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 10:41:57,366 >> Model weights saved in ./squad_roberta_base/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 10:41:57,368 >> tokenizer config file saved in ./squad_roberta_base/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 10:41:57,369 >> Special tokens file saved in ./squad_roberta_base/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     0.8022\n",
      "  train_runtime            = 2:06:19.55\n",
      "  train_samples            =      88568\n",
      "  train_samples_per_second =     11.685\n",
      "  train_steps_per_second   =      3.895\n",
      "04/28/2022 10:41:57 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:566] 2022-04-28 10:41:57,528 >> The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2416] 2022-04-28 10:41:57,533 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2022-04-28 10:41:57,533 >>   Num examples = 10790\n",
      "[INFO|trainer.py:2421] 2022-04-28 10:41:57,533 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1349/1349 [03:31<00:00,  6.90it/s]04/28/2022 10:45:42 - INFO - utils_qa - Post-processing 10570 example predictions split into 10790 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                       | 25/10570 [00:00<00:43, 244.47it/s]\u001b[A\n",
      "  1%|▏                                      | 62/10570 [00:00<00:33, 317.33it/s]\u001b[A\n",
      "  1%|▎                                     | 102/10570 [00:00<00:29, 352.80it/s]\u001b[A\n",
      "  1%|▌                                     | 145/10570 [00:00<00:27, 381.51it/s]\u001b[A\n",
      "  2%|▋                                     | 184/10570 [00:00<00:27, 384.36it/s]\u001b[A\n",
      "  2%|▊                                     | 223/10570 [00:00<00:30, 340.61it/s]\u001b[A\n",
      "  2%|▉                                     | 258/10570 [00:00<00:35, 290.25it/s]\u001b[A\n",
      "  3%|█                                     | 289/10570 [00:00<00:37, 272.58it/s]\u001b[A\n",
      "  3%|█▏                                    | 333/10570 [00:01<00:32, 315.73it/s]\u001b[A\n",
      "  4%|█▎                                    | 376/10570 [00:01<00:29, 344.58it/s]\u001b[A\n",
      "  4%|█▍                                    | 412/10570 [00:01<00:29, 346.53it/s]\u001b[A\n",
      "  4%|█▌                                    | 451/10570 [00:01<00:28, 358.27it/s]\u001b[A\n",
      "  5%|█▊                                    | 488/10570 [00:01<00:28, 356.90it/s]\u001b[A\n",
      "  5%|█▉                                    | 527/10570 [00:01<00:27, 366.33it/s]\u001b[A\n",
      "  5%|██                                    | 565/10570 [00:01<00:27, 370.06it/s]\u001b[A\n",
      "  6%|██▏                                   | 603/10570 [00:01<00:27, 369.06it/s]\u001b[A\n",
      "  6%|██▎                                   | 641/10570 [00:01<00:27, 366.97it/s]\u001b[A\n",
      "  6%|██▍                                   | 678/10570 [00:01<00:27, 365.07it/s]\u001b[A\n",
      "  7%|██▌                                   | 715/10570 [00:02<00:27, 363.19it/s]\u001b[A\n",
      "  7%|██▋                                   | 752/10570 [00:02<00:28, 345.04it/s]\u001b[A\n",
      "  7%|██▊                                   | 787/10570 [00:02<00:28, 341.19it/s]\u001b[A\n",
      "  8%|██▉                                   | 822/10570 [00:02<00:30, 317.97it/s]\u001b[A\n",
      "  8%|███                                   | 860/10570 [00:02<00:29, 332.83it/s]\u001b[A\n",
      "  8%|███▏                                  | 898/10570 [00:02<00:28, 344.62it/s]\u001b[A\n",
      "  9%|███▎                                  | 933/10570 [00:02<00:28, 341.17it/s]\u001b[A\n",
      "  9%|███▍                                  | 972/10570 [00:02<00:27, 352.93it/s]\u001b[A\n",
      " 10%|███▌                                 | 1008/10570 [00:02<00:27, 353.67it/s]\u001b[A\n",
      " 10%|███▋                                 | 1044/10570 [00:03<00:28, 337.02it/s]\u001b[A\n",
      " 10%|███▊                                 | 1078/10570 [00:03<00:28, 328.94it/s]\u001b[A\n",
      " 11%|███▉                                 | 1112/10570 [00:03<00:29, 322.21it/s]\u001b[A\n",
      " 11%|████                                 | 1145/10570 [00:03<00:30, 313.78it/s]\u001b[A\n",
      " 11%|████▏                                | 1181/10570 [00:03<00:28, 325.12it/s]\u001b[A\n",
      " 11%|████▏                                | 1214/10570 [00:03<00:28, 323.43it/s]\u001b[A\n",
      " 12%|████▎                                | 1248/10570 [00:03<00:28, 328.02it/s]\u001b[A\n",
      " 12%|████▍                                | 1281/10570 [00:03<00:29, 318.19it/s]\u001b[A\n",
      " 12%|████▌                                | 1315/10570 [00:03<00:28, 322.33it/s]\u001b[A\n",
      " 13%|████▋                                | 1349/10570 [00:03<00:28, 326.22it/s]\u001b[A\n",
      " 13%|████▊                                | 1384/10570 [00:04<00:27, 330.39it/s]\u001b[A\n",
      " 13%|████▉                                | 1418/10570 [00:04<00:27, 327.71it/s]\u001b[A\n",
      " 14%|█████                                | 1451/10570 [00:04<00:29, 308.83it/s]\u001b[A\n",
      " 14%|█████▏                               | 1483/10570 [00:04<00:29, 310.19it/s]\u001b[A\n",
      " 14%|█████▎                               | 1517/10570 [00:04<00:28, 317.34it/s]\u001b[A\n",
      " 15%|█████▍                               | 1550/10570 [00:04<00:28, 319.40it/s]\u001b[A\n",
      " 15%|█████▌                               | 1583/10570 [00:04<00:28, 312.54it/s]\u001b[A\n",
      " 15%|█████▋                               | 1617/10570 [00:04<00:28, 318.00it/s]\u001b[A\n",
      " 16%|█████▊                               | 1649/10570 [00:04<00:29, 303.70it/s]\u001b[A\n",
      " 16%|█████▉                               | 1680/10570 [00:05<00:29, 298.91it/s]\u001b[A\n",
      " 16%|█████▉                               | 1711/10570 [00:05<00:29, 301.27it/s]\u001b[A\n",
      " 16%|██████                               | 1743/10570 [00:05<00:29, 303.92it/s]\u001b[A\n",
      " 17%|██████▏                              | 1775/10570 [00:05<00:28, 307.63it/s]\u001b[A\n",
      " 17%|██████▎                              | 1806/10570 [00:05<00:29, 300.50it/s]\u001b[A\n",
      " 17%|██████▍                              | 1837/10570 [00:05<00:38, 228.63it/s]\u001b[A\n",
      " 18%|██████▌                              | 1865/10570 [00:05<00:36, 239.64it/s]\u001b[A\n",
      " 18%|██████▋                              | 1896/10570 [00:05<00:33, 255.68it/s]\u001b[A\n",
      " 18%|██████▋                              | 1927/10570 [00:06<00:32, 268.92it/s]\u001b[A\n",
      " 19%|██████▊                              | 1958/10570 [00:06<00:30, 279.57it/s]\u001b[A\n",
      " 19%|██████▉                              | 1989/10570 [00:06<00:30, 285.68it/s]\u001b[A\n",
      " 19%|███████                              | 2019/10570 [00:06<00:29, 287.21it/s]\u001b[A\n",
      " 19%|███████▏                             | 2050/10570 [00:06<00:29, 291.70it/s]\u001b[A\n",
      " 20%|███████▎                             | 2080/10570 [00:06<00:30, 278.50it/s]\u001b[A\n",
      " 20%|███████▍                             | 2113/10570 [00:06<00:29, 290.64it/s]\u001b[A\n",
      " 20%|███████▌                             | 2143/10570 [00:06<00:31, 265.32it/s]\u001b[A\n",
      " 21%|███████▌                             | 2171/10570 [00:06<00:31, 262.60it/s]\u001b[A\n",
      " 21%|███████▋                             | 2199/10570 [00:06<00:31, 265.81it/s]\u001b[A\n",
      " 21%|███████▊                             | 2226/10570 [00:07<00:31, 264.01it/s]\u001b[A\n",
      " 21%|███████▉                             | 2253/10570 [00:07<00:31, 264.61it/s]\u001b[A\n",
      " 22%|███████▉                             | 2281/10570 [00:07<00:30, 268.49it/s]\u001b[A\n",
      " 22%|████████                             | 2313/10570 [00:07<00:29, 282.58it/s]\u001b[A\n",
      " 22%|████████▏                            | 2348/10570 [00:07<00:27, 300.53it/s]\u001b[A\n",
      " 23%|████████▎                            | 2379/10570 [00:07<00:27, 295.97it/s]\u001b[A\n",
      " 23%|████████▍                            | 2411/10570 [00:07<00:26, 302.91it/s]\u001b[A\n",
      " 23%|████████▌                            | 2444/10570 [00:07<00:26, 307.96it/s]\u001b[A\n",
      " 23%|████████▋                            | 2475/10570 [00:07<00:26, 302.60it/s]\u001b[A\n",
      " 24%|████████▊                            | 2506/10570 [00:08<00:27, 290.48it/s]\u001b[A\n",
      " 24%|████████▉                            | 2537/10570 [00:08<00:27, 295.09it/s]\u001b[A\n",
      " 24%|████████▉                            | 2567/10570 [00:08<00:27, 286.21it/s]\u001b[A\n",
      " 25%|█████████                            | 2598/10570 [00:08<00:27, 291.79it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2628/10570 [00:08<00:27, 292.44it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2664/10570 [00:08<00:25, 310.61it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2699/10570 [00:08<00:24, 319.51it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2732/10570 [00:08<00:26, 298.56it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2763/10570 [00:08<00:25, 300.66it/s]\u001b[A\n",
      " 26%|█████████▊                           | 2794/10570 [00:08<00:26, 295.20it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2824/10570 [00:09<00:26, 287.30it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2854/10570 [00:09<00:26, 290.47it/s]\u001b[A\n",
      " 27%|██████████                           | 2884/10570 [00:09<00:26, 289.67it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2914/10570 [00:09<00:27, 283.12it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2945/10570 [00:09<00:26, 290.46it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2977/10570 [00:09<00:25, 297.13it/s]\u001b[A\n",
      " 28%|██████████▌                          | 3010/10570 [00:09<00:24, 305.35it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3045/10570 [00:09<00:23, 316.30it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3077/10570 [00:09<00:24, 308.45it/s]\u001b[A\n",
      " 29%|██████████▉                          | 3110/10570 [00:10<00:23, 312.93it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3142/10570 [00:10<00:24, 307.51it/s]\u001b[A\n",
      " 30%|███████████                          | 3173/10570 [00:10<00:25, 295.38it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3203/10570 [00:10<00:25, 293.52it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3233/10570 [00:10<00:26, 279.97it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3262/10570 [00:10<00:26, 279.74it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3291/10570 [00:10<00:26, 271.88it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3319/10570 [00:10<00:26, 269.36it/s]\u001b[A\n",
      " 32%|███████████▋                         | 3349/10570 [00:10<00:26, 276.58it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3382/10570 [00:11<00:24, 289.64it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3412/10570 [00:11<00:25, 281.58it/s]\u001b[A\n",
      " 33%|████████████                         | 3442/10570 [00:11<00:24, 285.64it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3471/10570 [00:11<00:25, 273.38it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3503/10570 [00:11<00:24, 284.71it/s]\u001b[A\n",
      " 33%|████████████▍                        | 3537/10570 [00:11<00:23, 298.65it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3570/10570 [00:11<00:22, 305.83it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3601/10570 [00:11<00:23, 295.25it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3631/10570 [00:11<00:23, 289.68it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3661/10570 [00:12<00:26, 262.44it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3688/10570 [00:12<00:26, 256.64it/s]\u001b[A\n",
      " 35%|█████████████                        | 3717/10570 [00:12<00:25, 264.31it/s]\u001b[A\n",
      " 35%|█████████████                        | 3746/10570 [00:12<00:25, 270.55it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3778/10570 [00:12<00:23, 283.54it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3807/10570 [00:12<00:23, 285.32it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3837/10570 [00:12<00:23, 288.53it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3870/10570 [00:12<00:22, 300.10it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3904/10570 [00:12<00:21, 309.41it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3938/10570 [00:12<00:20, 316.29it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3970/10570 [00:13<00:21, 304.58it/s]\u001b[A\n",
      " 38%|██████████████                       | 4001/10570 [00:13<00:22, 286.04it/s]\u001b[A\n",
      " 38%|██████████████                       | 4032/10570 [00:13<00:22, 291.70it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4064/10570 [00:13<00:21, 299.24it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4098/10570 [00:13<00:20, 308.49it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4130/10570 [00:13<00:21, 302.13it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4161/10570 [00:13<00:23, 272.25it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4189/10570 [00:13<00:25, 250.50it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4215/10570 [00:13<00:26, 235.96it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4245/10570 [00:14<00:25, 251.97it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4271/10570 [00:14<00:30, 203.96it/s]\u001b[A\n",
      " 41%|███████████████                      | 4294/10570 [00:14<00:34, 183.70it/s]\u001b[A\n",
      " 41%|███████████████                      | 4314/10570 [00:14<00:36, 173.48it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4341/10570 [00:14<00:31, 195.34it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4371/10570 [00:14<00:28, 220.71it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4399/10570 [00:14<00:26, 234.03it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4426/10570 [00:14<00:25, 242.46it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4453/10570 [00:15<00:24, 247.74it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4481/10570 [00:15<00:23, 254.85it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4509/10570 [00:15<00:23, 259.95it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4537/10570 [00:15<00:22, 263.02it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4565/10570 [00:15<00:22, 265.96it/s]\u001b[A\n",
      " 43%|████████████████                     | 4594/10570 [00:15<00:21, 272.10it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4622/10570 [00:15<00:23, 253.66it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4648/10570 [00:15<00:23, 251.67it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4676/10570 [00:15<00:22, 257.78it/s]\u001b[A\n",
      " 45%|████████████████▍                    | 4708/10570 [00:16<00:21, 273.52it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4737/10570 [00:16<00:21, 275.90it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4765/10570 [00:16<00:21, 265.37it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4792/10570 [00:16<00:21, 265.09it/s]\u001b[A\n",
      " 46%|████████████████▊                    | 4820/10570 [00:16<00:21, 268.64it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4848/10570 [00:16<00:21, 271.32it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4876/10570 [00:16<00:22, 249.68it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4904/10570 [00:16<00:22, 257.33it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4932/10570 [00:16<00:21, 262.40it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4964/10570 [00:17<00:20, 276.97it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4992/10570 [00:17<00:20, 274.22it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5020/10570 [00:17<00:20, 273.98it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5050/10570 [00:17<00:19, 281.31it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5082/10570 [00:17<00:18, 291.96it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5112/10570 [00:17<00:18, 292.12it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 5142/10570 [00:17<00:18, 290.98it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5172/10570 [00:17<00:18, 284.63it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5205/10570 [00:17<00:18, 296.65it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5235/10570 [00:17<00:18, 286.81it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5266/10570 [00:18<00:18, 291.80it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5296/10570 [00:18<00:18, 284.93it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 5325/10570 [00:18<00:18, 277.81it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5353/10570 [00:18<00:19, 269.60it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5382/10570 [00:18<00:18, 274.12it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5411/10570 [00:18<00:18, 276.91it/s]\u001b[A\n",
      " 51%|███████████████████                  | 5440/10570 [00:18<00:18, 280.56it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5469/10570 [00:18<00:19, 260.90it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5496/10570 [00:18<00:19, 262.69it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5527/10570 [00:19<00:18, 275.19it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5559/10570 [00:19<00:17, 286.02it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5589/10570 [00:19<00:17, 289.96it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5619/10570 [00:19<00:17, 275.35it/s]\u001b[A\n",
      " 53%|███████████████████▊                 | 5648/10570 [00:19<00:17, 279.16it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5677/10570 [00:19<00:18, 265.49it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5706/10570 [00:19<00:17, 271.88it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5734/10570 [00:19<00:17, 273.07it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5762/10570 [00:19<00:17, 269.88it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5790/10570 [00:19<00:18, 265.00it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5817/10570 [00:20<00:17, 266.22it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5844/10570 [00:20<00:17, 265.33it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5871/10570 [00:20<00:17, 265.36it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5899/10570 [00:20<00:17, 267.26it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5926/10570 [00:20<00:17, 267.33it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5957/10570 [00:20<00:16, 277.76it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5987/10570 [00:20<00:16, 281.76it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6017/10570 [00:20<00:15, 285.31it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6049/10570 [00:20<00:15, 294.68it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6079/10570 [00:21<00:15, 285.75it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6108/10570 [00:21<00:15, 280.30it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6137/10570 [00:21<00:16, 274.41it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6165/10570 [00:21<00:16, 268.52it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6192/10570 [00:21<00:16, 268.44it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6222/10570 [00:21<00:15, 275.41it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6250/10570 [00:21<00:15, 271.79it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6284/10570 [00:21<00:14, 289.42it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6314/10570 [00:21<00:14, 290.35it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6345/10570 [00:21<00:14, 294.13it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6375/10570 [00:22<00:14, 286.62it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6404/10570 [00:22<00:15, 275.27it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6436/10570 [00:22<00:14, 287.25it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6468/10570 [00:22<00:13, 294.13it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6498/10570 [00:22<00:13, 294.61it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6529/10570 [00:22<00:13, 298.80it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6559/10570 [00:22<00:14, 283.13it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6589/10570 [00:22<00:13, 285.37it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6618/10570 [00:22<00:13, 283.59it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6650/10570 [00:23<00:13, 292.21it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6683/10570 [00:23<00:12, 299.32it/s]\u001b[A\n",
      " 64%|███████████████████████▍             | 6713/10570 [00:23<00:13, 293.62it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6743/10570 [00:23<00:13, 293.27it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6776/10570 [00:23<00:12, 302.41it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6809/10570 [00:23<00:12, 308.03it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6840/10570 [00:23<00:12, 307.19it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6871/10570 [00:23<00:12, 303.06it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6902/10570 [00:23<00:12, 287.02it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6936/10570 [00:23<00:12, 299.14it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6969/10570 [00:24<00:11, 307.71it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7003/10570 [00:24<00:11, 316.12it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7035/10570 [00:24<00:11, 308.89it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7067/10570 [00:24<00:11, 300.03it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7098/10570 [00:24<00:11, 298.64it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7128/10570 [00:24<00:11, 292.08it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7158/10570 [00:24<00:11, 289.90it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7188/10570 [00:24<00:11, 291.39it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7218/10570 [00:24<00:11, 292.75it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7251/10570 [00:25<00:10, 303.39it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7282/10570 [00:25<00:11, 292.28it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7312/10570 [00:25<00:11, 285.94it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7341/10570 [00:25<00:12, 248.78it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7371/10570 [00:25<00:12, 260.98it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7398/10570 [00:25<00:12, 252.72it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7424/10570 [00:25<00:13, 231.44it/s]\u001b[A\n",
      " 71%|██████████████████████████           | 7452/10570 [00:25<00:12, 243.46it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7484/10570 [00:25<00:11, 263.73it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7517/10570 [00:26<00:10, 281.35it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7546/10570 [00:26<00:10, 283.04it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7575/10570 [00:26<00:10, 273.53it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7604/10570 [00:26<00:10, 276.74it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7638/10570 [00:26<00:09, 293.89it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7668/10570 [00:26<00:09, 290.81it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7700/10570 [00:26<00:09, 295.59it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7730/10570 [00:26<00:10, 282.78it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7759/10570 [00:26<00:10, 266.32it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7788/10570 [00:27<00:10, 271.03it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7816/10570 [00:27<00:11, 238.76it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7847/10570 [00:27<00:10, 254.11it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7874/10570 [00:27<00:10, 257.14it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7905/10570 [00:27<00:09, 270.79it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7938/10570 [00:27<00:09, 286.76it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7968/10570 [00:27<00:09, 286.18it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8001/10570 [00:27<00:08, 296.21it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8033/10570 [00:27<00:08, 302.14it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8064/10570 [00:28<00:08, 296.44it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8094/10570 [00:28<00:08, 283.87it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8124/10570 [00:28<00:08, 287.43it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8153/10570 [00:28<00:08, 283.08it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 8182/10570 [00:28<00:08, 283.26it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8211/10570 [00:28<00:08, 272.04it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8245/10570 [00:28<00:07, 290.70it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8275/10570 [00:28<00:08, 275.28it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8303/10570 [00:28<00:08, 272.81it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8336/10570 [00:28<00:07, 288.02it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8369/10570 [00:29<00:07, 298.59it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8400/10570 [00:29<00:07, 299.30it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8431/10570 [00:29<00:07, 294.11it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8462/10570 [00:29<00:07, 298.53it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8492/10570 [00:29<00:06, 298.68it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8522/10570 [00:29<00:06, 298.74it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8552/10570 [00:29<00:06, 294.65it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8582/10570 [00:29<00:06, 293.14it/s]\u001b[A\n",
      " 81%|██████████████████████████████▏      | 8612/10570 [00:29<00:06, 293.74it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8645/10570 [00:30<00:06, 302.07it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8676/10570 [00:30<00:06, 286.29it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8707/10570 [00:30<00:06, 292.76it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8737/10570 [00:30<00:06, 288.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8767/10570 [00:30<00:06, 285.91it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8796/10570 [00:30<00:06, 286.60it/s]\u001b[A\n",
      " 83%|██████████████████████████████▉      | 8825/10570 [00:30<00:06, 281.12it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8854/10570 [00:30<00:06, 269.80it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8883/10570 [00:30<00:06, 274.03it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8913/10570 [00:30<00:05, 279.89it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8942/10570 [00:31<00:06, 266.76it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8975/10570 [00:31<00:05, 283.75it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9004/10570 [00:31<00:05, 283.75it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9033/10570 [00:31<00:05, 277.30it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9062/10570 [00:31<00:05, 279.80it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9092/10570 [00:31<00:05, 284.21it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9123/10570 [00:31<00:04, 291.70it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9156/10570 [00:31<00:04, 300.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9187/10570 [00:31<00:04, 288.24it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9217/10570 [00:32<00:04, 289.46it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9250/10570 [00:32<00:04, 300.68it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9281/10570 [00:32<00:04, 289.20it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9311/10570 [00:32<00:04, 278.87it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9340/10570 [00:32<00:04, 280.67it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9371/10570 [00:32<00:04, 287.15it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9400/10570 [00:32<00:04, 287.54it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9433/10570 [00:32<00:03, 297.94it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9467/10570 [00:32<00:03, 308.74it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9499/10570 [00:32<00:03, 310.67it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9533/10570 [00:33<00:03, 317.02it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9565/10570 [00:33<00:03, 315.61it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9597/10570 [00:33<00:03, 306.96it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9629/10570 [00:33<00:03, 309.56it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9661/10570 [00:33<00:02, 305.69it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9692/10570 [00:33<00:02, 302.90it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9723/10570 [00:33<00:02, 303.98it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9754/10570 [00:33<00:02, 301.07it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9785/10570 [00:33<00:02, 289.35it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9815/10570 [00:34<00:02, 285.36it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9844/10570 [00:34<00:02, 282.74it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9875/10570 [00:34<00:02, 289.25it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9904/10570 [00:34<00:02, 282.16it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9938/10570 [00:34<00:02, 297.01it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9968/10570 [00:34<00:02, 292.16it/s]\u001b[A\n",
      " 95%|███████████████████████████████████  | 9999/10570 [00:34<00:01, 296.33it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10029/10570 [00:34<00:01, 281.04it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10059/10570 [00:34<00:01, 284.48it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10091/10570 [00:34<00:01, 293.13it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10125/10570 [00:35<00:01, 305.27it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10156/10570 [00:35<00:01, 297.99it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10186/10570 [00:35<00:01, 281.93it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10215/10570 [00:35<00:01, 277.18it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10244/10570 [00:35<00:01, 279.75it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10273/10570 [00:35<00:01, 277.62it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10301/10570 [00:35<00:01, 265.33it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10332/10570 [00:35<00:00, 276.78it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10362/10570 [00:35<00:00, 282.69it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10393/10570 [00:36<00:00, 290.25it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10424/10570 [00:36<00:00, 295.82it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10456/10570 [00:36<00:00, 302.44it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10487/10570 [00:36<00:00, 279.04it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10517/10570 [00:36<00:00, 283.20it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:36<00:00, 288.24it/s]\u001b[A\n",
      "04/28/2022 10:46:19 - INFO - utils_qa - Saving predictions to ./squad_roberta_base/eval_predictions.json.\n",
      "04/28/2022 10:46:19 - INFO - utils_qa - Saving nbest_preds to ./squad_roberta_base/eval_nbest_predictions.json.\n",
      "04/28/2022 10:46:25 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1349/1349 [04:28<00:00,  5.03it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match = 84.5222\n",
      "  eval_f1          = 90.8742\n",
      "  eval_samples     =   10790\n",
      "[INFO|modelcard.py:460] 2022-04-28 10:46:26,022 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "!sh train_roberta.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b7f72d-4af2-4e54-ad1f-f51c145b8920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88ba49b-3f9f-4d1d-9e2f-04a97ed30572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccc524f1-25c5-45a9-a576-38a9e3cd5540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.19.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.2.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (8.0.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: dill<0.3.5 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3e8604-337f-4103-abc1-f2b47c6c761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==1.9.0 in /opt/conda/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85fb9443-814e-4e07-875e-02133d710c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/22/2022 21:13:32 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "05/22/2022 21:13:32 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_bert_base_uncased_advanced/runs/May22_21-13-32_n9x0pczu2t,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_bert_base_uncased_advanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_bert_base_uncased_advanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/22/2022 21:13:33 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/22/2022 21:13:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/22/2022 21:13:33 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
      "05/22/2022 21:13:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/22/2022 21:13:33 - WARNING - datasets.builder - Reusing dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "05/22/2022 21:13:33 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 522.88it/s]\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 21:13:33,124 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 21:13:33,124 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 21:13:33,251 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 21:13:33,251 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 21:13:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 21:13:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 21:13:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 21:13:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 21:13:33,633 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 21:13:33,688 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 21:13:33,689 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1953] 2022-05-22 21:13:33,780 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-22 21:13:35,278 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing Advanced_BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-22 21:13:35,279 >> Some weights of Advanced_BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "05/22/2022 21:13:35 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-44e1c2b24ff802ea.arrow\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/22/2022 21:13:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-e1739d2955cb0037.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:35<00:00,  3.22s/ba]\n",
      "[INFO|trainer.py:502] 2022-05-22 21:14:15,723 >> Using amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-22 21:14:15,964 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-22 21:14:15,964 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1421] 2022-05-22 21:14:15,964 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1422] 2022-05-22 21:14:15,964 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-22 21:14:15,964 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-22 21:14:15,964 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-22 21:14:15,964 >>   Total optimization steps = 22131\n",
      "  0%|                                                 | 0/22131 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 2.2478, 'learning_rate': 2.932628439745154e-05, 'epoch': 0.07}         \n",
      "  2%|▊                                    | 500/22131 [01:52<1:22:02,  4.39it/s][INFO|trainer.py:2340] 2022-05-22 21:16:08,955 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:16:08,959 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:16:12,657 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:16:12,661 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:16:12,665 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/special_tokens_map.json\n",
      "  4%|█▎                                   | 811/22131 [03:13<1:20:00,  4.44it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.4832, 'learning_rate': 2.8649857665717773e-05, 'epoch': 0.14}        \n",
      "  5%|█▋                                  | 1000/22131 [03:56<1:19:52,  4.41it/s][INFO|trainer.py:2340] 2022-05-22 21:18:12,949 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:18:12,955 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:18:16,748 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:18:16,779 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:18:16,782 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3773, 'learning_rate': 2.7972075369391354e-05, 'epoch': 0.2}         \n",
      "  7%|██▍                                 | 1500/22131 [06:01<1:18:49,  4.36it/s][INFO|trainer.py:2340] 2022-05-22 21:20:17,073 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:20:17,114 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:20:20,982 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:20:21,003 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:20:21,006 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.3201, 'learning_rate': 2.729429307306493e-05, 'epoch': 0.27}         \n",
      "  9%|███▎                                | 2000/22131 [08:05<1:15:27,  4.45it/s][INFO|trainer.py:2340] 2022-05-22 21:22:21,076 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:22:21,080 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:22:25,074 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:22:25,112 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:22:25,116 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2573, 'learning_rate': 2.6616510776738512e-05, 'epoch': 0.34}        \n",
      " 11%|████                                | 2500/22131 [10:09<1:13:48,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:24:25,297 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:24:25,302 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:24:29,168 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:24:29,173 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:24:29,177 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1877, 'learning_rate': 2.5938728480412093e-05, 'epoch': 0.41}        \n",
      " 14%|████▉                               | 3000/22131 [12:13<1:11:57,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:26:29,162 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:26:29,167 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:26:32,866 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:26:32,947 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:26:32,952 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1945, 'learning_rate': 2.526094618408567e-05, 'epoch': 0.47}         \n",
      " 16%|█████▋                              | 3500/22131 [14:16<1:09:58,  4.44it/s][INFO|trainer.py:2340] 2022-05-22 21:28:32,598 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:28:32,603 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:28:36,161 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:28:36,197 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:28:36,201 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1256, 'learning_rate': 2.458316388775925e-05, 'epoch': 0.54}         \n",
      " 18%|██████▌                             | 4000/22131 [16:19<1:08:16,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:30:35,617 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:30:35,622 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:30:39,471 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:30:39,501 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:30:39,506 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0909, 'learning_rate': 2.3905381591432832e-05, 'epoch': 0.61}        \n",
      " 20%|███████▎                            | 4500/22131 [18:23<1:06:44,  4.40it/s][INFO|trainer.py:2340] 2022-05-22 21:32:39,241 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:32:39,246 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:32:42,694 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:32:42,699 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:32:42,703 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/special_tokens_map.json\n",
      " 22%|███████▉                            | 4904/22131 [20:04<1:04:48,  4.43it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.0876, 'learning_rate': 2.3228954859699066e-05, 'epoch': 0.68}        \n",
      " 23%|████████▏                           | 5000/22131 [20:26<1:04:32,  4.42it/s][INFO|trainer.py:2340] 2022-05-22 21:34:42,270 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:34:42,273 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:34:47,450 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:34:47,454 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:34:47,458 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.1142, 'learning_rate': 2.2551172563372647e-05, 'epoch': 0.75}        \n",
      " 25%|████████▉                           | 5500/22131 [22:31<1:02:35,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:36:47,559 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:36:47,563 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:36:51,399 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:36:51,422 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:36:51,426 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0767, 'learning_rate': 2.1873390267046228e-05, 'epoch': 0.81}        \n",
      " 27%|█████████▊                          | 6000/22131 [24:35<1:00:45,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:38:51,245 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:38:51,250 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:38:54,891 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:38:54,922 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:38:54,925 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0307, 'learning_rate': 2.1195607970719805e-05, 'epoch': 0.88}        \n",
      " 29%|███████████▏                          | 6500/22131 [26:38<58:51,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:40:54,797 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:40:54,801 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:40:58,760 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:40:58,764 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:40:58,768 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0635, 'learning_rate': 2.0517825674393386e-05, 'epoch': 0.95}        \n",
      " 32%|████████████                          | 7000/22131 [28:42<57:13,  4.41it/s][INFO|trainer.py:2340] 2022-05-22 21:42:58,458 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:42:58,462 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:43:01,883 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:43:01,908 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:43:01,911 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.9621, 'learning_rate': 1.9840043378066967e-05, 'epoch': 1.02}        \n",
      " 34%|████████████▉                         | 7500/22131 [30:45<55:34,  4.39it/s][INFO|trainer.py:2340] 2022-05-22 21:45:01,779 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-7500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:45:01,783 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:45:05,233 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:45:05,236 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:45:05,240 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 0.7428, 'learning_rate': 1.9162261081740544e-05, 'epoch': 1.08}        \n",
      " 36%|█████████████▋                        | 8000/22131 [32:48<53:06,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:47:04,894 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-8000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:47:04,899 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:47:08,806 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:47:08,811 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:47:08,846 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.7243, 'learning_rate': 1.8484478785414125e-05, 'epoch': 1.15}        \n",
      " 38%|██████████████▌                       | 8500/22131 [34:53<51:13,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:49:09,228 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-8500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:49:09,240 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:49:12,811 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:49:12,814 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:49:12,820 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/special_tokens_map.json\n",
      " 39%|██████████████▉                       | 8705/22131 [35:50<50:20,  4.45it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.7211, 'learning_rate': 1.7808052053680356e-05, 'epoch': 1.22}        \n",
      " 41%|███████████████▍                      | 9000/22131 [36:57<50:03,  4.37it/s][INFO|trainer.py:2340] 2022-05-22 21:51:13,008 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-9000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:51:13,048 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:51:16,401 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:51:16,415 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:51:16,420 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.7475, 'learning_rate': 1.7130269757353937e-05, 'epoch': 1.29}        \n",
      " 43%|████████████████▎                     | 9500/22131 [39:00<47:27,  4.44it/s][INFO|trainer.py:2340] 2022-05-22 21:53:16,537 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-9500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:53:16,541 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:53:20,147 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:53:20,164 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:53:20,169 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.7308, 'learning_rate': 1.645248746102752e-05, 'epoch': 1.36}         \n",
      " 45%|████████████████▋                    | 10000/22131 [41:03<45:38,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:55:19,696 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-10000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:55:19,702 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:55:23,614 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:55:23,656 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:55:23,658 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.7678, 'learning_rate': 1.57747051647011e-05, 'epoch': 1.42}          \n",
      " 47%|█████████████████▌                   | 10500/22131 [43:07<43:52,  4.42it/s][INFO|trainer.py:2340] 2022-05-22 21:57:23,571 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-10500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:57:23,575 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:57:26,975 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:57:26,978 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:57:26,982 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/special_tokens_map.json\n",
      " 49%|██████████████████▏                  | 10869/22131 [44:40<42:25,  4.42it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.7205, 'learning_rate': 1.5098278432967332e-05, 'epoch': 1.49}        \n",
      " 50%|██████████████████▍                  | 11000/22131 [45:10<41:54,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 21:59:26,608 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-11000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 21:59:26,613 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 21:59:30,169 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 21:59:30,173 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 21:59:30,176 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.7363, 'learning_rate': 1.4420496136640912e-05, 'epoch': 1.56}        \n",
      " 52%|███████████████████▏                 | 11500/22131 [47:14<40:09,  4.41it/s][INFO|trainer.py:2340] 2022-05-22 22:01:30,063 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-11500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:01:30,067 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:01:33,557 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:01:33,584 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:01:33,588 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.7337, 'learning_rate': 1.374271384031449e-05, 'epoch': 1.63}         \n",
      " 54%|████████████████████                 | 12000/22131 [49:17<38:05,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:03:33,052 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-12000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:03:33,056 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:03:36,718 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:03:36,724 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:03:36,728 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/special_tokens_map.json\n",
      " 55%|████████████████████▌                | 12264/22131 [50:26<37:08,  4.43it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.7023, 'learning_rate': 1.3066287108580723e-05, 'epoch': 1.69}        \n",
      " 56%|████████████████████▉                | 12500/22131 [51:20<36:05,  4.45it/s][INFO|trainer.py:2340] 2022-05-22 22:05:36,224 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-12500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:05:36,228 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:05:39,861 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:05:39,869 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:05:39,873 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.7364, 'learning_rate': 1.2388504812254306e-05, 'epoch': 1.76}        \n",
      " 59%|█████████████████████▋               | 13000/22131 [53:24<34:43,  4.38it/s][INFO|trainer.py:2340] 2022-05-22 22:07:40,486 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-13000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:07:40,491 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:07:44,213 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:07:44,217 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:07:44,219 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.7258, 'learning_rate': 1.1710722515927885e-05, 'epoch': 1.83}        \n",
      " 61%|██████████████████████▌              | 13500/22131 [55:29<32:42,  4.40it/s][INFO|trainer.py:2340] 2022-05-22 22:09:45,216 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-13500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:09:45,220 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:09:50,074 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:09:50,077 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:09:50,081 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.7372, 'learning_rate': 1.1032940219601464e-05, 'epoch': 1.9}         \n",
      " 63%|███████████████████████▍             | 14000/22131 [57:34<30:34,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:11:50,107 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-14000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:11:50,115 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:11:53,798 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:11:53,802 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:11:53,806 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.7127, 'learning_rate': 1.0355157923275045e-05, 'epoch': 1.97}        \n",
      " 66%|████████████████████████▏            | 14500/22131 [59:38<28:41,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:13:54,027 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-14500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:13:54,032 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:13:57,584 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:13:57,588 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:13:57,590 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 0.562, 'learning_rate': 9.677375626948624e-06, 'epoch': 2.03}          \n",
      " 68%|███████████████████████▋           | 15000/22131 [1:01:41<26:49,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:15:57,880 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-15000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:15:57,890 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:16:01,717 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:16:01,723 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:16:01,730 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 0.4745, 'learning_rate': 8.999593330622204e-06, 'epoch': 2.1}          \n",
      " 70%|████████████████████████▌          | 15500/22131 [1:03:45<24:50,  4.45it/s][INFO|trainer.py:2340] 2022-05-22 22:18:01,904 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-15500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:18:01,909 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:18:05,752 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:18:05,759 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:18:05,769 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 0.4567, 'learning_rate': 8.321811034295784e-06, 'epoch': 2.17}         \n",
      " 72%|█████████████████████████▎         | 16000/22131 [1:05:49<23:23,  4.37it/s][INFO|trainer.py:2340] 2022-05-22 22:20:05,905 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-16000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:20:05,944 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:20:09,633 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:20:09,636 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:20:09,670 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-16000/special_tokens_map.json\n",
      " 73%|█████████████████████████▌         | 16126/22131 [1:06:29<22:28,  4.45it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.4773, 'learning_rate': 7.645384302562018e-06, 'epoch': 2.24}         \n",
      " 75%|██████████████████████████         | 16500/22131 [1:07:53<21:07,  4.44it/s][INFO|trainer.py:2340] 2022-05-22 22:22:09,662 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-16500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:22:09,666 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:22:14,054 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:22:14,058 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:22:14,089 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 0.4581, 'learning_rate': 6.967602006235597e-06, 'epoch': 2.3}          \n",
      " 77%|██████████████████████████▉        | 17000/22131 [1:09:58<19:17,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:24:14,976 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-17000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:24:15,013 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:24:18,695 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:24:18,699 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:24:18,732 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 0.4819, 'learning_rate': 6.289819709909177e-06, 'epoch': 2.37}         \n",
      " 79%|███████████████████████████▋       | 17500/22131 [1:12:03<17:30,  4.41it/s][INFO|trainer.py:2340] 2022-05-22 22:26:19,614 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-17500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:26:19,649 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:26:23,655 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:26:23,662 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:26:23,693 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 0.4624, 'learning_rate': 5.612037413582758e-06, 'epoch': 2.44}         \n",
      " 81%|████████████████████████████▍      | 18000/22131 [1:14:08<15:32,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:28:24,302 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-18000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:28:24,338 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:28:27,731 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:28:27,738 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:28:27,743 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 0.4911, 'learning_rate': 4.934255117256338e-06, 'epoch': 2.51}         \n",
      " 84%|█████████████████████████████▎     | 18500/22131 [1:16:12<13:36,  4.44it/s][INFO|trainer.py:2340] 2022-05-22 22:30:28,072 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-18500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:30:28,109 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:30:31,892 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:30:31,896 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:30:31,930 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-18500/special_tokens_map.json\n",
      " 86%|█████████████████████████████▉     | 18937/22131 [1:18:01<11:59,  4.44it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.4694, 'learning_rate': 4.25782838552257e-06, 'epoch': 2.58}          \n",
      " 86%|██████████████████████████████     | 19000/22131 [1:18:16<11:46,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:32:32,198 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-19000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:32:32,204 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:32:35,700 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:32:35,705 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:32:35,738 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 0.4613, 'learning_rate': 3.5800460891961506e-06, 'epoch': 2.64}        \n",
      " 88%|██████████████████████████████▊    | 19500/22131 [1:20:19<09:53,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:34:35,578 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-19500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:34:35,583 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:34:39,228 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:34:39,232 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:34:39,236 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-19500/special_tokens_map.json\n",
      "{'loss': 0.4673, 'learning_rate': 2.90226379286973e-06, 'epoch': 2.71}          \n",
      " 90%|███████████████████████████████▋   | 20000/22131 [1:22:22<08:01,  4.43it/s][INFO|trainer.py:2340] 2022-05-22 22:36:38,704 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-20000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:36:38,710 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:36:42,425 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:36:42,430 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:36:42,432 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.4409, 'learning_rate': 2.2244814965433105e-06, 'epoch': 2.78}        \n",
      " 93%|████████████████████████████████▍  | 20500/22131 [1:24:26<06:06,  4.45it/s][INFO|trainer.py:2340] 2022-05-22 22:38:42,113 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-20500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:38:42,117 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:38:45,437 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:38:45,441 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:38:45,478 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-20500/special_tokens_map.json\n",
      " 94%|████████████████████████████████▉  | 20812/22131 [1:25:46<04:58,  4.42it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.4701, 'learning_rate': 1.5480547648095431e-06, 'epoch': 2.85}        \n",
      " 95%|█████████████████████████████████▏ | 21000/22131 [1:26:29<04:16,  4.41it/s][INFO|trainer.py:2340] 2022-05-22 22:40:45,139 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-21000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:40:45,176 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:40:48,789 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:40:48,798 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:40:48,800 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 0.4853, 'learning_rate': 8.702724684831233e-07, 'epoch': 2.91}         \n",
      " 97%|██████████████████████████████████ | 21500/22131 [1:28:32<02:21,  4.45it/s][INFO|trainer.py:2340] 2022-05-22 22:42:48,426 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-21500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:42:48,430 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:42:51,789 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:42:51,792 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:42:51,796 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-21500/special_tokens_map.json\n",
      "{'loss': 0.4604, 'learning_rate': 1.9249017215670325e-07, 'epoch': 2.98}        \n",
      " 99%|██████████████████████████████████▊| 22000/22131 [1:30:35<00:29,  4.44it/s][INFO|trainer.py:2340] 2022-05-22 22:44:51,337 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-22000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:44:51,375 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:44:54,863 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:44:54,867 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:44:54,871 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-22000/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 22131/22131 [1:31:15<00:00,  4.44it/s][INFO|trainer.py:1662] 2022-05-22 22:45:31,092 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5475.1278, 'train_samples_per_second': 48.505, 'train_steps_per_second': 4.042, 'train_loss': 0.815679027634422, 'epoch': 3.0}\n",
      "100%|███████████████████████████████████| 22131/22131 [1:31:15<00:00,  4.04it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-22 22:45:31,286 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 22:45:31,290 >> Configuration saved in ./squad_bert_base_uncased_advanced/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 22:45:34,503 >> Model weights saved in ./squad_bert_base_uncased_advanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 22:45:34,506 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 22:45:34,539 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        3.0\n",
      "  train_loss               =     0.8157\n",
      "  train_runtime            = 1:31:15.12\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     48.505\n",
      "  train_steps_per_second   =      4.042\n",
      "05/22/2022 22:45:34 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-22 22:45:34,912 >> The following columns in the evaluation set don't have a corresponding argument in `Advanced_BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `Advanced_BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-22 22:45:34,914 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-22 22:45:34,914 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2595] 2022-05-22 22:45:34,914 >>   Batch size = 12\n",
      "100%|████████████████████████████████████████▉| 898/899 [01:04<00:00, 13.95it/s]05/22/2022 22:46:48 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 41/10570 [00:00<00:25, 408.92it/s]\u001b[A\n",
      "  1%|▎                                      | 90/10570 [00:00<00:23, 451.43it/s]\u001b[A\n",
      "  1%|▍                                     | 139/10570 [00:00<00:22, 466.46it/s]\u001b[A\n",
      "  2%|▋                                     | 186/10570 [00:00<00:22, 459.42it/s]\u001b[A\n",
      "  2%|▊                                     | 232/10570 [00:00<00:23, 435.24it/s]\u001b[A\n",
      "  3%|▉                                     | 276/10570 [00:00<00:28, 360.85it/s]\u001b[A\n",
      "  3%|█▏                                    | 320/10570 [00:00<00:26, 381.50it/s]\u001b[A\n",
      "  3%|█▎                                    | 368/10570 [00:00<00:25, 407.55it/s]\u001b[A\n",
      "  4%|█▍                                    | 412/10570 [00:00<00:24, 415.31it/s]\u001b[A\n",
      "  4%|█▋                                    | 458/10570 [00:01<00:23, 426.53it/s]\u001b[A\n",
      "  5%|█▊                                    | 506/10570 [00:01<00:22, 439.83it/s]\u001b[A\n",
      "  5%|█▉                                    | 553/10570 [00:01<00:22, 445.36it/s]\u001b[A\n",
      "  6%|██▏                                   | 598/10570 [00:01<00:22, 445.52it/s]\u001b[A\n",
      "  6%|██▎                                   | 645/10570 [00:01<00:21, 452.17it/s]\u001b[A\n",
      "  7%|██▍                                   | 694/10570 [00:01<00:21, 460.95it/s]\u001b[A\n",
      "  7%|██▋                                   | 741/10570 [00:01<00:21, 452.74it/s]\u001b[A\n",
      "  7%|██▊                                   | 787/10570 [00:01<00:22, 432.84it/s]\u001b[A\n",
      "  8%|██▉                                   | 831/10570 [00:01<00:23, 417.66it/s]\u001b[A\n",
      "  8%|███▏                                  | 877/10570 [00:02<00:22, 427.67it/s]\u001b[A\n",
      "  9%|███▎                                  | 920/10570 [00:02<00:22, 422.93it/s]\u001b[A\n",
      "  9%|███▍                                  | 964/10570 [00:02<00:22, 425.16it/s]\u001b[A\n",
      " 10%|███▌                                 | 1008/10570 [00:02<00:22, 426.75it/s]\u001b[A\n",
      " 10%|███▋                                 | 1051/10570 [00:02<00:22, 420.59it/s]\u001b[A\n",
      " 10%|███▊                                 | 1094/10570 [00:02<00:22, 418.28it/s]\u001b[A\n",
      " 11%|███▉                                 | 1136/10570 [00:02<00:22, 413.51it/s]\u001b[A\n",
      " 11%|████▏                                | 1179/10570 [00:02<00:22, 417.31it/s]\u001b[A\n",
      " 12%|████▎                                | 1221/10570 [00:02<00:22, 411.95it/s]\u001b[A\n",
      " 12%|████▍                                | 1267/10570 [00:02<00:21, 423.24it/s]\u001b[A\n",
      " 12%|████▌                                | 1315/10570 [00:03<00:21, 439.06it/s]\u001b[A\n",
      " 13%|████▊                                | 1361/10570 [00:03<00:20, 444.66it/s]\u001b[A\n",
      " 13%|████▉                                | 1406/10570 [00:03<00:21, 435.95it/s]\u001b[A\n",
      " 14%|█████                                | 1452/10570 [00:03<00:20, 442.31it/s]\u001b[A\n",
      " 14%|█████▏                               | 1497/10570 [00:03<00:20, 441.69it/s]\u001b[A\n",
      " 15%|█████▍                               | 1542/10570 [00:03<00:20, 443.24it/s]\u001b[A\n",
      " 15%|█████▌                               | 1587/10570 [00:03<00:20, 438.13it/s]\u001b[A\n",
      " 15%|█████▋                               | 1633/10570 [00:03<00:20, 443.91it/s]\u001b[A\n",
      " 16%|█████▉                               | 1682/10570 [00:03<00:19, 457.44it/s]\u001b[A\n",
      " 16%|██████                               | 1728/10570 [00:03<00:19, 455.95it/s]\u001b[A\n",
      " 17%|██████▏                              | 1774/10570 [00:04<00:19, 454.15it/s]\u001b[A\n",
      " 17%|██████▎                              | 1821/10570 [00:04<00:19, 456.59it/s]\u001b[A\n",
      " 18%|██████▌                              | 1867/10570 [00:04<00:19, 452.68it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████| 899/899 [01:18<00:00, 13.95it/s]\u001b[A\n",
      " 19%|██████▊                              | 1961/10570 [00:04<00:19, 451.68it/s]\u001b[A\n",
      " 19%|███████                              | 2007/10570 [00:04<00:18, 452.17it/s]\u001b[A\n",
      " 19%|███████▏                             | 2054/10570 [00:04<00:18, 454.93it/s]\u001b[A\n",
      " 20%|███████▎                             | 2101/10570 [00:04<00:18, 458.67it/s]\u001b[A\n",
      " 20%|███████▌                             | 2147/10570 [00:04<00:20, 412.29it/s]\u001b[A\n",
      " 21%|███████▋                             | 2191/10570 [00:05<00:19, 419.59it/s]\u001b[A\n",
      " 21%|███████▊                             | 2239/10570 [00:05<00:19, 433.38it/s]\u001b[A\n",
      " 22%|███████▉                             | 2285/10570 [00:05<00:18, 439.03it/s]\u001b[A\n",
      " 22%|████████▏                            | 2331/10570 [00:05<00:18, 444.26it/s]\u001b[A\n",
      " 22%|████████▎                            | 2376/10570 [00:05<00:18, 441.34it/s]\u001b[A\n",
      " 23%|████████▍                            | 2421/10570 [00:05<00:18, 435.54it/s]\u001b[A\n",
      " 23%|████████▋                            | 2465/10570 [00:05<00:18, 432.24it/s]\u001b[A\n",
      " 24%|████████▊                            | 2509/10570 [00:05<00:19, 416.46it/s]\u001b[A\n",
      " 24%|████████▉                            | 2551/10570 [00:05<00:19, 415.89it/s]\u001b[A\n",
      " 25%|█████████                            | 2594/10570 [00:05<00:19, 418.13it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2641/10570 [00:06<00:18, 431.87it/s]\u001b[A\n",
      " 25%|█████████▍                           | 2687/10570 [00:06<00:17, 438.03it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2731/10570 [00:06<00:18, 433.79it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2779/10570 [00:06<00:17, 445.66it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2826/10570 [00:06<00:17, 450.59it/s]\u001b[A\n",
      " 27%|██████████                           | 2872/10570 [00:06<00:17, 447.18it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2917/10570 [00:06<00:17, 439.90it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2962/10570 [00:06<00:17, 427.34it/s]\u001b[A\n",
      " 28%|██████████▌                          | 3006/10570 [00:06<00:17, 430.80it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3051/10570 [00:07<00:17, 435.33it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3095/10570 [00:07<00:17, 436.01it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3139/10570 [00:07<00:17, 434.50it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3183/10570 [00:07<00:17, 423.39it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3227/10570 [00:07<00:17, 426.17it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3270/10570 [00:07<00:17, 423.65it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3314/10570 [00:07<00:17, 426.37it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3359/10570 [00:07<00:16, 431.28it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3403/10570 [00:07<00:16, 432.36it/s]\u001b[A\n",
      " 33%|████████████                         | 3447/10570 [00:07<00:16, 429.18it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3490/10570 [00:08<00:16, 429.01it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3535/10570 [00:08<00:16, 433.97it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3579/10570 [00:08<00:16, 421.42it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3622/10570 [00:08<00:16, 420.55it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3666/10570 [00:08<00:16, 424.07it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3710/10570 [00:08<00:16, 427.59it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3753/10570 [00:08<00:15, 427.51it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3797/10570 [00:08<00:15, 430.04it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3841/10570 [00:08<00:15, 432.86it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3886/10570 [00:08<00:15, 435.68it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3930/10570 [00:09<00:15, 436.27it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3975/10570 [00:09<00:15, 438.58it/s]\u001b[A\n",
      " 38%|██████████████                       | 4019/10570 [00:09<00:15, 429.87it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4064/10570 [00:09<00:14, 434.24it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4108/10570 [00:09<00:15, 425.23it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4151/10570 [00:09<00:17, 363.88it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4189/10570 [00:09<00:19, 332.65it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4224/10570 [00:09<00:19, 317.40it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4260/10570 [00:10<00:19, 327.38it/s]\u001b[A\n",
      " 41%|███████████████                      | 4294/10570 [00:10<00:24, 258.73it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4323/10570 [00:10<00:24, 256.11it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4363/10570 [00:10<00:21, 289.89it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4405/10570 [00:10<00:19, 322.40it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4449/10570 [00:10<00:17, 352.71it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4494/10570 [00:10<00:16, 376.83it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4534/10570 [00:10<00:15, 380.15it/s]\u001b[A\n",
      " 43%|████████████████                     | 4576/10570 [00:10<00:15, 390.82it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4616/10570 [00:11<00:15, 385.09it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4656/10570 [00:11<00:15, 379.56it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4698/10570 [00:11<00:15, 390.88it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4740/10570 [00:11<00:14, 398.73it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4781/10570 [00:11<00:15, 385.67it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4826/10570 [00:11<00:14, 402.79it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4867/10570 [00:11<00:14, 388.83it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4912/10570 [00:11<00:14, 403.81it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4954/10570 [00:11<00:13, 407.47it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4995/10570 [00:11<00:13, 407.09it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5036/10570 [00:12<00:13, 400.46it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5080/10570 [00:12<00:13, 410.06it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5123/10570 [00:12<00:13, 414.42it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5166/10570 [00:12<00:12, 416.29it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5209/10570 [00:12<00:12, 418.26it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5251/10570 [00:12<00:12, 415.39it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5296/10570 [00:12<00:12, 424.85it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5339/10570 [00:12<00:12, 423.60it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5382/10570 [00:12<00:12, 413.91it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5424/10570 [00:13<00:12, 410.63it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5466/10570 [00:13<00:13, 381.24it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5505/10570 [00:13<00:13, 371.54it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 5544/10570 [00:13<00:13, 375.75it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5582/10570 [00:13<00:13, 370.15it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5620/10570 [00:13<00:13, 372.45it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5658/10570 [00:13<00:13, 364.00it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5703/10570 [00:13<00:12, 387.53it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5744/10570 [00:13<00:12, 393.34it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5784/10570 [00:13<00:12, 394.88it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5825/10570 [00:14<00:11, 395.53it/s]\u001b[A\n",
      " 55%|████████████████████▌                | 5865/10570 [00:14<00:12, 391.56it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5908/10570 [00:14<00:11, 400.97it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5950/10570 [00:14<00:11, 405.33it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5992/10570 [00:14<00:11, 409.07it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6033/10570 [00:14<00:11, 405.80it/s]\u001b[A\n",
      " 57%|█████████████████████▎               | 6074/10570 [00:14<00:11, 393.60it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6116/10570 [00:14<00:11, 401.01it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6159/10570 [00:14<00:10, 408.72it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6200/10570 [00:15<00:10, 407.69it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6241/10570 [00:15<00:10, 396.83it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6284/10570 [00:15<00:10, 405.11it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6327/10570 [00:15<00:10, 411.75it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6372/10570 [00:15<00:09, 421.40it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6415/10570 [00:15<00:10, 408.57it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6460/10570 [00:15<00:09, 420.06it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6503/10570 [00:15<00:09, 420.60it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6546/10570 [00:15<00:09, 422.95it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6589/10570 [00:15<00:09, 416.47it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6633/10570 [00:16<00:09, 422.17it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6676/10570 [00:16<00:09, 419.35it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6718/10570 [00:16<00:09, 412.28it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6762/10570 [00:16<00:09, 418.65it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6805/10570 [00:16<00:08, 420.29it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6848/10570 [00:16<00:08, 417.86it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6891/10570 [00:16<00:08, 418.61it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6933/10570 [00:16<00:08, 418.25it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6977/10570 [00:16<00:08, 423.23it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7023/10570 [00:16<00:08, 432.81it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7068/10570 [00:17<00:08, 435.56it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7112/10570 [00:17<00:08, 431.83it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7156/10570 [00:17<00:07, 433.70it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7200/10570 [00:17<00:07, 422.63it/s]\u001b[A\n",
      " 69%|█████████████████████████▎           | 7243/10570 [00:17<00:07, 419.90it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7286/10570 [00:17<00:07, 419.72it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7330/10570 [00:17<00:07, 423.84it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7373/10570 [00:17<00:08, 391.73it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7415/10570 [00:17<00:07, 399.51it/s]\u001b[A\n",
      " 71%|██████████████████████████           | 7459/10570 [00:18<00:07, 408.67it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7503/10570 [00:18<00:07, 416.52it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7546/10570 [00:18<00:07, 418.61it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7590/10570 [00:18<00:07, 423.04it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7634/10570 [00:18<00:06, 427.18it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7677/10570 [00:18<00:06, 416.00it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7722/10570 [00:18<00:06, 424.00it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7765/10570 [00:18<00:06, 406.70it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7806/10570 [00:18<00:06, 399.14it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7848/10570 [00:18<00:06, 403.37it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7892/10570 [00:19<00:06, 412.89it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7936/10570 [00:19<00:06, 420.48it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7980/10570 [00:19<00:06, 423.99it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8024/10570 [00:19<00:05, 426.83it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8067/10570 [00:19<00:05, 418.06it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8111/10570 [00:19<00:05, 422.87it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8154/10570 [00:19<00:05, 416.14it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8196/10570 [00:19<00:05, 407.81it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8237/10570 [00:19<00:05, 398.32it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8277/10570 [00:20<00:06, 378.00it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8317/10570 [00:20<00:05, 381.63it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8357/10570 [00:20<00:05, 384.99it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8396/10570 [00:20<00:05, 382.33it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8438/10570 [00:20<00:05, 391.97it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8482/10570 [00:20<00:05, 403.62it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8523/10570 [00:20<00:05, 398.77it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8565/10570 [00:20<00:04, 401.95it/s]\u001b[A\n",
      " 81%|██████████████████████████████▏      | 8606/10570 [00:20<00:04, 401.19it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8647/10570 [00:20<00:04, 402.76it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8688/10570 [00:21<00:04, 402.82it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8731/10570 [00:21<00:04, 407.99it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8773/10570 [00:21<00:04, 409.25it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8815/10570 [00:21<00:04, 412.39it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8858/10570 [00:21<00:04, 415.43it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8901/10570 [00:21<00:03, 419.34it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8943/10570 [00:21<00:03, 419.13it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8987/10570 [00:21<00:03, 422.78it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9030/10570 [00:21<00:03, 416.14it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9072/10570 [00:21<00:03, 416.01it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9114/10570 [00:22<00:03, 417.17it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9156/10570 [00:22<00:04, 317.91it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9197/10570 [00:22<00:04, 340.02it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9240/10570 [00:22<00:03, 361.89it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9282/10570 [00:22<00:03, 376.33it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9323/10570 [00:22<00:03, 385.37it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9366/10570 [00:22<00:03, 396.99it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9407/10570 [00:22<00:03, 387.62it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9447/10570 [00:22<00:02, 387.03it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9491/10570 [00:23<00:02, 399.59it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9535/10570 [00:23<00:02, 410.47it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9580/10570 [00:23<00:02, 419.30it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9624/10570 [00:23<00:02, 425.30it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9667/10570 [00:23<00:02, 423.91it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9710/10570 [00:23<00:02, 425.51it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9753/10570 [00:23<00:01, 425.07it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9796/10570 [00:23<00:01, 415.62it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9838/10570 [00:23<00:01, 401.75it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9880/10570 [00:24<00:01, 406.29it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9924/10570 [00:24<00:01, 413.65it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9967/10570 [00:24<00:01, 416.25it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10009/10570 [00:24<00:01, 415.76it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10051/10570 [00:24<00:01, 416.83it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▍ | 10093/10570 [00:24<00:01, 409.13it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10134/10570 [00:24<00:01, 406.63it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10175/10570 [00:24<00:00, 399.64it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10219/10570 [00:24<00:00, 411.33it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10261/10570 [00:24<00:00, 413.56it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10305/10570 [00:25<00:00, 419.05it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10350/10570 [00:25<00:00, 425.56it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10393/10570 [00:25<00:00, 423.09it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10436/10570 [00:25<00:00, 424.59it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10479/10570 [00:25<00:00, 416.95it/s]\u001b[A\n",
      "100%|███████████████████████████████████▊| 10524/10570 [00:25<00:00, 425.91it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:25<00:00, 411.77it/s]\u001b[A\n",
      "05/22/2022 22:47:14 - INFO - utils_qa - Saving predictions to ./squad_bert_base_uncased_advanced/eval_predictions.json.\n",
      "05/22/2022 22:47:14 - INFO - utils_qa - Saving nbest_preds to ./squad_bert_base_uncased_advanced/eval_nbest_predictions.json.\n",
      "05/22/2022 22:47:19 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████| 899/899 [01:44<00:00,  8.59it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     3.0\n",
      "  eval_exact_match =  80.492\n",
      "  eval_f1          = 88.1831\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train_bert_squadv1_advanced.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0c671c-1528-491f-af85-e81638f91a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Question-Answer'\n",
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38f72b7d-411c-4798-90e0-94a04f438f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.2 MB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.2.2-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 32.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
      "\u001b[K     |████████████████████████████████| 84 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 26.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting pyarrow>=6.0.0\n",
      "  Downloading pyarrow-8.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 29.4 MB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 37.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill<0.3.5\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 34.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 26.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, pyarrow, multiprocess, huggingface-hub, transformers, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 5.0.0\n",
      "    Uninstalling pyarrow-5.0.0:\n",
      "      Successfully uninstalled pyarrow-5.0.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.2.2 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.6.0 multiprocess-0.70.12.2 pyarrow-8.0.0 responses-0.18.0 tokenizers-0.12.1 transformers-4.19.2 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc8ad5f7-d83a-4d0d-ba5b-dc86aa797af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 19.9 MB/s eta 0:00:01    |██▍                             | 61.4 MB 24.3 MB/s eta 0:00:32\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06eedc55-4b8b-47a8-9763-12a26a0faf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/22/2022 15:53:34 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "05/22/2022 15:53:34 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_bert_base_uncased_advanced/runs/May22_15-53-34_nb4jpi8nx7,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_bert_base_uncased_advanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_bert_base_uncased_advanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpgh7_iu1a\n",
      "Downloading builder script: 5.27kB [00:00, 3.90MB/s]                            \n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0f1393e0ef7efecb23b106e95c7037e66766cd17683910fd5f1a8bd2130ae03a.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp5x3lmgrr\n",
      "Downloading metadata: 2.36kB [00:00, 2.43MB/s]                                  \n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/22/2022 15:53:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/41157d6f5aa3734ab3208b74d0ef588fcfae4bf85507267bf9a0258c3b1c5056.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/22/2022 15:53:34 - INFO - datasets.builder - No config specified, defaulting to the single config: squad/plain_text\n",
      "05/22/2022 15:53:34 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/22/2022 15:53:34 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "05/22/2022 15:53:34 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]05/22/2022 15:53:35 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp_af385a0\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  88%|█████████████████▌  | 7.13M/8.12M [00:00<00:00, 71.3MB/s]\u001b[A\n",
      "Downloading data: 14.3MB [00:00, 63.0MB/s]                                      \u001b[A\n",
      "Downloading data: 20.6MB [00:00, 60.6MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 63.1MB/s]\u001b[A\n",
      "05/22/2022 15:53:35 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "05/22/2022 15:53:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.25s/it]05/22/2022 15:53:36 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpl30gxz5z\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 74.2MB/s]                                      \u001b[A\n",
      "05/22/2022 15:53:36 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "05/22/2022 15:53:36 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.35it/s]\n",
      "05/22/2022 15:53:36 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "05/22/2022 15:53:36 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1363.56it/s]\n",
      "05/22/2022 15:53:36 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "05/22/2022 15:53:36 - INFO - datasets.builder - Generating train split\n",
      "05/22/2022 15:53:43 - INFO - datasets.builder - Generating validation split     \n",
      "05/22/2022 15:53:44 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 651.44it/s]\n",
      "[INFO|hub.py:583] 2022-05-22 15:53:44,981 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpggfg1e_g\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 564kB/s]\n",
      "[INFO|hub.py:587] 2022-05-22 15:53:45,061 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|hub.py:595] 2022-05-22 15:53:45,061 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 15:53:45,061 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 15:53:45,062 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-22 15:53:45,120 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmph0enads6\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 24.6kB/s]\n",
      "[INFO|hub.py:587] 2022-05-22 15:53:45,224 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|hub.py:595] 2022-05-22 15:53:45,224 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 15:53:45,291 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 15:53:45,292 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-22 15:53:45,413 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmptrrwzgdf\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 6.81MB/s]\n",
      "[INFO|hub.py:587] 2022-05-22 15:53:45,517 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:595] 2022-05-22 15:53:45,517 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:583] 2022-05-22 15:53:45,591 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpc937icjn\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 9.66MB/s]\n",
      "[INFO|hub.py:587] 2022-05-22 15:53:45,709 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|hub.py:595] 2022-05-22 15:53:45,709 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 15:53:45,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 15:53:45,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 15:53:45,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 15:53:45,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1782] 2022-05-22 15:53:45,904 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:659] 2022-05-22 15:53:45,966 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:708] 2022-05-22 15:53:45,967 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.19.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-22 15:53:46,062 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpntd_dt2v\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [00:18<00:00, 23.2MB/s]\n",
      "[INFO|hub.py:587] 2022-05-22 15:54:05,129 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|hub.py:595] 2022-05-22 15:54:05,130 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1953] 2022-05-22 15:54:05,130 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2254] 2022-05-22 15:54:06,915 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing Advanced_BertForQuestionAnswering: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2265] 2022-05-22 15:54:06,915 >> Some weights of Advanced_BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/22/2022 15:54:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-44e1c2b24ff802ea.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:43<00:00,  2.02ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/22/2022 15:54:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-5cc55b4fe96b757c.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:42<00:00,  3.90s/ba]\n",
      "05/22/2022 15:55:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpq270ii0o\n",
      "Downloading builder script: 4.50kB [00:00, 3.71MB/s]                            \n",
      "05/22/2022 15:55:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/22/2022 15:55:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/a69481e409c8541bd80d11906b4a58184450bb7e5048dcfe264babfabf66edce.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/22/2022 15:55:34 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpxqxjdcy0\n",
      "Downloading extra modules: 3.31kB [00:00, 2.93MB/s]                             \n",
      "05/22/2022 15:55:34 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.2.2/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "05/22/2022 15:55:34 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/36255ff6f36b2aa58d7203f50857a1d907a5d1400fd760cb78ea483716f9d1f6.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "[INFO|trainer.py:502] 2022-05-22 15:55:38,743 >> Using amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1419] 2022-05-22 15:55:38,899 >> ***** Running training *****\n",
      "[INFO|trainer.py:1420] 2022-05-22 15:55:38,899 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1421] 2022-05-22 15:55:38,899 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1422] 2022-05-22 15:55:38,899 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1423] 2022-05-22 15:55:38,899 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1424] 2022-05-22 15:55:38,899 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1425] 2022-05-22 15:55:38,899 >>   Total optimization steps = 7377\n",
      "  0%|                                                  | 0/7377 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 2.2614, 'learning_rate': 2.7982919886132576e-05, 'epoch': 0.07}        \n",
      "  7%|██▌                                   | 500/7377 [06:02<1:22:50,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:01:41,395 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:01:41,401 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:01:45,185 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:01:45,200 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:01:45,202 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.4686, 'learning_rate': 2.5949572997153315e-05, 'epoch': 0.14}        \n",
      " 14%|█████                                | 1000/7377 [12:15<1:16:55,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:07:53,992 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:07:53,998 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:07:57,640 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:07:57,668 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:07:57,675 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3685, 'learning_rate': 2.3916226108174057e-05, 'epoch': 0.2}         \n",
      " 20%|███████▌                             | 1500/7377 [18:27<1:10:37,  1.39it/s][INFO|trainer.py:2340] 2022-05-22 16:14:06,219 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:14:06,258 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:14:10,099 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:14:10,133 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:14:10,137 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.293, 'learning_rate': 2.1882879219194796e-05, 'epoch': 0.27}         \n",
      " 27%|██████████                           | 2000/7377 [24:40<1:04:56,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:20:19,032 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:20:19,037 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:20:22,560 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:20:22,586 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:20:22,589 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2314, 'learning_rate': 1.9849532330215535e-05, 'epoch': 0.34}        \n",
      " 34%|█████████████▏                         | 2500/7377 [30:52<58:47,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:26:31,444 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:26:31,452 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:26:35,376 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:26:35,381 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:26:35,417 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1711, 'learning_rate': 1.7816185441236277e-05, 'epoch': 0.41}        \n",
      " 41%|███████████████▊                       | 3000/7377 [37:05<52:42,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:32:44,632 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:32:44,676 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:32:48,719 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:32:48,753 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:32:48,757 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1801, 'learning_rate': 1.5782838552257016e-05, 'epoch': 0.47}        \n",
      " 47%|██████████████████▌                    | 3500/7377 [43:19<46:35,  1.39it/s][INFO|trainer.py:2340] 2022-05-22 16:38:58,562 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:38:58,605 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:39:02,197 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:39:02,200 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:39:02,243 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1141, 'learning_rate': 1.3749491663277755e-05, 'epoch': 0.54}        \n",
      " 54%|█████████████████████▏                 | 4000/7377 [49:32<40:53,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:45:11,740 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:45:11,775 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:45:15,759 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:45:15,763 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:45:15,797 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0607, 'learning_rate': 1.1716144774298496e-05, 'epoch': 0.61}        \n",
      " 61%|███████████████████████▊               | 4500/7377 [55:45<34:44,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 16:51:24,884 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:51:24,920 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:51:28,547 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:51:28,578 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:51:28,587 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/special_tokens_map.json\n",
      " 64%|████████████████████████▊              | 4692/7377 [58:15<32:31,  1.38it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.0684, 'learning_rate': 9.686864579097194e-06, 'epoch': 0.68}         \n",
      " 68%|█████████████████████████            | 5000/7377 [1:01:58<28:34,  1.39it/s][INFO|trainer.py:2340] 2022-05-22 16:57:37,864 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 16:57:37,869 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 16:57:42,136 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 16:57:42,141 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 16:57:42,144 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.0929, 'learning_rate': 7.653517690117934e-06, 'epoch': 0.75}         \n",
      " 75%|███████████████████████████▌         | 5500/7377 [1:08:13<22:40,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 17:03:52,510 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 17:03:52,515 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 17:03:56,570 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 17:03:56,575 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 17:03:56,606 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0586, 'learning_rate': 5.620170801138674e-06, 'epoch': 0.81}         \n",
      " 81%|██████████████████████████████       | 6000/7377 [1:14:26<16:35,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 17:10:05,300 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 17:10:05,306 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 17:10:09,221 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 17:10:09,225 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 17:10:09,230 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0112, 'learning_rate': 3.586823912159414e-06, 'epoch': 0.88}         \n",
      " 88%|████████████████████████████████▌    | 6500/7377 [1:20:39<10:34,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 17:16:18,493 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6500\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 17:16:18,498 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 17:16:22,314 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 17:16:22,319 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 17:16:22,349 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0378, 'learning_rate': 1.5534770231801546e-06, 'epoch': 0.95}        \n",
      " 95%|███████████████████████████████████  | 7000/7377 [1:26:52<04:32,  1.38it/s][INFO|trainer.py:2340] 2022-05-22 17:22:31,507 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-7000\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 17:22:31,512 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 17:22:34,942 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 17:22:34,994 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 17:22:34,998 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/special_tokens_map.json\n",
      " 96%|███████████████████████████████████▍ | 7064/7377 [1:27:48<03:46,  1.38it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1597: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "100%|█████████████████████████████████████| 7377/7377 [1:31:35<00:00,  1.39it/s][INFO|trainer.py:1662] 2022-05-22 17:27:14,764 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5495.8654, 'train_samples_per_second': 16.107, 'train_steps_per_second': 1.342, 'train_loss': 1.2326806849122485, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████| 7377/7377 [1:31:35<00:00,  1.34it/s]\n",
      "[INFO|trainer.py:2340] 2022-05-22 17:27:14,886 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced\n",
      "[INFO|configuration_utils.py:446] 2022-05-22 17:27:14,891 >> Configuration saved in ./squad_bert_base_uncased_advanced/config.json\n",
      "[INFO|modeling_utils.py:1542] 2022-05-22 17:27:18,457 >> Model weights saved in ./squad_bert_base_uncased_advanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2108] 2022-05-22 17:27:18,461 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2114] 2022-05-22 17:27:18,467 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.2327\n",
      "  train_runtime            = 1:31:35.86\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     16.107\n",
      "  train_steps_per_second   =      1.342\n",
      "05/22/2022 17:27:18 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:622] 2022-05-22 17:27:18,580 >> The following columns in the evaluation set don't have a corresponding argument in `Advanced_BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `Advanced_BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2590] 2022-05-22 17:27:18,583 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2592] 2022-05-22 17:27:18,583 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2595] 2022-05-22 17:27:18,583 >>   Batch size = 12\n",
      "100%|█████████████████████████████████████████| 899/899 [03:58<00:00,  4.17it/s]05/22/2022 17:31:29 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 34/10570 [00:00<00:31, 336.58it/s]\u001b[A\n",
      "  1%|▎                                      | 71/10570 [00:00<00:29, 355.99it/s]\u001b[A\n",
      "  1%|▍                                     | 111/10570 [00:00<00:27, 375.55it/s]\u001b[A\n",
      "  1%|▌                                     | 150/10570 [00:00<00:27, 378.02it/s]\u001b[A\n",
      "  2%|▋                                     | 189/10570 [00:00<00:27, 380.38it/s]\u001b[A\n",
      "  2%|▊                                     | 228/10570 [00:00<00:28, 366.46it/s]\u001b[A\n",
      "  3%|▉                                     | 265/10570 [00:00<00:37, 274.00it/s]\u001b[A\n",
      "  3%|█                                     | 296/10570 [00:00<00:39, 260.46it/s]\u001b[A\n",
      "  3%|█▏                                    | 331/10570 [00:01<00:36, 281.80it/s]\u001b[A\n",
      "  3%|█▎                                    | 367/10570 [00:01<00:33, 300.84it/s]\u001b[A\n",
      "  4%|█▍                                    | 401/10570 [00:01<00:32, 311.32it/s]\u001b[A\n",
      "  4%|█▌                                    | 436/10570 [00:01<00:31, 320.63it/s]\u001b[A\n",
      "  4%|█▋                                    | 470/10570 [00:01<00:31, 316.44it/s]\u001b[A\n",
      "  5%|█▊                                    | 508/10570 [00:01<00:30, 333.58it/s]\u001b[A\n",
      "  5%|█▉                                    | 547/10570 [00:01<00:28, 349.53it/s]\u001b[A\n",
      "  6%|██                                    | 586/10570 [00:01<00:27, 358.49it/s]\u001b[A\n",
      "  6%|██▏                                   | 623/10570 [00:01<00:28, 352.95it/s]\u001b[A\n",
      "  6%|██▎                                   | 659/10570 [00:01<00:28, 347.26it/s]\u001b[A\n",
      "  7%|██▍                                   | 695/10570 [00:02<00:28, 349.58it/s]\u001b[A\n",
      "  7%|██▋                                   | 731/10570 [00:02<00:29, 334.24it/s]\u001b[A\n",
      "  7%|██▊                                   | 765/10570 [00:02<00:29, 331.68it/s]\u001b[A\n",
      "  8%|██▊                                   | 799/10570 [00:02<00:30, 317.07it/s]\u001b[A\n",
      "  8%|██▉                                   | 831/10570 [00:02<00:31, 304.77it/s]\u001b[A\n",
      "  8%|███▏                                  | 871/10570 [00:02<00:29, 330.66it/s]\u001b[A\n",
      "  9%|███▎                                  | 908/10570 [00:02<00:28, 340.64it/s]\u001b[A\n",
      "  9%|███▍                                  | 944/10570 [00:02<00:27, 343.86it/s]\u001b[A\n",
      "  9%|███▌                                  | 979/10570 [00:02<00:28, 334.09it/s]\u001b[A\n",
      " 10%|███▌                                 | 1013/10570 [00:03<00:29, 329.02it/s]\u001b[A\n",
      " 10%|███▋                                 | 1047/10570 [00:03<00:29, 318.72it/s]\u001b[A\n",
      " 10%|███▊                                 | 1084/10570 [00:03<00:28, 332.79it/s]\u001b[A\n",
      " 11%|███▉                                 | 1118/10570 [00:03<00:28, 328.78it/s]\u001b[A\n",
      " 11%|████                                 | 1152/10570 [00:03<00:28, 328.28it/s]\u001b[A\n",
      " 11%|████▏                                | 1189/10570 [00:03<00:27, 338.28it/s]\u001b[A\n",
      " 12%|████▎                                | 1223/10570 [00:03<00:28, 329.60it/s]\u001b[A\n",
      " 12%|████▍                                | 1259/10570 [00:03<00:27, 336.19it/s]\u001b[A\n",
      " 12%|████▌                                | 1298/10570 [00:03<00:26, 351.65it/s]\u001b[A\n",
      " 13%|████▋                                | 1337/10570 [00:04<00:25, 361.80it/s]\u001b[A\n",
      " 13%|████▊                                | 1374/10570 [00:04<00:25, 362.08it/s]\u001b[A\n",
      " 13%|████▉                                | 1411/10570 [00:04<00:25, 359.08it/s]\u001b[A\n",
      " 14%|█████                                | 1449/10570 [00:04<00:25, 362.72it/s]\u001b[A\n",
      " 14%|█████▏                               | 1486/10570 [00:04<00:25, 350.45it/s]\u001b[A\n",
      " 14%|█████▎                               | 1523/10570 [00:04<00:25, 353.42it/s]\u001b[A\n",
      " 15%|█████▍                               | 1562/10570 [00:04<00:24, 361.59it/s]\u001b[A\n",
      " 15%|█████▌                               | 1599/10570 [00:04<00:25, 345.98it/s]\u001b[A\n",
      " 15%|█████▋                               | 1634/10570 [00:04<00:25, 344.99it/s]\u001b[A\n",
      " 16%|█████▊                               | 1673/10570 [00:04<00:24, 356.83it/s]\u001b[A\n",
      " 16%|█████▉                               | 1711/10570 [00:05<00:24, 363.33it/s]\u001b[A\n",
      " 17%|██████▏                              | 1751/10570 [00:05<00:23, 371.64it/s]\u001b[A\n",
      " 17%|██████▎                              | 1790/10570 [00:05<00:23, 375.70it/s]\u001b[A\n",
      " 17%|██████▍                              | 1831/10570 [00:05<00:22, 384.79it/s]\u001b[A\n",
      " 18%|██████▌                              | 1870/10570 [00:05<00:23, 376.68it/s]\u001b[A\n",
      " 18%|██████▋                              | 1910/10570 [00:05<00:22, 383.34it/s]\u001b[A\n",
      " 18%|██████▊                              | 1949/10570 [00:05<00:23, 372.21it/s]\u001b[A\n",
      " 19%|██████▉                              | 1987/10570 [00:05<00:23, 363.89it/s]\u001b[A\n",
      " 19%|███████                              | 2024/10570 [00:05<00:24, 346.93it/s]\u001b[A\n",
      " 19%|███████▏                             | 2059/10570 [00:06<00:24, 345.82it/s]\u001b[A\n",
      " 20%|███████▎                             | 2095/10570 [00:06<00:24, 348.79it/s]\u001b[A\n",
      " 20%|███████▍                             | 2130/10570 [00:06<00:27, 308.19it/s]\u001b[A\n",
      " 20%|███████▌                             | 2165/10570 [00:06<00:26, 318.02it/s]\u001b[A\n",
      " 21%|███████▋                             | 2200/10570 [00:06<00:25, 325.52it/s]\u001b[A\n",
      " 21%|███████▊                             | 2237/10570 [00:06<00:24, 334.66it/s]\u001b[A\n",
      " 21%|███████▉                             | 2272/10570 [00:06<00:24, 338.97it/s]\u001b[A\n",
      " 22%|████████                             | 2309/10570 [00:06<00:23, 345.79it/s]\u001b[A\n",
      " 22%|████████▏                            | 2345/10570 [00:06<00:23, 346.22it/s]\u001b[A\n",
      " 23%|████████▎                            | 2380/10570 [00:06<00:24, 339.48it/s]\u001b[A\n",
      " 23%|████████▍                            | 2415/10570 [00:07<00:24, 339.17it/s]\u001b[A\n",
      " 23%|████████▌                            | 2450/10570 [00:07<00:25, 323.15it/s]\u001b[A\n",
      " 24%|████████▋                            | 2487/10570 [00:07<00:24, 333.89it/s]\u001b[A\n",
      " 24%|████████▊                            | 2521/10570 [00:07<00:25, 314.73it/s]\u001b[A\n",
      " 24%|████████▉                            | 2553/10570 [00:07<00:25, 309.41it/s]\u001b[A\n",
      " 25%|█████████                            | 2590/10570 [00:07<00:24, 325.40it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2629/10570 [00:07<00:23, 343.53it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2668/10570 [00:07<00:22, 356.87it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2704/10570 [00:07<00:22, 345.43it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2739/10570 [00:08<00:22, 341.95it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2781/10570 [00:08<00:21, 363.14it/s]\u001b[A\n",
      " 27%|█████████▊                           | 2819/10570 [00:08<00:21, 366.66it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2856/10570 [00:08<00:21, 361.53it/s]\u001b[A\n",
      " 27%|██████████▏                          | 2893/10570 [00:08<00:23, 326.20it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2927/10570 [00:08<00:23, 327.10it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2962/10570 [00:08<00:22, 333.11it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2996/10570 [00:08<00:22, 333.89it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3032/10570 [00:08<00:22, 338.68it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3067/10570 [00:09<00:23, 321.60it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3102/10570 [00:09<00:22, 326.96it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3138/10570 [00:09<00:22, 336.12it/s]\u001b[A\n",
      " 30%|███████████                          | 3172/10570 [00:09<00:22, 331.79it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3206/10570 [00:09<00:22, 328.86it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3239/10570 [00:09<00:22, 326.41it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3275/10570 [00:09<00:21, 333.20it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3309/10570 [00:09<00:21, 334.14it/s]\u001b[A\n",
      " 32%|███████████▋                         | 3343/10570 [00:09<00:22, 320.69it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3376/10570 [00:09<00:22, 313.10it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3411/10570 [00:10<00:22, 322.36it/s]\u001b[A\n",
      " 33%|████████████                         | 3449/10570 [00:10<00:21, 337.53it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3486/10570 [00:10<00:20, 345.65it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3524/10570 [00:10<00:19, 353.09it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3560/10570 [00:10<00:20, 347.21it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3595/10570 [00:10<00:20, 344.92it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3630/10570 [00:10<00:20, 341.09it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3665/10570 [00:10<00:20, 333.74it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3699/10570 [00:10<00:20, 327.69it/s]\u001b[A\n",
      " 35%|█████████████                        | 3733/10570 [00:11<00:20, 330.58it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3768/10570 [00:11<00:20, 334.39it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3805/10570 [00:11<00:19, 342.48it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3842/10570 [00:11<00:19, 348.23it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3877/10570 [00:11<00:19, 340.99it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3912/10570 [00:11<00:19, 339.17it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3948/10570 [00:11<00:19, 343.52it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3987/10570 [00:11<00:18, 354.42it/s]\u001b[A\n",
      " 38%|██████████████                       | 4023/10570 [00:11<00:18, 353.36it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4059/10570 [00:11<00:18, 347.48it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4095/10570 [00:12<00:18, 347.39it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4130/10570 [00:12<00:20, 319.53it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4163/10570 [00:12<00:25, 255.07it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4191/10570 [00:12<00:26, 239.19it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4217/10570 [00:12<00:26, 237.13it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4252/10570 [00:12<00:23, 264.86it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4280/10570 [00:12<00:29, 212.27it/s]\u001b[A\n",
      " 41%|███████████████                      | 4304/10570 [00:13<00:34, 179.38it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4334/10570 [00:13<00:30, 203.86it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4369/10570 [00:13<00:26, 236.19it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4399/10570 [00:13<00:24, 250.78it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4430/10570 [00:13<00:23, 264.60it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4464/10570 [00:13<00:21, 283.24it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4496/10570 [00:13<00:20, 293.14it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4527/10570 [00:13<00:20, 295.28it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4558/10570 [00:13<00:20, 298.07it/s]\u001b[A\n",
      " 43%|████████████████                     | 4589/10570 [00:14<00:20, 298.59it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4620/10570 [00:14<00:20, 285.62it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4649/10570 [00:14<00:21, 278.80it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4681/10570 [00:14<00:20, 288.35it/s]\u001b[A\n",
      " 45%|████████████████▍                    | 4711/10570 [00:14<00:20, 289.61it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4741/10570 [00:14<00:21, 267.77it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4769/10570 [00:14<00:22, 258.76it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4796/10570 [00:14<00:22, 254.52it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4824/10570 [00:14<00:22, 258.39it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4851/10570 [00:15<00:22, 257.04it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4877/10570 [00:15<00:23, 245.93it/s]\u001b[A\n",
      " 47%|█████████████████▏                   | 4916/10570 [00:15<00:19, 282.85it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4952/10570 [00:15<00:18, 302.57it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4987/10570 [00:15<00:17, 313.45it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5019/10570 [00:15<00:17, 314.33it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5053/10570 [00:15<00:17, 321.10it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5088/10570 [00:15<00:16, 327.97it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5124/10570 [00:15<00:16, 336.60it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5158/10570 [00:16<00:16, 334.17it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5193/10570 [00:16<00:15, 336.75it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5227/10570 [00:16<00:15, 336.07it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5262/10570 [00:16<00:15, 337.92it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5301/10570 [00:16<00:14, 352.26it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 5337/10570 [00:16<00:15, 347.41it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5372/10570 [00:16<00:15, 336.70it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5408/10570 [00:16<00:15, 341.16it/s]\u001b[A\n",
      " 51%|███████████████████                  | 5443/10570 [00:16<00:15, 331.42it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5477/10570 [00:16<00:16, 304.84it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5508/10570 [00:17<00:17, 293.41it/s]\u001b[A\n",
      " 52%|███████████████████▍                 | 5538/10570 [00:17<00:17, 292.21it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5568/10570 [00:17<00:17, 293.68it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5598/10570 [00:17<00:17, 289.37it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5629/10570 [00:17<00:16, 294.63it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5659/10570 [00:17<00:16, 291.25it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5697/10570 [00:17<00:15, 316.04it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5732/10570 [00:17<00:14, 325.39it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5767/10570 [00:17<00:14, 329.67it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5802/10570 [00:18<00:14, 333.14it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5839/10570 [00:18<00:13, 341.67it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5874/10570 [00:18<00:13, 341.41it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5909/10570 [00:18<00:14, 325.47it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5942/10570 [00:18<00:14, 310.54it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5974/10570 [00:18<00:14, 312.42it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6008/10570 [00:18<00:14, 318.19it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6040/10570 [00:18<00:14, 318.14it/s]\u001b[A\n",
      " 57%|█████████████████████▎               | 6072/10570 [00:18<00:14, 316.55it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6104/10570 [00:18<00:14, 304.95it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6135/10570 [00:19<00:15, 290.49it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6165/10570 [00:19<00:15, 282.55it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6195/10570 [00:19<00:15, 286.21it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6224/10570 [00:19<00:15, 271.86it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6252/10570 [00:19<00:16, 257.56it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6280/10570 [00:19<00:16, 263.18it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6315/10570 [00:19<00:14, 285.55it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6350/10570 [00:19<00:13, 303.30it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6381/10570 [00:19<00:14, 296.28it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6413/10570 [00:20<00:13, 301.24it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6449/10570 [00:20<00:13, 315.93it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6485/10570 [00:20<00:12, 327.61it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6520/10570 [00:20<00:12, 333.69it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6554/10570 [00:20<00:11, 334.77it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6588/10570 [00:20<00:12, 330.80it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6622/10570 [00:20<00:12, 326.17it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6656/10570 [00:20<00:11, 329.26it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6689/10570 [00:20<00:12, 317.87it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6721/10570 [00:21<00:12, 317.52it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6754/10570 [00:21<00:11, 320.53it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6792/10570 [00:21<00:11, 335.25it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6826/10570 [00:21<00:11, 327.03it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6859/10570 [00:21<00:11, 318.20it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6891/10570 [00:21<00:11, 311.69it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6923/10570 [00:21<00:12, 303.79it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6957/10570 [00:21<00:11, 313.57it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6997/10570 [00:21<00:10, 337.39it/s]\u001b[A\n",
      " 67%|████████████████████████▌            | 7032/10570 [00:21<00:10, 338.27it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7066/10570 [00:22<00:10, 330.82it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7100/10570 [00:22<00:10, 329.57it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7134/10570 [00:22<00:10, 332.20it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7168/10570 [00:22<00:10, 328.05it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7201/10570 [00:22<00:10, 308.45it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7233/10570 [00:22<00:10, 304.76it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7268/10570 [00:22<00:10, 314.98it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7304/10570 [00:22<00:10, 325.56it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7337/10570 [00:22<00:10, 307.32it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7369/10570 [00:23<00:10, 300.64it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7401/10570 [00:23<00:10, 305.01it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7436/10570 [00:23<00:09, 316.53it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7468/10570 [00:23<00:09, 313.85it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7500/10570 [00:23<00:10, 295.13it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7530/10570 [00:23<00:11, 256.72it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7557/10570 [00:23<00:11, 259.77it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7584/10570 [00:23<00:11, 260.93it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7614/10570 [00:23<00:10, 270.51it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7647/10570 [00:24<00:10, 286.14it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7678/10570 [00:24<00:09, 292.32it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7712/10570 [00:24<00:09, 305.49it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7744/10570 [00:24<00:09, 302.52it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7775/10570 [00:24<00:10, 265.90it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7803/10570 [00:24<00:10, 266.87it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7831/10570 [00:24<00:11, 247.61it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7857/10570 [00:24<00:12, 213.93it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7881/10570 [00:25<00:12, 218.67it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7917/10570 [00:25<00:10, 253.97it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7955/10570 [00:25<00:09, 287.62it/s]\u001b[A\n",
      " 76%|███████████████████████████▉         | 7989/10570 [00:25<00:08, 300.92it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8025/10570 [00:25<00:08, 315.73it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8058/10570 [00:25<00:07, 314.83it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8098/10570 [00:25<00:07, 336.68it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8134/10570 [00:25<00:07, 340.15it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8169/10570 [00:25<00:07, 337.33it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8203/10570 [00:25<00:07, 330.44it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8237/10570 [00:26<00:07, 321.70it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8270/10570 [00:26<00:07, 318.38it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8302/10570 [00:26<00:07, 306.26it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8338/10570 [00:26<00:06, 319.68it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8373/10570 [00:26<00:06, 326.54it/s]\u001b[A\n",
      " 80%|█████████████████████████████▍       | 8406/10570 [00:26<00:06, 322.48it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8442/10570 [00:26<00:06, 333.12it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8478/10570 [00:26<00:06, 340.26it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8514/10570 [00:26<00:05, 345.52it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8549/10570 [00:26<00:05, 341.03it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8584/10570 [00:27<00:06, 322.22it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8619/10570 [00:27<00:05, 329.20it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8653/10570 [00:27<00:06, 314.44it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8685/10570 [00:27<00:06, 293.95it/s]\u001b[A\n",
      " 82%|██████████████████████████████▌      | 8715/10570 [00:27<00:06, 278.42it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8744/10570 [00:27<00:06, 273.87it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8772/10570 [00:27<00:06, 263.99it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8802/10570 [00:27<00:06, 271.54it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8835/10570 [00:28<00:06, 285.61it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8871/10570 [00:28<00:05, 305.07it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8902/10570 [00:28<00:05, 305.55it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8936/10570 [00:28<00:05, 313.84it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8968/10570 [00:28<00:05, 314.06it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9000/10570 [00:28<00:05, 296.25it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9030/10570 [00:28<00:05, 275.52it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9058/10570 [00:28<00:05, 267.62it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9086/10570 [00:28<00:05, 257.99it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9113/10570 [00:29<00:05, 244.92it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9138/10570 [00:29<00:06, 225.88it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9166/10570 [00:29<00:05, 238.85it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9197/10570 [00:29<00:05, 257.18it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9230/10570 [00:29<00:04, 275.95it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9265/10570 [00:29<00:04, 295.73it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9301/10570 [00:29<00:04, 313.30it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9334/10570 [00:29<00:03, 317.47it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9371/10570 [00:29<00:03, 330.28it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9405/10570 [00:29<00:03, 324.85it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9439/10570 [00:30<00:03, 327.29it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9475/10570 [00:30<00:03, 336.45it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9510/10570 [00:30<00:03, 339.62it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9545/10570 [00:30<00:03, 337.69it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9579/10570 [00:30<00:03, 328.49it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9613/10570 [00:30<00:02, 330.95it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9648/10570 [00:30<00:02, 334.71it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9682/10570 [00:30<00:02, 333.94it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9716/10570 [00:30<00:02, 323.44it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9749/10570 [00:31<00:02, 323.23it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▏  | 9782/10570 [00:31<00:02, 311.08it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9817/10570 [00:31<00:02, 321.19it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9850/10570 [00:31<00:02, 323.57it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▌  | 9885/10570 [00:31<00:02, 329.73it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9919/10570 [00:31<00:02, 320.25it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9955/10570 [00:31<00:01, 330.26it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▉  | 9989/10570 [00:31<00:01, 318.14it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10026/10570 [00:31<00:01, 332.49it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10061/10570 [00:31<00:01, 335.17it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10095/10570 [00:32<00:01, 321.53it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10130/10570 [00:32<00:01, 327.19it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10163/10570 [00:32<00:01, 316.17it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10195/10570 [00:32<00:01, 315.13it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10228/10570 [00:32<00:01, 315.72it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10260/10570 [00:32<00:01, 306.42it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10294/10570 [00:32<00:00, 315.68it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10330/10570 [00:32<00:00, 326.01it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10364/10570 [00:32<00:00, 327.67it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10397/10570 [00:33<00:00, 315.06it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10429/10570 [00:33<00:00, 316.40it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10464/10570 [00:33<00:00, 325.17it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10499/10570 [00:33<00:00, 331.03it/s]\u001b[A\n",
      "100%|███████████████████████████████████▊| 10533/10570 [00:33<00:00, 328.20it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:33<00:00, 314.98it/s]\u001b[A\n",
      "05/22/2022 17:32:03 - INFO - utils_qa - Saving predictions to ./squad_bert_base_uncased_advanced/eval_predictions.json.\n",
      "05/22/2022 17:32:03 - INFO - utils_qa - Saving nbest_preds to ./squad_bert_base_uncased_advanced/eval_nbest_predictions.json.\n",
      "05/22/2022 17:32:09 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████| 899/899 [04:50<00:00,  3.09it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match = 79.6783\n",
      "  eval_f1          =  87.552\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train_bert_squadv1_advanced.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

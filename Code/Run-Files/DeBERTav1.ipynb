{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d506e50c-b124-44f3-a2ba-5c1a52805e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Question-Answer' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Ahmedashorit/Question-Answer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7173be-70f9-4e2b-9d8f-57f8301aac69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 26.6 MB/s eta 0:00:01     |█████████▋                      | 250.3 MB 21.5 MB/s eta 0:00:28     |███████████▍                    | 295.6 MB 24.4 MB/s eta 0:00:22     |████████████                    | 312.8 MB 20.9 MB/s eta 0:00:25     |██████████████████              | 466.9 MB 21.2 MB/s eta 0:00:18\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0878a52-02df-4278-80f5-5d5cd3205f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 33.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 35.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 30.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 44.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 23.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 37.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.1.0 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.5.1 multiprocess-0.70.12.2 responses-0.18.0 tokenizers-0.12.1 transformers-4.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29a345f0-dc11-45b6-b3e1-ec5e1cc9c17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "214af421-73f5-4e3e-af0e-f8fd453fc920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/27/2022 13:49:44 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "04/27/2022 13:49:44 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_deberta_base/runs/Apr27_13-49-44_ngq8wuerrj,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_deberta_base,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=3,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_deberta_base,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/27/2022 13:49:44 - INFO - __main__ - Checkpoint detected, resuming training at ./squad_deberta_base/checkpoint-2000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n",
      "04/27/2022 13:49:44 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp401_umsg\n",
      "Downloading builder script: 5.27kB [00:00, 3.37MB/s]                            \n",
      "04/27/2022 13:49:44 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/27/2022 13:49:44 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/27/2022 13:49:44 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjz5u0cjh\n",
      "Downloading metadata: 2.36kB [00:00, 1.23MB/s]                                  \n",
      "04/27/2022 13:49:49 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/27/2022 13:49:49 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/27/2022 13:49:49 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\n",
      "04/27/2022 13:49:49 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "04/27/2022 13:49:49 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "04/27/2022 13:49:50 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpi63bphys\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  83%|████████████████▋   | 6.75M/8.12M [00:00<00:00, 67.5MB/s]\u001b[A\n",
      "Downloading data: 13.5MB [00:00, 66.9MB/s]                                      \u001b[A\n",
      "Downloading data: 20.2MB [00:00, 63.2MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 62.8MB/s]\u001b[A\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:00<00:00,  1.73it/s]04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp50kn2v1a\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 72.9MB/s]                                      \u001b[A\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:00<00:00,  2.79it/s]\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1547.43it/s]\n",
      "04/27/2022 13:49:50 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "04/27/2022 13:49:50 - INFO - datasets.builder - Generating train split\n",
      "04/27/2022 13:49:59 - INFO - datasets.builder - Generating validation split     \n",
      "04/27/2022 13:50:01 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 411.57it/s]\n",
      "[INFO|hub.py:583] 2022-04-27 13:50:01,078 >> https://huggingface.co/microsoft/deberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1se0pet2\n",
      "Downloading: 100%|██████████████████████████████| 474/474 [00:00<00:00, 319kB/s]\n",
      "[INFO|hub.py:587] 2022-04-27 13:50:01,153 >> storing https://huggingface.co/microsoft/deberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|hub.py:595] 2022-04-27 13:50:01,153 >> creating metadata file for /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 13:50:01,154 >> loading configuration file https://huggingface.co/microsoft/deberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 13:50:01,155 >> Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-27 13:50:01,225 >> https://huggingface.co/microsoft/deberta-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpu0x5ckgi\n",
      "Downloading: 100%|███████████████████████████| 52.0/52.0 [00:00<00:00, 29.8kB/s]\n",
      "[INFO|hub.py:587] 2022-04-27 13:50:01,296 >> storing https://huggingface.co/microsoft/deberta-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c2bc27a1c7529c177696ff76b1e74cba8667be14e202359f20f9114e407f43e2.a39abb1c6179fb264c2db685f9a056b7cb8d4bc48d729888d292a2280debf8e2\n",
      "[INFO|hub.py:595] 2022-04-27 13:50:01,297 >> creating metadata file for /root/.cache/huggingface/transformers/c2bc27a1c7529c177696ff76b1e74cba8667be14e202359f20f9114e407f43e2.a39abb1c6179fb264c2db685f9a056b7cb8d4bc48d729888d292a2280debf8e2\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 13:50:01,365 >> loading configuration file https://huggingface.co/microsoft/deberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 13:50:01,366 >> Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-27 13:50:01,508 >> https://huggingface.co/microsoft/deberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphz2oqbv7\n",
      "Downloading: 100%|███████████████████████████| 878k/878k [00:00<00:00, 8.79MB/s]\n",
      "[INFO|hub.py:587] 2022-04-27 13:50:01,715 >> storing https://huggingface.co/microsoft/deberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/ce0ac094af27cf80bbf403595a6d47f1fc632981bf1d4c5bf69968568cbea410.e8ad27cc324bb0dc448d4d95f63e48f72688fb318a4c4c3f623485621b0b515c\n",
      "[INFO|hub.py:595] 2022-04-27 13:50:01,715 >> creating metadata file for /root/.cache/huggingface/transformers/ce0ac094af27cf80bbf403595a6d47f1fc632981bf1d4c5bf69968568cbea410.e8ad27cc324bb0dc448d4d95f63e48f72688fb318a4c4c3f623485621b0b515c\n",
      "[INFO|hub.py:583] 2022-04-27 13:50:01,783 >> https://huggingface.co/microsoft/deberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp23l49ups\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 9.03MB/s]\n",
      "[INFO|hub.py:587] 2022-04-27 13:50:01,926 >> storing https://huggingface.co/microsoft/deberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/05056f257c8d2b63ad16fd26f847c9ab9ee34e33cdfad926e132be824b237869.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:595] 2022-04-27 13:50:01,927 >> creating metadata file for /root/.cache/huggingface/transformers/05056f257c8d2b63ad16fd26f847c9ab9ee34e33cdfad926e132be824b237869.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,221 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/ce0ac094af27cf80bbf403595a6d47f1fc632981bf1d4c5bf69968568cbea410.e8ad27cc324bb0dc448d4d95f63e48f72688fb318a4c4c3f623485621b0b515c\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,222 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/05056f257c8d2b63ad16fd26f847c9ab9ee34e33cdfad926e132be824b237869.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,222 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,222 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,222 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-27 13:50:02,222 >> loading file https://huggingface.co/microsoft/deberta-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c2bc27a1c7529c177696ff76b1e74cba8667be14e202359f20f9114e407f43e2.a39abb1c6179fb264c2db685f9a056b7cb8d4bc48d729888d292a2280debf8e2\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 13:50:02,290 >> loading configuration file https://huggingface.co/microsoft/deberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 13:50:02,291 >> Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:654] 2022-04-27 13:50:02,462 >> loading configuration file https://huggingface.co/microsoft/deberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/e313266bff73867debdfa78c78a9a4966d5e78281ac4ed7048c178b16a37eba7.fb501413b9cef9cef6babdc543bb4153cbec58d52bce077647efba3e3f14ccf3\n",
      "[INFO|configuration_utils.py:690] 2022-04-27 13:50:02,463 >> Model config DebertaConfig {\n",
      "  \"_name_or_path\": \"microsoft/deberta-base\",\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 768,\n",
      "  \"pos_att_type\": [\n",
      "    \"c2p\",\n",
      "    \"p2c\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"relative_attention\": true,\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-27 13:50:02,688 >> https://huggingface.co/microsoft/deberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp2hdbiw94\n",
      "Downloading: 100%|███████████████████████████| 533M/533M [00:23<00:00, 23.3MB/s]\n",
      "[INFO|hub.py:587] 2022-04-27 13:50:26,701 >> storing https://huggingface.co/microsoft/deberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/dde0725208c11536042f6f416c538792d44a2d57d1ae399bbd1bc5867e02c465.0a3ec262cb3d4f634c72ce55f2766bb88771e6499b2512830e2e63bf19dbf97a\n",
      "[INFO|hub.py:595] 2022-04-27 13:50:26,701 >> creating metadata file for /root/.cache/huggingface/transformers/dde0725208c11536042f6f416c538792d44a2d57d1ae399bbd1bc5867e02c465.0a3ec262cb3d4f634c72ce55f2766bb88771e6499b2512830e2e63bf19dbf97a\n",
      "[INFO|modeling_utils.py:1772] 2022-04-27 13:50:26,701 >> loading weights file https://huggingface.co/microsoft/deberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/dde0725208c11536042f6f416c538792d44a2d57d1ae399bbd1bc5867e02c465.0a3ec262cb3d4f634c72ce55f2766bb88771e6499b2512830e2e63bf19dbf97a\n",
      "[WARNING|modeling_utils.py:2048] 2022-04-27 13:50:28,779 >> Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForQuestionAnswering: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'deberta.embeddings.position_embeddings.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias']\n",
      "- This IS expected if you are initializing DebertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2059] 2022-04-27 13:50:28,779 >> Some weights of DebertaForQuestionAnswering were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]04/27/2022 13:50:29 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-cde2f3238cb947cf.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:55<00:00,  1.59ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]04/27/2022 13:51:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-d337d4a23e3e37e8.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:42<00:00,  3.88s/ba]\n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpa3ez1qz0\n",
      "Downloading builder script: 4.50kB [00:00, 2.48MB/s]                            \n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp6f7t3ccz\n",
      "Downloading extra modules: 3.31kB [00:00, 1.83MB/s]                             \n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "04/27/2022 13:52:07 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "[INFO|trainer.py:1172] 2022-04-27 13:52:11,503 >> Loading model from ./squad_deberta_base/checkpoint-2000).\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1290] 2022-04-27 13:52:36,715 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-27 13:52:36,715 >>   Num examples = 88550\n",
      "[INFO|trainer.py:1292] 2022-04-27 13:52:36,715 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1293] 2022-04-27 13:52:36,715 >>   Instantaneous batch size per device = 3\n",
      "[INFO|trainer.py:1294] 2022-04-27 13:52:36,715 >>   Total train batch size (w. parallel, distributed & accumulation) = 3\n",
      "[INFO|trainer.py:1295] 2022-04-27 13:52:36,715 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-27 13:52:36,716 >>   Total optimization steps = 29517\n",
      "{'loss': 0.9998, 'learning_rate': 2.949181827421486e-05, 'epoch': 0.02}         \n",
      "  2%|▋                                    | 500/29517 [03:07<3:00:57,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 13:55:44,341 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 13:55:44,405 >> Configuration saved in ./squad_deberta_base/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 13:55:45,508 >> Model weights saved in ./squad_deberta_base/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 13:55:45,516 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 13:55:45,521 >> Special tokens file saved in ./squad_deberta_base/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 0.8849, 'learning_rate': 2.898363654842972e-05, 'epoch': 0.03}         \n",
      "  3%|█▏                                  | 1000/29517 [06:19<2:58:31,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 13:58:56,427 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-1000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 13:58:56,430 >> Configuration saved in ./squad_deberta_base/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 13:58:57,499 >> Model weights saved in ./squad_deberta_base/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 13:58:57,505 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 13:58:57,508 >> Special tokens file saved in ./squad_deberta_base/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 0.8089, 'learning_rate': 2.8475454822644578e-05, 'epoch': 0.05}        \n",
      "  5%|█▊                                  | 1500/29517 [09:31<2:54:24,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:02:08,551 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-1500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:02:08,555 >> Configuration saved in ./squad_deberta_base/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:02:09,603 >> Model weights saved in ./squad_deberta_base/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:02:09,607 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:02:09,610 >> Special tokens file saved in ./squad_deberta_base/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 0.807, 'learning_rate': 2.796727309685944e-05, 'epoch': 0.07}          \n",
      "  7%|██▍                                 | 2000/29517 [12:43<2:51:22,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:05:20,572 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-2000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:05:20,578 >> Configuration saved in ./squad_deberta_base/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:05:21,678 >> Model weights saved in ./squad_deberta_base/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:05:21,682 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:05:21,684 >> Special tokens file saved in ./squad_deberta_base/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2408, 'learning_rate': 2.7459091371074297e-05, 'epoch': 0.08}        \n",
      "  8%|███                                 | 2500/29517 [15:55<2:48:20,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:08:32,281 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-2500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:08:32,283 >> Configuration saved in ./squad_deberta_base/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:08:33,329 >> Model weights saved in ./squad_deberta_base/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:08:33,331 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:08:33,332 >> Special tokens file saved in ./squad_deberta_base/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1229, 'learning_rate': 2.695090964528916e-05, 'epoch': 0.1}          \n",
      " 10%|███▋                                | 3000/29517 [19:07<2:45:23,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:11:44,114 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-3000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:11:44,116 >> Configuration saved in ./squad_deberta_base/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:11:45,129 >> Model weights saved in ./squad_deberta_base/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:11:45,132 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:11:45,135 >> Special tokens file saved in ./squad_deberta_base/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.2185, 'learning_rate': 2.6442727919504016e-05, 'epoch': 0.12}        \n",
      " 12%|████▎                               | 3500/29517 [22:19<2:42:25,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:14:56,396 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-3500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:14:56,399 >> Configuration saved in ./squad_deberta_base/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:14:57,424 >> Model weights saved in ./squad_deberta_base/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:14:57,426 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:14:57,428 >> Special tokens file saved in ./squad_deberta_base/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.2171, 'learning_rate': 2.5934546193718874e-05, 'epoch': 0.14}        \n",
      " 14%|████▉                               | 4000/29517 [25:31<2:39:05,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:18:08,069 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-4000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:18:08,070 >> Configuration saved in ./squad_deberta_base/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:18:09,094 >> Model weights saved in ./squad_deberta_base/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:18:09,096 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:18:09,096 >> Special tokens file saved in ./squad_deberta_base/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.1553, 'learning_rate': 2.5426364467933735e-05, 'epoch': 0.15}        \n",
      " 15%|█████▍                              | 4500/29517 [28:43<2:35:57,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:21:20,071 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-4500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:21:20,073 >> Configuration saved in ./squad_deberta_base/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:21:36,937 >> Model weights saved in ./squad_deberta_base/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:21:36,943 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:21:36,951 >> Special tokens file saved in ./squad_deberta_base/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.1801, 'learning_rate': 2.4918182742148593e-05, 'epoch': 0.17}        \n",
      " 17%|██████                              | 5000/29517 [32:10<2:33:20,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 14:24:47,277 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-5000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:24:47,280 >> Configuration saved in ./squad_deberta_base/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:24:48,331 >> Model weights saved in ./squad_deberta_base/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:24:48,333 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:24:48,334 >> Special tokens file saved in ./squad_deberta_base/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.1268, 'learning_rate': 2.4410001016363455e-05, 'epoch': 0.19}        \n",
      " 19%|██████▋                             | 5500/29517 [35:22<2:29:33,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:27:59,046 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-5500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:27:59,048 >> Configuration saved in ./squad_deberta_base/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:28:00,063 >> Model weights saved in ./squad_deberta_base/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:28:00,065 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:28:00,066 >> Special tokens file saved in ./squad_deberta_base/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 1.0718, 'learning_rate': 2.3901819290578312e-05, 'epoch': 0.2}         \n",
      " 20%|███████▎                            | 6000/29517 [38:34<2:26:44,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:31:10,978 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-6000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:31:10,980 >> Configuration saved in ./squad_deberta_base/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:31:12,055 >> Model weights saved in ./squad_deberta_base/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:31:12,056 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:31:12,057 >> Special tokens file saved in ./squad_deberta_base/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0454, 'learning_rate': 2.3393637564793174e-05, 'epoch': 0.22}        \n",
      " 22%|███████▉                            | 6500/29517 [41:45<2:23:03,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:34:22,662 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-6500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:34:22,663 >> Configuration saved in ./squad_deberta_base/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:34:23,766 >> Model weights saved in ./squad_deberta_base/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:34:23,768 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:34:23,769 >> Special tokens file saved in ./squad_deberta_base/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0784, 'learning_rate': 2.288545583900803e-05, 'epoch': 0.24}         \n",
      " 24%|████████▌                           | 7000/29517 [44:57<2:20:13,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:37:34,452 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-7000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:37:34,455 >> Configuration saved in ./squad_deberta_base/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:37:35,530 >> Model weights saved in ./squad_deberta_base/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:37:35,535 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:37:35,541 >> Special tokens file saved in ./squad_deberta_base/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 1.0283, 'learning_rate': 2.2377274113222886e-05, 'epoch': 0.25}        \n",
      " 25%|█████████▏                          | 7500/29517 [48:09<2:17:42,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 14:40:46,123 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-7500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:40:46,125 >> Configuration saved in ./squad_deberta_base/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:40:47,233 >> Model weights saved in ./squad_deberta_base/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:40:47,275 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:40:47,281 >> Special tokens file saved in ./squad_deberta_base/checkpoint-7500/special_tokens_map.json\n",
      "{'loss': 1.1105, 'learning_rate': 2.1869092387437747e-05, 'epoch': 0.27}        \n",
      " 27%|█████████▊                          | 8000/29517 [51:21<2:13:44,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:43:58,135 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-8000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:43:58,137 >> Configuration saved in ./squad_deberta_base/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:43:59,198 >> Model weights saved in ./squad_deberta_base/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:43:59,201 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:43:59,202 >> Special tokens file saved in ./squad_deberta_base/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 1.0758, 'learning_rate': 2.1360910661652605e-05, 'epoch': 0.29}        \n",
      " 29%|██████████▎                         | 8500/29517 [54:33<2:10:32,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 14:47:09,872 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-8500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:47:09,875 >> Configuration saved in ./squad_deberta_base/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:47:10,928 >> Model weights saved in ./squad_deberta_base/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:47:10,930 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:47:10,932 >> Special tokens file saved in ./squad_deberta_base/checkpoint-8500/special_tokens_map.json\n",
      "{'loss': 1.0897, 'learning_rate': 2.0852728935867467e-05, 'epoch': 0.3}         \n",
      " 30%|██████████▉                         | 9000/29517 [57:44<2:08:23,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 14:50:21,207 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-9000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:50:21,209 >> Configuration saved in ./squad_deberta_base/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:50:22,342 >> Model weights saved in ./squad_deberta_base/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:50:22,345 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:50:22,347 >> Special tokens file saved in ./squad_deberta_base/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 1.0837, 'learning_rate': 2.0344547210082325e-05, 'epoch': 0.32}        \n",
      " 32%|██████████▉                       | 9500/29517 [1:00:56<2:05:04,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:53:32,754 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-9500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:53:32,757 >> Configuration saved in ./squad_deberta_base/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:53:33,888 >> Model weights saved in ./squad_deberta_base/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:53:33,895 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:53:33,900 >> Special tokens file saved in ./squad_deberta_base/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 1.1158, 'learning_rate': 1.9836365484297186e-05, 'epoch': 0.34}        \n",
      " 34%|███████████▏                     | 10000/29517 [1:04:07<2:02:10,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 14:56:44,585 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-10000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:56:44,586 >> Configuration saved in ./squad_deberta_base/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:56:45,619 >> Model weights saved in ./squad_deberta_base/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:56:45,628 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:56:45,635 >> Special tokens file saved in ./squad_deberta_base/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.9998, 'learning_rate': 1.9328183758512044e-05, 'epoch': 0.36}        \n",
      " 36%|███████████▋                     | 10500/29517 [1:07:19<1:58:36,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 14:59:56,336 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-10500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 14:59:56,340 >> Configuration saved in ./squad_deberta_base/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 14:59:57,426 >> Model weights saved in ./squad_deberta_base/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 14:59:57,428 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 14:59:57,430 >> Special tokens file saved in ./squad_deberta_base/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 1.0619, 'learning_rate': 1.88200020327269e-05, 'epoch': 0.37}          \n",
      " 37%|████████████▎                    | 11000/29517 [1:10:31<1:54:59,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 15:03:07,931 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-11000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:03:07,952 >> Configuration saved in ./squad_deberta_base/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:04:03,459 >> Model weights saved in ./squad_deberta_base/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:04:03,462 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:04:03,498 >> Special tokens file saved in ./squad_deberta_base/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 1.1119, 'learning_rate': 1.8311820306941763e-05, 'epoch': 0.39}        \n",
      " 39%|████████████▊                    | 11500/29517 [1:16:09<1:52:57,  2.66it/s][INFO|trainer.py:2166] 2022-04-27 15:08:45,907 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-11500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:08:45,914 >> Configuration saved in ./squad_deberta_base/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:09:31,129 >> Model weights saved in ./squad_deberta_base/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:09:31,137 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:09:31,152 >> Special tokens file saved in ./squad_deberta_base/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 1.0627, 'learning_rate': 1.780363858115662e-05, 'epoch': 0.41}         \n",
      " 41%|█████████████▍                   | 12000/29517 [1:21:31<1:48:46,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 15:14:08,076 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-12000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:14:08,083 >> Configuration saved in ./squad_deberta_base/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:15:25,733 >> Model weights saved in ./squad_deberta_base/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:15:25,746 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:15:25,752 >> Special tokens file saved in ./squad_deberta_base/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 1.0523, 'learning_rate': 1.7295456855371482e-05, 'epoch': 0.42}        \n",
      " 42%|█████████████▉                   | 12500/29517 [1:28:22<1:46:18,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 15:20:59,469 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-12500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:20:59,536 >> Configuration saved in ./squad_deberta_base/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:22:16,632 >> Model weights saved in ./squad_deberta_base/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:22:16,734 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:22:16,740 >> Special tokens file saved in ./squad_deberta_base/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 1.0195, 'learning_rate': 1.678727512958634e-05, 'epoch': 0.44}         \n",
      " 44%|██████████████▌                  | 13000/29517 [1:35:37<1:42:49,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 15:28:14,589 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-13000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:28:14,596 >> Configuration saved in ./squad_deberta_base/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:29:26,206 >> Model weights saved in ./squad_deberta_base/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:29:26,211 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:29:26,218 >> Special tokens file saved in ./squad_deberta_base/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 1.0422, 'learning_rate': 1.62790934038012e-05, 'epoch': 0.46}          \n",
      " 46%|███████████████                  | 13500/29517 [1:41:36<1:39:55,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 15:34:13,681 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-13500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:34:13,688 >> Configuration saved in ./squad_deberta_base/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:35:02,995 >> Model weights saved in ./squad_deberta_base/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:35:03,000 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:35:03,006 >> Special tokens file saved in ./squad_deberta_base/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.9493, 'learning_rate': 1.577091167801606e-05, 'epoch': 0.47}         \n",
      " 47%|███████████████▋                 | 14000/29517 [1:47:08<1:36:49,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 15:39:44,954 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-14000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:39:44,962 >> Configuration saved in ./squad_deberta_base/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:40:34,185 >> Model weights saved in ./squad_deberta_base/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:40:34,192 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:40:34,198 >> Special tokens file saved in ./squad_deberta_base/checkpoint-14000/special_tokens_map.json\n",
      "{'loss': 0.9303, 'learning_rate': 1.5262729952230917e-05, 'epoch': 0.49}        \n",
      " 49%|████████████████▏                | 14500/29517 [1:53:03<1:33:48,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 15:45:40,025 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-14500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:45:40,032 >> Configuration saved in ./squad_deberta_base/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:47:24,167 >> Model weights saved in ./squad_deberta_base/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:47:24,171 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:47:24,176 >> Special tokens file saved in ./squad_deberta_base/checkpoint-14500/special_tokens_map.json\n",
      "{'loss': 0.9967, 'learning_rate': 1.4754548226445778e-05, 'epoch': 0.51}        \n",
      " 51%|████████████████▊                | 15000/29517 [2:00:59<1:30:29,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 15:53:36,681 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-15000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:53:36,688 >> Configuration saved in ./squad_deberta_base/checkpoint-15000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 15:54:38,857 >> Model weights saved in ./squad_deberta_base/checkpoint-15000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 15:54:38,862 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-15000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 15:54:38,867 >> Special tokens file saved in ./squad_deberta_base/checkpoint-15000/special_tokens_map.json\n",
      "{'loss': 1.0571, 'learning_rate': 1.4246366500660636e-05, 'epoch': 0.53}        \n",
      " 53%|█████████████████▎               | 15500/29517 [2:07:05<1:27:17,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 15:59:42,073 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-15500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 15:59:42,081 >> Configuration saved in ./squad_deberta_base/checkpoint-15500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:00:43,085 >> Model weights saved in ./squad_deberta_base/checkpoint-15500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:00:43,089 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-15500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:00:43,095 >> Special tokens file saved in ./squad_deberta_base/checkpoint-15500/special_tokens_map.json\n",
      "{'loss': 0.9985, 'learning_rate': 1.3738184774875496e-05, 'epoch': 0.54}        \n",
      " 54%|█████████████████▉               | 16000/29517 [2:13:55<1:24:24,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:06:31,845 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-16000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:06:31,860 >> Configuration saved in ./squad_deberta_base/checkpoint-16000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:07:28,763 >> Model weights saved in ./squad_deberta_base/checkpoint-16000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:07:28,769 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-16000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:07:28,772 >> Special tokens file saved in ./squad_deberta_base/checkpoint-16000/special_tokens_map.json\n",
      "{'loss': 0.9984, 'learning_rate': 1.3230003049090355e-05, 'epoch': 0.56}        \n",
      " 56%|██████████████████▍              | 16500/29517 [2:20:01<1:20:59,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 16:12:37,830 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-16500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:12:37,836 >> Configuration saved in ./squad_deberta_base/checkpoint-16500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:13:46,013 >> Model weights saved in ./squad_deberta_base/checkpoint-16500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:13:46,018 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-16500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:13:46,026 >> Special tokens file saved in ./squad_deberta_base/checkpoint-16500/special_tokens_map.json\n",
      "{'loss': 0.9527, 'learning_rate': 1.2721821323305215e-05, 'epoch': 0.58}        \n",
      " 58%|███████████████████              | 17000/29517 [2:26:47<1:18:05,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:19:23,941 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-17000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:19:23,949 >> Configuration saved in ./squad_deberta_base/checkpoint-17000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:20:35,147 >> Model weights saved in ./squad_deberta_base/checkpoint-17000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:20:35,151 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-17000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:20:35,156 >> Special tokens file saved in ./squad_deberta_base/checkpoint-17000/special_tokens_map.json\n",
      "{'loss': 0.9401, 'learning_rate': 1.2213639597520075e-05, 'epoch': 0.59}        \n",
      " 59%|███████████████████▌             | 17500/29517 [2:33:00<1:14:53,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:25:37,081 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-17500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:25:37,097 >> Configuration saved in ./squad_deberta_base/checkpoint-17500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:26:38,885 >> Model weights saved in ./squad_deberta_base/checkpoint-17500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:26:38,892 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-17500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:26:38,895 >> Special tokens file saved in ./squad_deberta_base/checkpoint-17500/special_tokens_map.json\n",
      "{'loss': 0.9488, 'learning_rate': 1.1705457871734933e-05, 'epoch': 0.61}        \n",
      " 61%|████████████████████             | 18000/29517 [2:39:14<1:11:48,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:31:51,601 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-18000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:31:51,627 >> Configuration saved in ./squad_deberta_base/checkpoint-18000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:32:46,649 >> Model weights saved in ./squad_deberta_base/checkpoint-18000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:32:46,654 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-18000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:32:46,666 >> Special tokens file saved in ./squad_deberta_base/checkpoint-18000/special_tokens_map.json\n",
      "{'loss': 0.946, 'learning_rate': 1.1197276145949792e-05, 'epoch': 0.63}         \n",
      " 63%|████████████████████▋            | 18500/29517 [2:45:11<1:08:47,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:37:48,625 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-18500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:37:48,630 >> Configuration saved in ./squad_deberta_base/checkpoint-18500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:38:38,027 >> Model weights saved in ./squad_deberta_base/checkpoint-18500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:38:38,030 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-18500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:38:38,035 >> Special tokens file saved in ./squad_deberta_base/checkpoint-18500/special_tokens_map.json\n",
      "{'loss': 0.9347, 'learning_rate': 1.068909442016465e-05, 'epoch': 0.64}         \n",
      " 64%|█████████████████████▏           | 19000/29517 [2:50:52<1:05:41,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 16:43:29,135 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-19000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:43:29,142 >> Configuration saved in ./squad_deberta_base/checkpoint-19000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:44:32,487 >> Model weights saved in ./squad_deberta_base/checkpoint-19000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:44:32,491 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-19000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:44:32,495 >> Special tokens file saved in ./squad_deberta_base/checkpoint-19000/special_tokens_map.json\n",
      "{'loss': 0.9641, 'learning_rate': 1.018091269437951e-05, 'epoch': 0.66}         \n",
      " 66%|█████████████████████▊           | 19500/29517 [2:57:02<1:02:19,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 16:49:39,688 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-19500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:49:39,696 >> Configuration saved in ./squad_deberta_base/checkpoint-19500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:51:02,197 >> Model weights saved in ./squad_deberta_base/checkpoint-19500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:51:02,203 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-19500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:51:02,207 >> Special tokens file saved in ./squad_deberta_base/checkpoint-19500/special_tokens_map.json\n",
      "{'loss': 0.9418, 'learning_rate': 9.67273096859437e-06, 'epoch': 0.68}          \n",
      " 68%|███████████████████████▋           | 20000/29517 [3:03:45<59:10,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 16:56:21,904 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-20000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 16:56:21,950 >> Configuration saved in ./squad_deberta_base/checkpoint-20000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 16:57:22,370 >> Model weights saved in ./squad_deberta_base/checkpoint-20000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 16:57:22,376 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-20000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 16:57:22,392 >> Special tokens file saved in ./squad_deberta_base/checkpoint-20000/special_tokens_map.json\n",
      "{'loss': 0.8575, 'learning_rate': 9.164549242809229e-06, 'epoch': 0.69}         \n",
      " 69%|████████████████████████▎          | 20500/29517 [3:09:46<56:09,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:02:22,872 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-20500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:02:22,879 >> Configuration saved in ./squad_deberta_base/checkpoint-20500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:03:13,400 >> Model weights saved in ./squad_deberta_base/checkpoint-20500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:03:13,404 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-20500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:03:13,409 >> Special tokens file saved in ./squad_deberta_base/checkpoint-20500/special_tokens_map.json\n",
      "{'loss': 0.8513, 'learning_rate': 8.656367517024088e-06, 'epoch': 0.71}         \n",
      " 71%|████████████████████████▉          | 21000/29517 [3:16:21<52:55,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:08:57,888 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-21000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:08:57,897 >> Configuration saved in ./squad_deberta_base/checkpoint-21000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:09:39,056 >> Model weights saved in ./squad_deberta_base/checkpoint-21000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:09:39,060 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-21000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:09:39,063 >> Special tokens file saved in ./squad_deberta_base/checkpoint-21000/special_tokens_map.json\n",
      "{'loss': 0.944, 'learning_rate': 8.148185791238948e-06, 'epoch': 0.73}          \n",
      " 73%|█████████████████████████▍         | 21500/29517 [3:21:28<50:00,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 17:14:05,413 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-21500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:14:05,421 >> Configuration saved in ./squad_deberta_base/checkpoint-21500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:14:45,354 >> Model weights saved in ./squad_deberta_base/checkpoint-21500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:14:45,358 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-21500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:14:45,361 >> Special tokens file saved in ./squad_deberta_base/checkpoint-21500/special_tokens_map.json\n",
      "{'loss': 0.9672, 'learning_rate': 7.640004065453806e-06, 'epoch': 0.75}         \n",
      " 75%|██████████████████████████         | 22000/29517 [3:26:37<46:47,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:19:14,158 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-22000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:19:14,164 >> Configuration saved in ./squad_deberta_base/checkpoint-22000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:19:53,331 >> Model weights saved in ./squad_deberta_base/checkpoint-22000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:19:53,343 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-22000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:19:53,347 >> Special tokens file saved in ./squad_deberta_base/checkpoint-22000/special_tokens_map.json\n",
      "{'loss': 0.8922, 'learning_rate': 7.131822339668666e-06, 'epoch': 0.76}         \n",
      " 76%|██████████████████████████▋        | 22500/29517 [3:31:44<43:42,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:24:21,488 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-22500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:24:21,494 >> Configuration saved in ./squad_deberta_base/checkpoint-22500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:25:01,967 >> Model weights saved in ./squad_deberta_base/checkpoint-22500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:25:01,974 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-22500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:25:01,979 >> Special tokens file saved in ./squad_deberta_base/checkpoint-22500/special_tokens_map.json\n",
      "{'loss': 0.9085, 'learning_rate': 6.623640613883524e-06, 'epoch': 0.78}         \n",
      " 78%|███████████████████████████▎       | 23000/29517 [3:36:52<40:30,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:29:29,438 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-23000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:29:29,445 >> Configuration saved in ./squad_deberta_base/checkpoint-23000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:30:10,102 >> Model weights saved in ./squad_deberta_base/checkpoint-23000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:30:10,107 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-23000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:30:10,114 >> Special tokens file saved in ./squad_deberta_base/checkpoint-23000/special_tokens_map.json\n",
      "{'loss': 0.9506, 'learning_rate': 6.115458888098384e-06, 'epoch': 0.8}          \n",
      " 80%|███████████████████████████▊       | 23500/29517 [3:42:01<37:22,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:34:38,263 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-23500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:34:38,268 >> Configuration saved in ./squad_deberta_base/checkpoint-23500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:35:18,209 >> Model weights saved in ./squad_deberta_base/checkpoint-23500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:35:18,214 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-23500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:35:18,220 >> Special tokens file saved in ./squad_deberta_base/checkpoint-23500/special_tokens_map.json\n",
      "{'loss': 0.8907, 'learning_rate': 5.607277162313243e-06, 'epoch': 0.81}         \n",
      " 81%|████████████████████████████▍      | 24000/29517 [3:47:09<34:14,  2.69it/s][INFO|trainer.py:2166] 2022-04-27 17:39:45,769 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-24000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:39:45,777 >> Configuration saved in ./squad_deberta_base/checkpoint-24000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:40:26,335 >> Model weights saved in ./squad_deberta_base/checkpoint-24000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:40:26,344 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-24000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:40:26,348 >> Special tokens file saved in ./squad_deberta_base/checkpoint-24000/special_tokens_map.json\n",
      "{'loss': 0.8823, 'learning_rate': 5.099095436528102e-06, 'epoch': 0.83}         \n",
      " 83%|█████████████████████████████      | 24500/29517 [3:52:17<31:15,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 17:44:53,973 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-24500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:44:53,979 >> Configuration saved in ./squad_deberta_base/checkpoint-24500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:45:33,939 >> Model weights saved in ./squad_deberta_base/checkpoint-24500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:45:33,948 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-24500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:45:33,961 >> Special tokens file saved in ./squad_deberta_base/checkpoint-24500/special_tokens_map.json\n",
      "{'loss': 0.908, 'learning_rate': 4.590913710742962e-06, 'epoch': 0.85}          \n",
      " 85%|█████████████████████████████▋     | 25000/29517 [3:57:26<28:01,  2.69it/s][INFO|trainer.py:2166] 2022-04-27 17:50:03,255 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-25000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:50:03,262 >> Configuration saved in ./squad_deberta_base/checkpoint-25000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:50:43,354 >> Model weights saved in ./squad_deberta_base/checkpoint-25000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:50:43,359 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-25000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:50:43,364 >> Special tokens file saved in ./squad_deberta_base/checkpoint-25000/special_tokens_map.json\n",
      "{'loss': 0.8281, 'learning_rate': 4.082731984957821e-06, 'epoch': 0.86}         \n",
      " 86%|██████████████████████████████▏    | 25500/29517 [4:02:32<25:00,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 17:55:09,169 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-25500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 17:55:09,176 >> Configuration saved in ./squad_deberta_base/checkpoint-25500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 17:55:49,583 >> Model weights saved in ./squad_deberta_base/checkpoint-25500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 17:55:49,589 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-25500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 17:55:49,592 >> Special tokens file saved in ./squad_deberta_base/checkpoint-25500/special_tokens_map.json\n",
      "{'loss': 0.8898, 'learning_rate': 3.57455025917268e-06, 'epoch': 0.88}          \n",
      " 88%|██████████████████████████████▊    | 26000/29517 [4:07:42<21:51,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 18:00:19,091 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-26000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:00:19,101 >> Configuration saved in ./squad_deberta_base/checkpoint-26000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:01:01,753 >> Model weights saved in ./squad_deberta_base/checkpoint-26000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:01:01,757 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-26000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:01:01,762 >> Special tokens file saved in ./squad_deberta_base/checkpoint-26000/special_tokens_map.json\n",
      "{'loss': 0.8925, 'learning_rate': 3.0663685333875397e-06, 'epoch': 0.9}         \n",
      " 90%|███████████████████████████████▍   | 26500/29517 [4:12:56<18:47,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 18:05:33,608 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-26500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:05:33,616 >> Configuration saved in ./squad_deberta_base/checkpoint-26500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:06:14,520 >> Model weights saved in ./squad_deberta_base/checkpoint-26500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:06:14,524 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-26500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:06:14,527 >> Special tokens file saved in ./squad_deberta_base/checkpoint-26500/special_tokens_map.json\n",
      "{'loss': 0.8577, 'learning_rate': 2.558186807602399e-06, 'epoch': 0.91}         \n",
      " 91%|████████████████████████████████   | 27000/29517 [4:18:07<15:42,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 18:10:44,516 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-27000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:10:44,523 >> Configuration saved in ./squad_deberta_base/checkpoint-27000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:11:29,039 >> Model weights saved in ./squad_deberta_base/checkpoint-27000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:11:29,044 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-27000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:11:29,047 >> Special tokens file saved in ./squad_deberta_base/checkpoint-27000/special_tokens_map.json\n",
      "{'loss': 0.9235, 'learning_rate': 2.0500050818172576e-06, 'epoch': 0.93}        \n",
      " 93%|████████████████████████████████▌  | 27500/29517 [4:23:23<12:31,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 18:15:59,895 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-27500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:15:59,902 >> Configuration saved in ./squad_deberta_base/checkpoint-27500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:16:41,110 >> Model weights saved in ./squad_deberta_base/checkpoint-27500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:16:41,116 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-27500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:16:41,121 >> Special tokens file saved in ./squad_deberta_base/checkpoint-27500/special_tokens_map.json\n",
      "{'loss': 0.8244, 'learning_rate': 1.5418233560321172e-06, 'epoch': 0.95}        \n",
      " 95%|█████████████████████████████████▏ | 28000/29517 [4:28:33<09:25,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 18:21:10,596 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-28000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:21:10,603 >> Configuration saved in ./squad_deberta_base/checkpoint-28000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:21:52,097 >> Model weights saved in ./squad_deberta_base/checkpoint-28000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:21:52,101 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-28000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:21:52,107 >> Special tokens file saved in ./squad_deberta_base/checkpoint-28000/special_tokens_map.json\n",
      "{'loss': 0.8677, 'learning_rate': 1.0336416302469764e-06, 'epoch': 0.97}        \n",
      " 97%|█████████████████████████████████▊ | 28500/29517 [4:33:45<06:21,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 18:26:22,404 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-28500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:26:22,410 >> Configuration saved in ./squad_deberta_base/checkpoint-28500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:27:02,384 >> Model weights saved in ./squad_deberta_base/checkpoint-28500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:27:02,390 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-28500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:27:02,396 >> Special tokens file saved in ./squad_deberta_base/checkpoint-28500/special_tokens_map.json\n",
      "{'loss': 0.8999, 'learning_rate': 5.254599044618356e-07, 'epoch': 0.98}         \n",
      " 98%|██████████████████████████████████▍| 29000/29517 [4:38:55<03:13,  2.68it/s][INFO|trainer.py:2166] 2022-04-27 18:31:31,857 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-29000\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:31:31,864 >> Configuration saved in ./squad_deberta_base/checkpoint-29000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:32:12,142 >> Model weights saved in ./squad_deberta_base/checkpoint-29000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:32:12,149 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-29000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:32:12,153 >> Special tokens file saved in ./squad_deberta_base/checkpoint-29000/special_tokens_map.json\n",
      "{'loss': 0.8988, 'learning_rate': 1.727817867669479e-08, 'epoch': 1.0}          \n",
      "100%|██████████████████████████████████▉| 29500/29517 [4:44:06<00:06,  2.67it/s][INFO|trainer.py:2166] 2022-04-27 18:36:43,259 >> Saving model checkpoint to ./squad_deberta_base/checkpoint-29500\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:36:43,265 >> Configuration saved in ./squad_deberta_base/checkpoint-29500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:37:24,012 >> Model weights saved in ./squad_deberta_base/checkpoint-29500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:37:24,018 >> tokenizer config file saved in ./squad_deberta_base/checkpoint-29500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:37:24,024 >> Special tokens file saved in ./squad_deberta_base/checkpoint-29500/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 29517/29517 [4:46:15<00:00,  2.14it/s][INFO|trainer.py:1530] 2022-04-27 18:38:52,281 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 17175.5739, 'train_samples_per_second': 5.156, 'train_steps_per_second': 1.719, 'train_loss': 0.9880257828568332, 'epoch': 1.0}\n",
      "100%|███████████████████████████████████| 29517/29517 [4:46:15<00:00,  1.72it/s]\n",
      "[INFO|trainer.py:2166] 2022-04-27 18:38:52,308 >> Saving model checkpoint to ./squad_deberta_base\n",
      "[INFO|configuration_utils.py:441] 2022-04-27 18:38:52,316 >> Configuration saved in ./squad_deberta_base/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-27 18:39:32,672 >> Model weights saved in ./squad_deberta_base/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-27 18:39:32,678 >> tokenizer config file saved in ./squad_deberta_base/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-27 18:39:32,684 >> Special tokens file saved in ./squad_deberta_base/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =      0.988\n",
      "  train_runtime            = 4:46:15.57\n",
      "  train_samples            =      88550\n",
      "  train_samples_per_second =      5.156\n",
      "  train_steps_per_second   =      1.719\n",
      "04/27/2022 18:39:33 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:566] 2022-04-27 18:39:33,134 >> The following columns in the evaluation set  don't have a corresponding argument in `DebertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `DebertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2416] 2022-04-27 18:39:33,141 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2022-04-27 18:39:33,141 >>   Num examples = 10788\n",
      "[INFO|trainer.py:2421] 2022-04-27 18:39:33,141 >>   Batch size = 8\n",
      "100%|███████████████████████████████████████| 1349/1349 [05:39<00:00,  4.61it/s]04/27/2022 18:45:29 - INFO - utils_qa - Post-processing 10570 example predictions split into 10788 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                       | 29/10570 [00:00<00:38, 277.33it/s]\u001b[A\n",
      "  1%|▏                                      | 63/10570 [00:00<00:33, 310.10it/s]\u001b[A\n",
      "  1%|▎                                     | 102/10570 [00:00<00:30, 343.90it/s]\u001b[A\n",
      "  1%|▌                                     | 140/10570 [00:00<00:29, 357.91it/s]\u001b[A\n",
      "  2%|▋                                     | 176/10570 [00:00<00:29, 350.19it/s]\u001b[A\n",
      "  2%|▊                                     | 212/10570 [00:00<00:30, 340.54it/s]\u001b[A\n",
      "  2%|▉                                     | 247/10570 [00:00<00:32, 322.43it/s]\u001b[A\n",
      "  3%|█                                     | 280/10570 [00:00<00:39, 263.79it/s]\u001b[A\n",
      "  3%|█                                     | 309/10570 [00:01<00:38, 264.79it/s]\u001b[A\n",
      "  3%|█▏                                    | 343/10570 [00:01<00:36, 282.83it/s]\u001b[A\n",
      "  4%|█▎                                    | 379/10570 [00:01<00:33, 302.04it/s]\u001b[A\n",
      "  4%|█▍                                    | 412/10570 [00:01<00:32, 308.61it/s]\u001b[A\n",
      "  4%|█▌                                    | 445/10570 [00:01<00:32, 311.02it/s]\u001b[A\n",
      "  5%|█▋                                    | 477/10570 [00:01<00:32, 311.58it/s]\u001b[A\n",
      "  5%|█▊                                    | 512/10570 [00:01<00:31, 321.72it/s]\u001b[A\n",
      "  5%|█▉                                    | 547/10570 [00:01<00:30, 329.08it/s]\u001b[A\n",
      "  5%|██                                    | 581/10570 [00:01<00:30, 322.95it/s]\u001b[A\n",
      "  6%|██▏                                   | 616/10570 [00:01<00:30, 327.92it/s]\u001b[A\n",
      "  6%|██▎                                   | 651/10570 [00:02<00:29, 332.68it/s]\u001b[A\n",
      "  7%|██▍                                   | 690/10570 [00:02<00:28, 347.45it/s]\u001b[A\n",
      "  7%|██▌                                   | 725/10570 [00:02<00:28, 340.24it/s]\u001b[A\n",
      "  7%|██▋                                   | 760/10570 [00:02<00:29, 335.68it/s]\u001b[A\n",
      "  8%|██▊                                   | 794/10570 [00:02<00:29, 331.39it/s]\u001b[A\n",
      "  8%|██▉                                   | 828/10570 [00:02<00:32, 303.71it/s]\u001b[A\n",
      "  8%|███                                   | 859/10570 [00:02<00:32, 303.05it/s]\u001b[A\n",
      "  8%|███▏                                  | 893/10570 [00:02<00:30, 313.12it/s]\u001b[A\n",
      "  9%|███▎                                  | 925/10570 [00:02<00:31, 307.74it/s]\u001b[A\n",
      "  9%|███▍                                  | 956/10570 [00:03<00:32, 298.25it/s]\u001b[A\n",
      "  9%|███▌                                  | 986/10570 [00:03<00:34, 273.89it/s]\u001b[A\n",
      " 10%|███▌                                 | 1014/10570 [00:03<00:35, 266.88it/s]\u001b[A\n",
      " 10%|███▋                                 | 1041/10570 [00:03<00:36, 264.69it/s]\u001b[A\n",
      " 10%|███▋                                 | 1068/10570 [00:03<00:36, 258.90it/s]\u001b[A\n",
      " 10%|███▊                                 | 1095/10570 [00:03<00:38, 246.31it/s]\u001b[A\n",
      " 11%|███▉                                 | 1121/10570 [00:03<00:38, 248.16it/s]\u001b[A\n",
      " 11%|████                                 | 1146/10570 [00:03<00:38, 244.10it/s]\u001b[A\n",
      " 11%|████                                 | 1176/10570 [00:03<00:36, 258.01it/s]\u001b[A\n",
      " 11%|████▏                                | 1205/10570 [00:04<00:35, 266.29it/s]\u001b[A\n",
      " 12%|████▎                                | 1234/10570 [00:04<00:34, 271.27it/s]\u001b[A\n",
      " 12%|████▍                                | 1262/10570 [00:04<00:34, 269.84it/s]\u001b[A\n",
      " 12%|████▌                                | 1290/10570 [00:04<00:34, 270.36it/s]\u001b[A\n",
      " 12%|████▌                                | 1318/10570 [00:04<00:37, 248.06it/s]\u001b[A\n",
      " 13%|████▋                                | 1346/10570 [00:04<00:36, 254.15it/s]\u001b[A\n",
      " 13%|████▊                                | 1375/10570 [00:04<00:35, 261.72it/s]\u001b[A\n",
      " 13%|████▉                                | 1402/10570 [00:04<00:34, 262.28it/s]\u001b[A\n",
      " 14%|█████                                | 1429/10570 [00:04<00:35, 256.27it/s]\u001b[A\n",
      " 14%|█████                                | 1455/10570 [00:05<00:35, 253.80it/s]\u001b[A\n",
      " 14%|█████▏                               | 1481/10570 [00:05<00:36, 247.40it/s]\u001b[A\n",
      " 14%|█████▎                               | 1509/10570 [00:05<00:35, 256.24it/s]\u001b[A\n",
      " 15%|█████▎                               | 1535/10570 [00:05<00:36, 249.65it/s]\u001b[A\n",
      " 15%|█████▍                               | 1561/10570 [00:05<00:37, 241.08it/s]\u001b[A\n",
      " 15%|█████▌                               | 1586/10570 [00:05<00:37, 239.55it/s]\u001b[A\n",
      " 15%|█████▋                               | 1614/10570 [00:05<00:35, 250.14it/s]\u001b[A\n",
      " 16%|█████▋                               | 1640/10570 [00:05<00:35, 251.37it/s]\u001b[A\n",
      " 16%|█████▊                               | 1667/10570 [00:05<00:35, 253.95it/s]\u001b[A\n",
      " 16%|█████▉                               | 1694/10570 [00:05<00:34, 256.16it/s]\u001b[A\n",
      " 16%|██████                               | 1720/10570 [00:06<00:36, 241.47it/s]\u001b[A\n",
      " 17%|██████                               | 1745/10570 [00:06<00:37, 236.62it/s]\u001b[A\n",
      " 17%|██████▏                              | 1770/10570 [00:06<00:36, 238.35it/s]\u001b[A\n",
      " 17%|██████▎                              | 1797/10570 [00:06<00:35, 246.56it/s]\u001b[A\n",
      " 17%|██████▍                              | 1824/10570 [00:06<00:34, 253.31it/s]\u001b[A\n",
      " 18%|██████▍                              | 1850/10570 [00:06<00:34, 253.50it/s]\u001b[A\n",
      " 18%|██████▌                              | 1876/10570 [00:06<00:34, 254.51it/s]\u001b[A\n",
      " 18%|██████▋                              | 1905/10570 [00:06<00:33, 262.47it/s]\u001b[A\n",
      " 18%|██████▊                              | 1932/10570 [00:06<00:32, 262.38it/s]\u001b[A\n",
      " 19%|██████▊                              | 1960/10570 [00:07<00:32, 267.26it/s]\u001b[A\n",
      " 19%|██████▉                              | 1987/10570 [00:07<00:32, 262.36it/s]\u001b[A\n",
      " 19%|███████                              | 2014/10570 [00:07<00:34, 246.62it/s]\u001b[A\n",
      " 19%|███████▏                             | 2039/10570 [00:07<00:35, 237.78it/s]\u001b[A\n",
      " 20%|███████▏                             | 2064/10570 [00:07<00:35, 239.93it/s]\u001b[A\n",
      " 20%|███████▎                             | 2093/10570 [00:07<00:33, 253.56it/s]\u001b[A\n",
      " 20%|███████▍                             | 2119/10570 [00:07<00:33, 255.06it/s]\u001b[A\n",
      " 20%|███████▌                             | 2145/10570 [00:07<00:35, 238.12it/s]\u001b[A\n",
      " 21%|███████▌                             | 2172/10570 [00:07<00:34, 244.64it/s]\u001b[A\n",
      " 21%|███████▋                             | 2200/10570 [00:07<00:32, 254.30it/s]\u001b[A\n",
      " 21%|███████▊                             | 2226/10570 [00:08<00:33, 247.74it/s]\u001b[A\n",
      " 21%|███████▉                             | 2251/10570 [00:08<00:34, 243.94it/s]\u001b[A\n",
      " 22%|███████▉                             | 2278/10570 [00:08<00:33, 249.68it/s]\u001b[A\n",
      " 22%|████████                             | 2304/10570 [00:08<00:33, 245.10it/s]\u001b[A\n",
      " 22%|████████▏                            | 2329/10570 [00:08<00:33, 242.56it/s]\u001b[A\n",
      " 22%|████████▏                            | 2356/10570 [00:08<00:32, 249.75it/s]\u001b[A\n",
      " 23%|████████▎                            | 2382/10570 [00:08<00:33, 244.58it/s]\u001b[A\n",
      " 23%|████████▍                            | 2407/10570 [00:08<00:33, 243.64it/s]\u001b[A\n",
      " 23%|████████▌                            | 2432/10570 [00:08<00:33, 241.59it/s]\u001b[A\n",
      " 23%|████████▌                            | 2460/10570 [00:09<00:32, 252.48it/s]\u001b[A\n",
      " 24%|████████▋                            | 2486/10570 [00:09<00:32, 252.05it/s]\u001b[A\n",
      " 24%|████████▊                            | 2512/10570 [00:09<00:31, 253.89it/s]\u001b[A\n",
      " 24%|████████▉                            | 2540/10570 [00:09<00:31, 258.83it/s]\u001b[A\n",
      " 24%|████████▉                            | 2566/10570 [00:09<00:33, 241.17it/s]\u001b[A\n",
      " 25%|█████████                            | 2591/10570 [00:09<00:33, 240.22it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2616/10570 [00:09<00:33, 237.14it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2642/10570 [00:09<00:32, 242.11it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2672/10570 [00:09<00:30, 258.23it/s]\u001b[A\n",
      " 26%|█████████▍                           | 2701/10570 [00:10<00:29, 265.64it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2728/10570 [00:10<00:29, 263.82it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2755/10570 [00:10<00:30, 257.94it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2781/10570 [00:10<00:30, 258.08it/s]\u001b[A\n",
      " 27%|█████████▊                           | 2810/10570 [00:10<00:29, 265.09it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2837/10570 [00:10<00:30, 251.74it/s]\u001b[A\n",
      " 27%|██████████                           | 2863/10570 [00:10<00:30, 250.45it/s]\u001b[A\n",
      " 27%|██████████                           | 2889/10570 [00:10<00:32, 235.23it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2913/10570 [00:10<00:34, 223.74it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2936/10570 [00:10<00:33, 224.75it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2959/10570 [00:11<00:33, 225.84it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2982/10570 [00:11<00:33, 226.87it/s]\u001b[A\n",
      " 28%|██████████▌                          | 3007/10570 [00:11<00:32, 232.09it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3033/10570 [00:11<00:31, 239.56it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3060/10570 [00:11<00:30, 246.82it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3086/10570 [00:11<00:30, 247.63it/s]\u001b[A\n",
      " 29%|██████████▉                          | 3115/10570 [00:11<00:28, 258.10it/s]\u001b[A\n",
      " 30%|███████████                          | 3144/10570 [00:11<00:27, 266.80it/s]\u001b[A\n",
      " 30%|███████████                          | 3174/10570 [00:11<00:26, 274.35it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3202/10570 [00:12<00:28, 259.92it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3229/10570 [00:12<00:30, 243.92it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3254/10570 [00:12<00:30, 242.09it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3279/10570 [00:12<00:29, 243.26it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3307/10570 [00:12<00:28, 253.49it/s]\u001b[A\n",
      " 32%|███████████▋                         | 3333/10570 [00:12<00:28, 253.33it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3359/10570 [00:12<00:29, 248.42it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3386/10570 [00:12<00:28, 252.56it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3412/10570 [00:12<00:28, 248.93it/s]\u001b[A\n",
      " 33%|████████████                         | 3437/10570 [00:13<00:30, 237.17it/s]\u001b[A\n",
      " 33%|████████████                         | 3462/10570 [00:13<00:29, 238.68it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3490/10570 [00:13<00:28, 248.39it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3516/10570 [00:13<00:28, 247.57it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3541/10570 [00:13<00:28, 242.97it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3566/10570 [00:13<00:29, 240.40it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3591/10570 [00:13<00:28, 241.45it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3618/10570 [00:13<00:28, 248.14it/s]\u001b[A\n",
      " 34%|████████████▊                        | 3643/10570 [00:13<00:28, 240.85it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3671/10570 [00:13<00:27, 250.84it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3697/10570 [00:14<00:27, 249.83it/s]\u001b[A\n",
      " 35%|█████████████                        | 3726/10570 [00:14<00:26, 259.32it/s]\u001b[A\n",
      " 35%|█████████████▏                       | 3752/10570 [00:14<00:27, 247.53it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3780/10570 [00:14<00:26, 254.79it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3806/10570 [00:14<00:27, 245.47it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3831/10570 [00:14<00:27, 242.76it/s]\u001b[A\n",
      " 36%|█████████████▌                       | 3857/10570 [00:14<00:27, 247.23it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3885/10570 [00:14<00:26, 255.33it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3914/10570 [00:14<00:25, 264.49it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3943/10570 [00:15<00:24, 270.89it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3971/10570 [00:15<00:24, 266.88it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3998/10570 [00:15<00:24, 263.41it/s]\u001b[A\n",
      " 38%|██████████████                       | 4025/10570 [00:15<00:25, 256.19it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4051/10570 [00:15<00:25, 254.95it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4077/10570 [00:15<00:26, 242.21it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4105/10570 [00:15<00:25, 251.33it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4133/10570 [00:15<00:24, 258.81it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4160/10570 [00:15<00:29, 216.79it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4183/10570 [00:16<00:31, 204.46it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4205/10570 [00:16<00:34, 184.00it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4227/10570 [00:16<00:33, 191.06it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4253/10570 [00:16<00:30, 207.39it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4275/10570 [00:16<00:37, 168.05it/s]\u001b[A\n",
      " 41%|███████████████                      | 4294/10570 [00:16<00:40, 155.92it/s]\u001b[A\n",
      " 41%|███████████████                      | 4311/10570 [00:16<00:40, 153.44it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4334/10570 [00:16<00:36, 171.69it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4360/10570 [00:17<00:32, 191.21it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4386/10570 [00:17<00:29, 208.82it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4414/10570 [00:17<00:27, 227.05it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4441/10570 [00:17<00:25, 238.16it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4469/10570 [00:17<00:24, 248.86it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4497/10570 [00:17<00:23, 255.91it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4525/10570 [00:17<00:23, 260.71it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4553/10570 [00:17<00:22, 265.27it/s]\u001b[A\n",
      " 43%|████████████████                     | 4581/10570 [00:17<00:22, 268.63it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4608/10570 [00:18<00:23, 255.98it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4634/10570 [00:18<00:24, 243.74it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4659/10570 [00:18<00:25, 230.15it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4688/10570 [00:18<00:24, 243.14it/s]\u001b[A\n",
      " 45%|████████████████▍                    | 4713/10570 [00:18<00:25, 230.57it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4738/10570 [00:18<00:24, 234.72it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4762/10570 [00:18<00:25, 229.77it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4787/10570 [00:18<00:24, 235.28it/s]\u001b[A\n",
      " 46%|████████████████▊                    | 4811/10570 [00:18<00:24, 236.61it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4839/10570 [00:19<00:23, 246.81it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4864/10570 [00:19<00:25, 223.07it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4888/10570 [00:19<00:25, 225.95it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4914/10570 [00:19<00:24, 234.84it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4938/10570 [00:19<00:24, 228.16it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4962/10570 [00:19<00:25, 220.69it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4985/10570 [00:19<00:25, 220.53it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5009/10570 [00:19<00:24, 225.94it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5036/10570 [00:19<00:23, 238.14it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5064/10570 [00:19<00:22, 249.93it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5092/10570 [00:20<00:21, 257.51it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5119/10570 [00:20<00:20, 260.24it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5146/10570 [00:20<00:20, 258.79it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5172/10570 [00:20<00:22, 242.91it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5197/10570 [00:20<00:22, 243.39it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5222/10570 [00:20<00:22, 236.72it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5252/10570 [00:20<00:21, 252.46it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5279/10570 [00:20<00:20, 257.23it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5305/10570 [00:20<00:20, 256.55it/s]\u001b[A\n",
      " 50%|██████████████████▋                  | 5331/10570 [00:21<00:21, 249.14it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5357/10570 [00:21<00:21, 246.80it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5382/10570 [00:21<00:21, 244.45it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5407/10570 [00:21<00:21, 237.11it/s]\u001b[A\n",
      " 51%|███████████████████                  | 5431/10570 [00:21<00:22, 225.95it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5454/10570 [00:21<00:25, 203.77it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5479/10570 [00:21<00:23, 215.27it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5501/10570 [00:21<00:23, 211.93it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5527/10570 [00:21<00:22, 223.44it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5555/10570 [00:22<00:21, 237.92it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5580/10570 [00:22<00:21, 234.94it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5606/10570 [00:22<00:20, 240.79it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5631/10570 [00:22<00:21, 230.71it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5655/10570 [00:22<00:21, 224.42it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5678/10570 [00:22<00:21, 223.22it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5703/10570 [00:22<00:21, 228.96it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5729/10570 [00:22<00:20, 235.84it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 5757/10570 [00:22<00:19, 248.01it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5782/10570 [00:23<00:19, 245.20it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5807/10570 [00:23<00:25, 188.19it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5832/10570 [00:23<00:23, 202.05it/s]\u001b[A\n",
      " 55%|████████████████████▌                | 5859/10570 [00:23<00:21, 218.55it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5885/10570 [00:23<00:20, 228.21it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5911/10570 [00:23<00:19, 235.84it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5936/10570 [00:23<00:19, 237.98it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5963/10570 [00:23<00:18, 246.15it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5991/10570 [00:23<00:18, 253.82it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6017/10570 [00:24<00:18, 246.49it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6045/10570 [00:24<00:17, 253.47it/s]\u001b[A\n",
      " 57%|█████████████████████▎               | 6071/10570 [00:24<00:18, 239.22it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6096/10570 [00:24<00:18, 241.97it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6122/10570 [00:24<00:18, 245.73it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6148/10570 [00:24<00:17, 249.34it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6174/10570 [00:24<00:18, 242.03it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6201/10570 [00:24<00:17, 247.84it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6226/10570 [00:24<00:19, 225.95it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6251/10570 [00:25<00:18, 230.78it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6275/10570 [00:25<00:18, 226.71it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6302/10570 [00:25<00:18, 236.74it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6331/10570 [00:25<00:16, 250.76it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6357/10570 [00:25<00:16, 249.75it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6383/10570 [00:25<00:17, 237.84it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6409/10570 [00:25<00:17, 241.63it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6434/10570 [00:25<00:16, 243.35it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6459/10570 [00:25<00:17, 241.21it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6484/10570 [00:25<00:16, 242.15it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6510/10570 [00:26<00:16, 244.99it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6535/10570 [00:26<00:16, 238.88it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6562/10570 [00:26<00:16, 247.31it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6588/10570 [00:26<00:15, 249.39it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6613/10570 [00:26<00:15, 248.05it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6638/10570 [00:26<00:15, 247.89it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6663/10570 [00:26<00:16, 234.36it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6688/10570 [00:26<00:16, 236.70it/s]\u001b[A\n",
      " 64%|███████████████████████▍             | 6712/10570 [00:26<00:17, 219.65it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6741/10570 [00:27<00:16, 237.75it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6766/10570 [00:27<00:15, 240.32it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6791/10570 [00:27<00:15, 239.16it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6817/10570 [00:27<00:15, 241.45it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6843/10570 [00:27<00:15, 246.05it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6871/10570 [00:27<00:14, 253.19it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6897/10570 [00:27<00:14, 249.96it/s]\u001b[A\n",
      " 66%|████████████████████████▏            | 6925/10570 [00:27<00:14, 256.93it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6951/10570 [00:27<00:14, 250.91it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6977/10570 [00:28<00:15, 238.11it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7004/10570 [00:28<00:14, 244.61it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7029/10570 [00:28<00:14, 237.67it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7055/10570 [00:28<00:14, 243.51it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7082/10570 [00:28<00:13, 249.21it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7108/10570 [00:28<00:13, 251.79it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7134/10570 [00:28<00:14, 242.76it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7161/10570 [00:28<00:13, 249.40it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7188/10570 [00:28<00:13, 254.57it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7214/10570 [00:28<00:13, 247.64it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7239/10570 [00:29<00:13, 246.23it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7265/10570 [00:29<00:13, 249.22it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7291/10570 [00:29<00:12, 252.34it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7318/10570 [00:29<00:12, 255.30it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7344/10570 [00:29<00:13, 233.85it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7372/10570 [00:29<00:12, 246.51it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7398/10570 [00:29<00:12, 247.96it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7424/10570 [00:29<00:12, 247.58it/s]\u001b[A\n",
      " 71%|██████████████████████████           | 7453/10570 [00:29<00:12, 258.05it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7479/10570 [00:30<00:11, 258.60it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7507/10570 [00:30<00:11, 263.91it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7536/10570 [00:30<00:11, 270.27it/s]\u001b[A\n",
      " 72%|██████████████████████████▍          | 7564/10570 [00:30<00:11, 265.07it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7591/10570 [00:30<00:11, 262.21it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7618/10570 [00:30<00:11, 258.18it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7644/10570 [00:30<00:11, 247.49it/s]\u001b[A\n",
      " 73%|██████████████████████████▊          | 7669/10570 [00:30<00:12, 232.64it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7693/10570 [00:30<00:12, 231.81it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7719/10570 [00:30<00:11, 238.74it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7744/10570 [00:31<00:11, 239.81it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7769/10570 [00:31<00:12, 230.58it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7793/10570 [00:31<00:11, 232.52it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7819/10570 [00:31<00:11, 239.23it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7844/10570 [00:31<00:11, 233.82it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7868/10570 [00:31<00:11, 232.97it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7893/10570 [00:31<00:11, 235.28it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7917/10570 [00:31<00:12, 218.72it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7945/10570 [00:31<00:11, 233.43it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7969/10570 [00:32<00:11, 230.35it/s]\u001b[A\n",
      " 76%|███████████████████████████▉         | 7993/10570 [00:32<00:11, 224.12it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8017/10570 [00:32<00:11, 228.12it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8040/10570 [00:32<00:11, 221.88it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8063/10570 [00:32<00:11, 221.27it/s]\u001b[A\n",
      " 76%|████████████████████████████▎        | 8086/10570 [00:32<00:11, 209.75it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8113/10570 [00:32<00:10, 224.84it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8136/10570 [00:32<00:11, 216.85it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8162/10570 [00:32<00:10, 228.51it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 8188/10570 [00:33<00:10, 233.36it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8212/10570 [00:33<00:10, 227.79it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8237/10570 [00:33<00:09, 234.07it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8262/10570 [00:33<00:09, 237.22it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 8286/10570 [00:33<00:10, 215.07it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8312/10570 [00:33<00:09, 226.97it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8336/10570 [00:33<00:09, 225.61it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8361/10570 [00:33<00:09, 229.19it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8388/10570 [00:33<00:09, 239.33it/s]\u001b[A\n",
      " 80%|█████████████████████████████▍       | 8418/10570 [00:34<00:08, 254.55it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8446/10570 [00:34<00:08, 260.29it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8473/10570 [00:34<00:08, 261.22it/s]\u001b[A\n",
      " 80%|█████████████████████████████▊       | 8500/10570 [00:34<00:08, 248.65it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8526/10570 [00:34<00:08, 240.93it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8551/10570 [00:34<00:08, 237.11it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8575/10570 [00:34<00:08, 236.82it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8603/10570 [00:34<00:07, 247.28it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8628/10570 [00:34<00:07, 246.75it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8655/10570 [00:34<00:07, 251.53it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8681/10570 [00:35<00:07, 247.20it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8706/10570 [00:35<00:07, 246.33it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8731/10570 [00:35<00:07, 241.83it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8757/10570 [00:35<00:07, 246.00it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8782/10570 [00:35<00:07, 242.31it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8807/10570 [00:35<00:07, 235.15it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8831/10570 [00:35<00:07, 225.49it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8854/10570 [00:35<00:07, 219.56it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8877/10570 [00:35<00:07, 222.39it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8902/10570 [00:36<00:07, 230.16it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8927/10570 [00:36<00:06, 235.44it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8954/10570 [00:36<00:06, 243.90it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8979/10570 [00:36<00:06, 236.37it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9006/10570 [00:36<00:06, 245.62it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9031/10570 [00:36<00:06, 242.61it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9056/10570 [00:36<00:06, 241.75it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9084/10570 [00:36<00:05, 252.29it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9110/10570 [00:36<00:06, 228.51it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9138/10570 [00:37<00:05, 241.64it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9165/10570 [00:37<00:05, 247.14it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9191/10570 [00:37<00:05, 240.32it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9216/10570 [00:37<00:05, 240.74it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9241/10570 [00:37<00:05, 238.03it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9267/10570 [00:37<00:05, 240.90it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9294/10570 [00:37<00:05, 247.65it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9320/10570 [00:37<00:05, 249.59it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9347/10570 [00:37<00:04, 252.91it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9373/10570 [00:37<00:04, 253.18it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9401/10570 [00:38<00:04, 259.13it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9427/10570 [00:38<00:04, 252.95it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9453/10570 [00:38<00:04, 243.39it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9479/10570 [00:38<00:04, 247.63it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9506/10570 [00:38<00:04, 253.86it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9532/10570 [00:38<00:04, 251.64it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9558/10570 [00:38<00:04, 246.29it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9585/10570 [00:38<00:03, 249.04it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9610/10570 [00:38<00:04, 238.77it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9636/10570 [00:39<00:03, 242.62it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9662/10570 [00:39<00:03, 246.39it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9687/10570 [00:39<00:03, 238.98it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9715/10570 [00:39<00:03, 248.24it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9742/10570 [00:39<00:03, 251.79it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9768/10570 [00:39<00:03, 249.40it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9794/10570 [00:39<00:03, 249.81it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9820/10570 [00:39<00:03, 245.09it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9845/10570 [00:39<00:02, 246.04it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9874/10570 [00:39<00:02, 256.26it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9900/10570 [00:40<00:02, 253.61it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9926/10570 [00:40<00:02, 251.57it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9952/10570 [00:40<00:02, 250.35it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9979/10570 [00:40<00:02, 254.99it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10005/10570 [00:40<00:02, 253.30it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10032/10570 [00:40<00:02, 257.79it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10058/10570 [00:40<00:02, 252.81it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10084/10570 [00:40<00:01, 253.93it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10110/10570 [00:40<00:02, 219.53it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10133/10570 [00:41<00:02, 210.35it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10155/10570 [00:41<00:02, 207.22it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10177/10570 [00:41<00:01, 208.71it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10199/10570 [00:41<00:01, 211.38it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10225/10570 [00:41<00:01, 223.19it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10251/10570 [00:41<00:01, 232.83it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10277/10570 [00:41<00:01, 239.93it/s]\u001b[A\n",
      " 98%|███████████████████████████████████ | 10306/10570 [00:41<00:01, 252.10it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10334/10570 [00:41<00:00, 258.01it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10361/10570 [00:42<00:00, 260.05it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10388/10570 [00:42<00:00, 259.07it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▍| 10414/10570 [00:42<00:00, 257.52it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10440/10570 [00:42<00:00, 251.21it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10466/10570 [00:42<00:00, 249.66it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10492/10570 [00:42<00:00, 249.96it/s]\u001b[A\n",
      "100%|███████████████████████████████████▊| 10518/10570 [00:42<00:00, 246.22it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:42<00:00, 246.74it/s]\u001b[A\n",
      "04/27/2022 18:46:12 - INFO - utils_qa - Saving predictions to ./squad_deberta_base/eval_predictions.json.\n",
      "04/27/2022 18:46:12 - INFO - utils_qa - Saving nbest_preds to ./squad_deberta_base/eval_nbest_predictions.json.\n",
      "04/27/2022 18:46:22 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|███████████████████████████████████████| 1349/1349 [06:48<00:00,  3.30it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match = 86.2914\n",
      "  eval_f1          = 92.2638\n",
      "  eval_samples     =   10788\n",
      "[INFO|modelcard.py:460] 2022-04-27 18:46:22,353 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Question Answering', 'type': 'question-answering'}, 'dataset': {'name': 'squad', 'type': 'squad', 'args': 'plain_text'}}\n"
     ]
    }
   ],
   "source": [
    "!sh train_deberta.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dfa2c4-43f8-424f-ad1c-00e9ee16ef62",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb00710-1494-4967-9213-d49ad8906c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa6df812-e371-4a66-bbfb-179311ba5b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 23.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[K     |████████████████████████████████| 325 kB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 45.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 40.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 35.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill\n",
      "  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[K     |████████████████████████████████| 86 kB 57.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[K     |████████████████████████████████| 158 kB 31.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Installing collected packages: frozenlist, async-timeout, aiosignal, dill, aiohttp, xxhash, tokenizers, responses, multiprocess, huggingface-hub, transformers, datasets\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 datasets-2.1.0 dill-0.3.4 frozenlist-1.3.0 huggingface-hub-0.5.1 multiprocess-0.70.12.2 responses-0.18.0 tokenizers-0.12.1 transformers-4.18.0 xxhash-3.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e1624e-224e-462c-95c8-9c1fbbb76c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torch==1.9.0\n",
      "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 831.4 MB 24.8 MB/s eta 0:00:01     |████████████████                | 414.4 MB 20.5 MB/s eta 0:00:21\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "Installing collected packages: torch\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.11.0a0+17540c5\n",
      "    Uninstalling torch-1.11.0a0+17540c5:\n",
      "      Successfully uninstalled torch-1.11.0a0+17540c5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torchtext 0.12.0a0 requires torch==1.11.0a0+17540c5, but you have torch 1.9.0 which is incompatible.\n",
      "torch-tensorrt 1.1.0a0 requires torch>=1.10.0+cu113<1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
      "Successfully installed torch-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11fcf91c-9b98-41db-ad8b-45ef1d5712b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/04/2022 12:42:33 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "05/04/2022 12:42:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_bert_base_uncased_advanced/runs/May04_12-42-33_nlijsfla4v,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_bert_base_uncased_advanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_bert_base_uncased_advanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpg4nbj7bz\n",
      "Downloading builder script: 5.27kB [00:00, 3.23MB/s]                            \n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpiszvczts\n",
      "Downloading metadata: 2.36kB [00:00, 1.42MB/s]                                  \n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/04/2022 12:42:33 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "05/04/2022 12:42:33 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\n",
      "05/04/2022 12:42:33 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "05/04/2022 12:42:33 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "05/04/2022 12:42:33 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]05/04/2022 12:42:34 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpmu5f4_oc\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  88%|█████████████████▌  | 7.10M/8.12M [00:00<00:00, 71.0MB/s]\u001b[A\n",
      "Downloading data: 14.2MB [00:00, 71.2MB/s]                                      \u001b[A\n",
      "Downloading data: 21.5MB [00:00, 71.6MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 71.3MB/s]\u001b[A\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.21s/it]05/04/2022 12:42:35 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpvmxfwnev\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 85.8MB/s]                                      \u001b[A\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.39it/s]\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1495.03it/s]\n",
      "05/04/2022 12:42:35 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "05/04/2022 12:42:35 - INFO - datasets.builder - Generating train split\n",
      "05/04/2022 12:42:42 - INFO - datasets.builder - Generating validation split     \n",
      "05/04/2022 12:42:43 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 517.72it/s]\n",
      "[INFO|hub.py:583] 2022-05-04 12:42:43,325 >> https://huggingface.co/bert-base-uncased/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6z_7xyhw\n",
      "Downloading: 100%|██████████████████████████████| 570/570 [00:00<00:00, 406kB/s]\n",
      "[INFO|hub.py:587] 2022-05-04 12:42:43,391 >> storing https://huggingface.co/bert-base-uncased/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|hub.py:595] 2022-05-04 12:42:43,391 >> creating metadata file for /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:654] 2022-05-04 12:42:43,391 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-05-04 12:42:43,392 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-04 12:42:43,458 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp3857sagg\n",
      "Downloading: 100%|███████████████████████████| 28.0/28.0 [00:00<00:00, 37.7kB/s]\n",
      "[INFO|hub.py:587] 2022-05-04 12:42:43,527 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|hub.py:595] 2022-05-04 12:42:43,528 >> creating metadata file for /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:654] 2022-05-04 12:42:43,589 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-05-04 12:42:43,590 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-04 12:42:43,720 >> https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpvigrg77x\n",
      "Downloading: 100%|███████████████████████████| 226k/226k [00:00<00:00, 6.46MB/s]\n",
      "[INFO|hub.py:587] 2022-05-04 12:42:43,825 >> storing https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:595] 2022-05-04 12:42:43,825 >> creating metadata file for /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|hub.py:583] 2022-05-04 12:42:43,887 >> https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpmwoiv8h0\n",
      "Downloading: 100%|███████████████████████████| 455k/455k [00:00<00:00, 11.6MB/s]\n",
      "[INFO|hub.py:587] 2022-05-04 12:42:44,010 >> storing https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|hub.py:595] 2022-05-04 12:42:44,010 >> creating metadata file for /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-05-04 12:42:44,201 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-05-04 12:42:44,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-05-04 12:42:44,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-05-04 12:42:44,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-05-04 12:42:44,202 >> loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "[INFO|configuration_utils.py:654] 2022-05-04 12:42:44,262 >> loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "[INFO|configuration_utils.py:690] 2022-05-04 12:42:44,262 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-05-04 12:42:44,356 >> https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmps55_6q20\n",
      "Downloading: 100%|███████████████████████████| 420M/420M [00:18<00:00, 23.6MB/s]\n",
      "[INFO|hub.py:587] 2022-05-04 12:43:03,112 >> storing https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|hub.py:595] 2022-05-04 12:43:03,112 >> creating metadata file for /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[INFO|modeling_utils.py:1772] 2022-05-04 12:43:03,113 >> loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "[WARNING|modeling_utils.py:2048] 2022-05-04 12:43:04,740 >> Some weights of the model checkpoint at bert-base-uncased were not used when initializing Advanced_BertForQuestionAnswering: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Advanced_BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2059] 2022-05-04 12:43:04,740 >> Some weights of Advanced_BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]05/04/2022 12:43:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-d323f00a5ef1bf76.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:44<00:00,  2.00ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]05/04/2022 12:43:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-91fc149e60f919a4.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:36<00:00,  3.34s/ba]\n",
      "05/04/2022 12:44:25 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjwziv52l\n",
      "Downloading builder script: 4.50kB [00:00, 3.49MB/s]                            \n",
      "05/04/2022 12:44:26 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/04/2022 12:44:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "05/04/2022 12:44:26 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp0fgb8x2n\n",
      "Downloading extra modules: 3.31kB [00:00, 2.41MB/s]                             \n",
      "05/04/2022 12:44:26 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "05/04/2022 12:44:26 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "[INFO|trainer.py:453] 2022-05-04 12:44:29,278 >> Using amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1290] 2022-05-04 12:44:29,435 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-05-04 12:44:29,436 >>   Num examples = 88524\n",
      "[INFO|trainer.py:1292] 2022-05-04 12:44:29,436 >>   Num Epochs = 2\n",
      "[INFO|trainer.py:1293] 2022-05-04 12:44:29,436 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1294] 2022-05-04 12:44:29,436 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1295] 2022-05-04 12:44:29,436 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-05-04 12:44:29,436 >>   Total optimization steps = 14754\n",
      "  0%|                                                 | 0/14754 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 2.2574, 'learning_rate': 2.899145994306629e-05, 'epoch': 0.07}         \n",
      "  3%|█▎                                   | 500/14754 [05:59<2:51:33,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 12:50:29,367 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 12:50:29,370 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 12:50:30,479 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 12:50:30,510 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 12:50:30,529 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.4774, 'learning_rate': 2.7974786498576658e-05, 'epoch': 0.14}        \n",
      "  7%|██▍                                 | 1000/14754 [12:03<2:45:37,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 12:56:33,203 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 12:56:33,204 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 12:56:34,454 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 12:56:34,487 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 12:56:34,488 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.3746, 'learning_rate': 2.695811305408703e-05, 'epoch': 0.2}          \n",
      " 10%|███▋                                | 1500/14754 [18:07<2:39:13,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:02:37,002 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-1500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:02:37,004 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:02:38,148 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:02:38,150 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:02:38,174 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.297, 'learning_rate': 2.59414396095974e-05, 'epoch': 0.27}           \n",
      " 14%|████▉                               | 2000/14754 [24:11<2:33:06,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:08:40,549 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:08:40,571 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:08:41,564 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:08:41,581 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:08:41,582 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.2402, 'learning_rate': 2.4924766165107768e-05, 'epoch': 0.34}        \n",
      " 17%|██████                              | 2500/14754 [30:14<2:26:49,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:14:43,728 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-2500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:14:43,730 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:14:44,766 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:14:44,776 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:14:44,776 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.1796, 'learning_rate': 2.390809272061814e-05, 'epoch': 0.41}         \n",
      " 20%|███████▎                            | 3000/14754 [36:17<2:21:13,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:20:47,099 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:20:47,102 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:20:48,190 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:20:48,211 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:20:48,212 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 1.1896, 'learning_rate': 2.2891419276128507e-05, 'epoch': 0.47}        \n",
      " 24%|████████▌                           | 3500/14754 [42:21<2:15:20,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:26:50,904 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-3500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:26:50,930 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:26:52,160 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:26:52,162 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:26:52,185 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 1.1249, 'learning_rate': 2.1874745831638878e-05, 'epoch': 0.54}        \n",
      " 27%|█████████▊                          | 4000/14754 [48:25<2:09:13,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:32:54,654 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:32:54,655 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:32:55,893 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:32:55,899 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:32:55,900 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4000/special_tokens_map.json\n",
      "{'loss': 1.0773, 'learning_rate': 2.085807238714925e-05, 'epoch': 0.61}         \n",
      " 31%|██████████▉                         | 4500/14754 [54:29<2:03:08,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:38:58,534 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-4500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:38:58,540 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:38:59,659 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:38:59,662 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:38:59,663 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-4500/special_tokens_map.json\n",
      "{'loss': 1.0855, 'learning_rate': 1.9841398942659617e-05, 'epoch': 0.68}        \n",
      " 34%|███████████▌                      | 5000/14754 [1:00:32<1:57:17,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 13:45:02,306 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:45:02,308 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:45:03,355 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:45:03,374 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:45:03,375 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 1.1133, 'learning_rate': 1.8824725498169988e-05, 'epoch': 0.75}        \n",
      " 37%|████████████▋                     | 5500/14754 [1:06:36<1:51:25,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 13:51:06,084 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-5500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:51:06,086 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:51:07,101 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:51:07,143 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:51:07,144 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-5500/special_tokens_map.json\n",
      " 40%|█████████████▍                    | 5853/14754 [1:10:54<1:46:52,  1.39it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.0717, 'learning_rate': 1.7810085400569338e-05, 'epoch': 0.81}        \n",
      " 41%|█████████████▊                    | 6000/14754 [1:12:40<1:45:42,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 13:57:09,632 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 13:57:09,681 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 13:57:10,831 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 13:57:10,833 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 13:57:10,834 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 1.0199, 'learning_rate': 1.679341195607971e-05, 'epoch': 0.88}         \n",
      " 44%|██████████████▉                   | 6500/14754 [1:18:44<1:39:26,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 14:03:13,738 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-6500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:03:13,740 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:03:14,885 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:03:14,886 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:03:14,887 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 1.0444, 'learning_rate': 1.5776738511590077e-05, 'epoch': 0.95}        \n",
      " 47%|████████████████▏                 | 7000/14754 [1:24:48<1:33:10,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:09:17,500 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-7000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:09:17,504 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:09:18,459 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:09:18,478 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:09:18,481 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-7000/special_tokens_map.json\n",
      "{'loss': 0.9512, 'learning_rate': 1.4760065067100448e-05, 'epoch': 1.02}        \n",
      " 51%|█████████████████▎                | 7500/14754 [1:30:53<1:28:20,  1.37it/s][INFO|trainer.py:2166] 2022-05-04 14:15:23,109 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-7500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:15:23,111 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:15:24,172 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:15:24,173 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:15:24,174 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-7500/special_tokens_map.json\n",
      " 54%|██████████████████▏               | 7895/14754 [1:35:43<1:22:49,  1.38it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.7516, 'learning_rate': 1.3747458316388776e-05, 'epoch': 1.08}        \n",
      " 54%|██████████████████▍               | 8000/14754 [1:36:59<1:21:29,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 14:21:29,237 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-8000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:21:29,240 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:21:30,309 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:21:30,328 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:21:30,329 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-8000/special_tokens_map.json\n",
      "{'loss': 0.7073, 'learning_rate': 1.2730784871899148e-05, 'epoch': 1.15}        \n",
      " 58%|███████████████████▌              | 8500/14754 [1:43:05<1:15:29,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 14:27:35,422 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-8500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:27:35,425 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:27:36,718 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:27:36,733 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:27:36,734 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-8500/special_tokens_map.json\n",
      " 58%|███████████████████▋              | 8532/14754 [1:43:32<1:15:03,  1.38it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.7215, 'learning_rate': 1.1718178121187474e-05, 'epoch': 1.22}        \n",
      " 61%|████████████████████▋             | 9000/14754 [1:49:10<1:08:59,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:33:39,584 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-9000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:33:39,606 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:33:40,571 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:33:40,589 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:33:40,589 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-9000/special_tokens_map.json\n",
      "{'loss': 0.7431, 'learning_rate': 1.0701504676697845e-05, 'epoch': 1.29}        \n",
      " 64%|█████████████████████▉            | 9500/14754 [1:55:12<1:03:02,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:39:42,398 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-9500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:39:42,399 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:39:43,358 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:39:43,379 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:39:43,380 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-9500/special_tokens_map.json\n",
      "{'loss': 0.7044, 'learning_rate': 9.684831232208215e-06, 'epoch': 1.36}         \n",
      " 68%|███████████████████████▋           | 10000/14754 [2:01:15<57:03,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:45:45,248 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-10000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:45:45,272 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:45:46,304 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:45:46,320 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:45:46,321 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-10000/special_tokens_map.json\n",
      "{'loss': 0.7459, 'learning_rate': 8.668157787718586e-06, 'epoch': 1.42}         \n",
      " 71%|████████████████████████▉          | 10500/14754 [2:07:18<51:02,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:51:48,304 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-10500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:51:48,328 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:51:49,329 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:51:49,346 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:51:49,347 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-10500/special_tokens_map.json\n",
      "{'loss': 0.7022, 'learning_rate': 7.651484343228956e-06, 'epoch': 1.49}         \n",
      " 75%|██████████████████████████         | 11000/14754 [2:13:21<44:57,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 14:57:51,313 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-11000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 14:57:51,339 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 14:57:52,627 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 14:57:52,678 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 14:57:52,683 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-11000/special_tokens_map.json\n",
      "{'loss': 0.7089, 'learning_rate': 6.634810898739325e-06, 'epoch': 1.56}         \n",
      " 78%|███████████████████████████▎       | 11500/14754 [2:19:26<39:18,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:03:56,412 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-11500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:03:56,434 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:03:57,476 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:03:57,478 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:03:57,479 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-11500/special_tokens_map.json\n",
      "{'loss': 0.7043, 'learning_rate': 5.618137454249695e-06, 'epoch': 1.63}         \n",
      " 81%|████████████████████████████▍      | 12000/14754 [2:25:32<33:17,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:10:01,830 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-12000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:10:01,831 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:10:02,811 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:10:02,812 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:10:02,813 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-12000/special_tokens_map.json\n",
      "{'loss': 0.6836, 'learning_rate': 4.601464009760066e-06, 'epoch': 1.69}         \n",
      " 85%|█████████████████████████████▋     | 12500/14754 [2:31:37<27:15,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:16:07,037 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-12500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:16:07,039 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:16:08,085 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:16:08,106 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:16:08,107 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-12500/special_tokens_map.json\n",
      "{'loss': 0.6951, 'learning_rate': 3.584790565270435e-06, 'epoch': 1.76}         \n",
      " 88%|██████████████████████████████▊    | 13000/14754 [2:37:42<21:12,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:22:12,276 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-13000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:22:12,299 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:22:13,269 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:22:13,271 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:22:13,292 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-13000/special_tokens_map.json\n",
      "{'loss': 0.6937, 'learning_rate': 2.5681171207808054e-06, 'epoch': 1.83}        \n",
      " 92%|████████████████████████████████   | 13500/14754 [2:43:47<15:04,  1.39it/s][INFO|trainer.py:2166] 2022-05-04 15:28:17,361 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-13500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:28:17,363 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:28:18,645 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:28:18,653 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:28:18,654 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-13500/special_tokens_map.json\n",
      "{'loss': 0.6937, 'learning_rate': 1.5514436762911753e-06, 'epoch': 1.9}         \n",
      " 95%|█████████████████████████████████▏ | 14000/14754 [2:49:53<09:04,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:34:22,482 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-14000\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:34:22,504 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:34:23,766 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:34:23,784 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:34:23,785 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-14000/special_tokens_map.json\n",
      " 95%|█████████████████████████████████▎ | 14059/14754 [2:50:38<08:21,  1.39it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.6881, 'learning_rate': 5.368035786905246e-07, 'epoch': 1.97}         \n",
      " 98%|██████████████████████████████████▍| 14500/14754 [2:55:58<03:03,  1.38it/s][INFO|trainer.py:2166] 2022-05-04 15:40:27,503 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced/checkpoint-14500\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:40:27,505 >> Configuration saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:40:28,510 >> Model weights saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:40:28,512 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:40:28,513 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/checkpoint-14500/special_tokens_map.json\n",
      "100%|███████████████████████████████████| 14754/14754 [2:59:04<00:00,  1.39it/s][INFO|trainer.py:1530] 2022-05-04 15:43:34,182 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 10744.746, 'train_samples_per_second': 16.478, 'train_steps_per_second': 1.373, 'train_loss': 0.9754309709178444, 'epoch': 2.0}\n",
      "100%|███████████████████████████████████| 14754/14754 [2:59:04<00:00,  1.37it/s]\n",
      "[INFO|trainer.py:2166] 2022-05-04 15:43:34,306 >> Saving model checkpoint to ./squad_bert_base_uncased_advanced\n",
      "[INFO|configuration_utils.py:441] 2022-05-04 15:43:34,308 >> Configuration saved in ./squad_bert_base_uncased_advanced/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-05-04 15:43:35,341 >> Model weights saved in ./squad_bert_base_uncased_advanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-05-04 15:43:35,342 >> tokenizer config file saved in ./squad_bert_base_uncased_advanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-05-04 15:43:35,363 >> Special tokens file saved in ./squad_bert_base_uncased_advanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        2.0\n",
      "  train_loss               =     0.9754\n",
      "  train_runtime            = 2:59:04.74\n",
      "  train_samples            =      88524\n",
      "  train_samples_per_second =     16.478\n",
      "  train_steps_per_second   =      1.373\n",
      "05/04/2022 15:43:35 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:566] 2022-05-04 15:43:35,440 >> The following columns in the evaluation set  don't have a corresponding argument in `Advanced_BertForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `Advanced_BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2416] 2022-05-04 15:43:35,453 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2022-05-04 15:43:35,453 >>   Num examples = 10784\n",
      "[INFO|trainer.py:2421] 2022-05-04 15:43:35,453 >>   Batch size = 12\n",
      "100%|█████████████████████████████████████████| 899/899 [03:58<00:00,  4.15it/s]05/04/2022 15:47:49 - INFO - utils_qa - Post-processing 10570 example predictions split into 10784 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                       | 20/10570 [00:00<00:53, 195.86it/s]\u001b[A\n",
      "  0%|▏                                      | 44/10570 [00:00<00:47, 220.10it/s]\u001b[A\n",
      "  1%|▎                                      | 78/10570 [00:00<00:38, 272.53it/s]\u001b[A\n",
      "  1%|▍                                     | 114/10570 [00:00<00:34, 306.46it/s]\u001b[A\n",
      "  1%|▌                                     | 152/10570 [00:00<00:31, 330.29it/s]\u001b[A\n",
      "  2%|▋                                     | 189/10570 [00:00<00:30, 343.29it/s]\u001b[A\n",
      "  2%|▊                                     | 224/10570 [00:00<00:29, 345.32it/s]\u001b[A\n",
      "  2%|▉                                     | 259/10570 [00:00<00:35, 293.19it/s]\u001b[A\n",
      "  3%|█                                     | 290/10570 [00:01<00:40, 256.19it/s]\u001b[A\n",
      "  3%|█▏                                    | 325/10570 [00:01<00:36, 278.03it/s]\u001b[A\n",
      "  3%|█▎                                    | 363/10570 [00:01<00:33, 303.18it/s]\u001b[A\n",
      "  4%|█▍                                    | 400/10570 [00:01<00:31, 320.60it/s]\u001b[A\n",
      "  4%|█▌                                    | 436/10570 [00:01<00:30, 329.82it/s]\u001b[A\n",
      "  4%|█▋                                    | 475/10570 [00:01<00:29, 346.82it/s]\u001b[A\n",
      "  5%|█▊                                    | 518/10570 [00:01<00:27, 370.16it/s]\u001b[A\n",
      "  5%|██                                    | 560/10570 [00:01<00:26, 383.92it/s]\u001b[A\n",
      "  6%|██▏                                   | 601/10570 [00:01<00:25, 390.75it/s]\u001b[A\n",
      "  6%|██▎                                   | 644/10570 [00:01<00:24, 399.24it/s]\u001b[A\n",
      "  6%|██▍                                   | 686/10570 [00:02<00:24, 402.81it/s]\u001b[A\n",
      "  7%|██▌                                   | 727/10570 [00:02<00:26, 377.30it/s]\u001b[A\n",
      "  7%|██▊                                   | 766/10570 [00:02<00:26, 365.73it/s]\u001b[A\n",
      "  8%|██▉                                   | 803/10570 [00:02<00:29, 333.04it/s]\u001b[A\n",
      "  8%|███                                   | 837/10570 [00:02<00:30, 322.68it/s]\u001b[A\n",
      "  8%|███▏                                  | 874/10570 [00:02<00:29, 334.20it/s]\u001b[A\n",
      "  9%|███▎                                  | 908/10570 [00:02<00:29, 322.79it/s]\u001b[A\n",
      "  9%|███▍                                  | 943/10570 [00:02<00:29, 329.18it/s]\u001b[A\n",
      "  9%|███▌                                  | 977/10570 [00:02<00:29, 320.81it/s]\u001b[A\n",
      " 10%|███▌                                 | 1010/10570 [00:03<00:33, 287.28it/s]\u001b[A\n",
      " 10%|███▋                                 | 1040/10570 [00:03<00:32, 289.18it/s]\u001b[A\n",
      " 10%|███▋                                 | 1070/10570 [00:03<00:32, 290.13it/s]\u001b[A\n",
      " 10%|███▊                                 | 1100/10570 [00:03<00:32, 290.94it/s]\u001b[A\n",
      " 11%|███▉                                 | 1133/10570 [00:03<00:31, 299.97it/s]\u001b[A\n",
      " 11%|████                                 | 1166/10570 [00:03<00:30, 307.49it/s]\u001b[A\n",
      " 11%|████▏                                | 1197/10570 [00:03<00:30, 303.90it/s]\u001b[A\n",
      " 12%|████▎                                | 1228/10570 [00:03<00:38, 241.75it/s]\u001b[A\n",
      " 12%|████▍                                | 1260/10570 [00:04<00:35, 260.50it/s]\u001b[A\n",
      " 12%|████▌                                | 1289/10570 [00:04<00:34, 267.73it/s]\u001b[A\n",
      " 12%|████▌                                | 1318/10570 [00:04<00:35, 261.99it/s]\u001b[A\n",
      " 13%|████▋                                | 1346/10570 [00:04<00:37, 248.32it/s]\u001b[A\n",
      " 13%|████▊                                | 1372/10570 [00:04<00:36, 248.76it/s]\u001b[A\n",
      " 13%|████▉                                | 1398/10570 [00:04<00:39, 234.11it/s]\u001b[A\n",
      " 13%|████▉                                | 1422/10570 [00:04<00:42, 215.71it/s]\u001b[A\n",
      " 14%|█████                                | 1445/10570 [00:04<00:44, 206.50it/s]\u001b[A\n",
      " 14%|█████▏                               | 1471/10570 [00:04<00:41, 220.16it/s]\u001b[A\n",
      " 14%|█████▏                               | 1499/10570 [00:05<00:38, 236.14it/s]\u001b[A\n",
      " 14%|█████▎                               | 1524/10570 [00:05<00:40, 222.54it/s]\u001b[A\n",
      " 15%|█████▍                               | 1547/10570 [00:05<00:41, 216.95it/s]\u001b[A\n",
      " 15%|█████▌                               | 1577/10570 [00:05<00:37, 237.52it/s]\u001b[A\n",
      " 15%|█████▌                               | 1602/10570 [00:05<00:38, 231.13it/s]\u001b[A\n",
      " 15%|█████▋                               | 1631/10570 [00:05<00:36, 245.26it/s]\u001b[A\n",
      " 16%|█████▊                               | 1665/10570 [00:05<00:33, 269.64it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3792/10570 [00:13<00:25, 266.91it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3820/10570 [00:13<00:25, 269.55it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3848/10570 [00:13<00:27, 248.78it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3874/10570 [00:14<00:28, 234.58it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3901/10570 [00:14<00:27, 242.12it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3929/10570 [00:14<00:26, 251.82it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3960/10570 [00:14<00:24, 266.14it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3991/10570 [00:14<00:23, 275.94it/s]\u001b[A\n",
      " 38%|██████████████                       | 4019/10570 [00:14<00:24, 272.05it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4047/10570 [00:14<00:24, 270.22it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4076/10570 [00:14<00:23, 272.57it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4104/10570 [00:14<00:23, 274.56it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4133/10570 [00:15<00:23, 276.68it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4161/10570 [00:15<00:26, 239.93it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4186/10570 [00:15<00:27, 228.55it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4210/10570 [00:15<00:30, 210.42it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4238/10570 [00:15<00:27, 227.34it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4262/10570 [00:15<00:30, 207.86it/s]\u001b[A\n",
      " 41%|██████████████▉                      | 4284/10570 [00:15<00:39, 157.91it/s]\u001b[A\n",
      " 41%|███████████████                      | 4302/10570 [00:16<00:39, 156.75it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4325/10570 [00:16<00:36, 172.51it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4356/10570 [00:16<00:30, 205.18it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4382/10570 [00:16<00:28, 219.00it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4407/10570 [00:16<00:27, 225.43it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4435/10570 [00:16<00:25, 239.75it/s]\u001b[A\n",
      " 42%|███████████████▋                     | 4464/10570 [00:16<00:24, 253.37it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4494/10570 [00:16<00:23, 263.75it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4524/10570 [00:16<00:22, 272.23it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4554/10570 [00:16<00:21, 279.77it/s]\u001b[A\n",
      " 43%|████████████████                     | 4583/10570 [00:17<00:21, 279.74it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4612/10570 [00:17<00:21, 274.68it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4640/10570 [00:17<00:22, 268.22it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4667/10570 [00:17<00:25, 235.57it/s]\u001b[A\n",
      " 44%|████████████████▍                    | 4692/10570 [00:17<00:26, 225.27it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4716/10570 [00:17<00:26, 217.11it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4739/10570 [00:17<00:27, 215.33it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4762/10570 [00:17<00:26, 218.41it/s]\u001b[A\n",
      " 45%|████████████████▊                    | 4789/10570 [00:17<00:25, 229.95it/s]\u001b[A\n",
      " 46%|████████████████▊                    | 4818/10570 [00:18<00:23, 245.28it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4847/10570 [00:18<00:22, 257.89it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4874/10570 [00:18<00:23, 245.75it/s]\u001b[A\n",
      " 46%|█████████████████▏                   | 4903/10570 [00:18<00:21, 257.66it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4930/10570 [00:18<00:23, 243.11it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4956/10570 [00:18<00:22, 245.75it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4986/10570 [00:18<00:21, 260.62it/s]\u001b[A\n",
      " 47%|█████████████████▌                   | 5016/10570 [00:18<00:20, 270.61it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5045/10570 [00:18<00:20, 275.75it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5076/10570 [00:19<00:19, 284.51it/s]\u001b[A\n",
      " 48%|█████████████████▉                   | 5107/10570 [00:19<00:18, 291.28it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 5138/10570 [00:19<00:18, 295.98it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5168/10570 [00:19<00:18, 293.45it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5199/10570 [00:19<00:18, 296.96it/s]\u001b[A\n",
      " 49%|██████████████████▎                  | 5230/10570 [00:19<00:17, 300.72it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5261/10570 [00:19<00:18, 288.17it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5290/10570 [00:19<00:18, 278.89it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5319/10570 [00:19<00:19, 265.73it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5348/10570 [00:19<00:19, 271.05it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5376/10570 [00:20<00:20, 255.09it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5402/10570 [00:20<00:21, 241.11it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5427/10570 [00:20<00:22, 231.70it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5451/10570 [00:20<00:22, 231.70it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5475/10570 [00:20<00:21, 233.22it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5499/10570 [00:20<00:22, 225.74it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5527/10570 [00:20<00:21, 239.61it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5553/10570 [00:20<00:20, 243.60it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5583/10570 [00:20<00:19, 258.98it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5611/10570 [00:21<00:18, 263.07it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5641/10570 [00:21<00:18, 271.27it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5669/10570 [00:21<00:18, 258.25it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5699/10570 [00:21<00:18, 269.25it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5727/10570 [00:21<00:17, 269.40it/s]\u001b[A\n",
      " 54%|████████████████████▏                | 5755/10570 [00:21<00:17, 269.64it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5783/10570 [00:21<00:18, 263.35it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5810/10570 [00:21<00:18, 262.30it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5839/10570 [00:21<00:17, 269.90it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5867/10570 [00:22<00:17, 271.40it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5896/10570 [00:22<00:17, 274.36it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5924/10570 [00:22<00:17, 272.22it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5953/10570 [00:22<00:16, 275.68it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5982/10570 [00:22<00:16, 278.23it/s]\u001b[A\n",
      " 57%|█████████████████████                | 6010/10570 [00:22<00:16, 276.67it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6038/10570 [00:22<00:16, 274.49it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6067/10570 [00:22<00:16, 276.87it/s]\u001b[A\n",
      " 58%|█████████████████████▎               | 6097/10570 [00:22<00:15, 283.42it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6130/10570 [00:22<00:15, 295.30it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6162/10570 [00:23<00:14, 300.68it/s]\u001b[A\n",
      " 59%|█████████████████████▋               | 6193/10570 [00:23<00:14, 301.86it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6224/10570 [00:23<00:14, 298.57it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6254/10570 [00:23<00:15, 285.03it/s]\u001b[A\n",
      " 59%|██████████████████████               | 6286/10570 [00:23<00:14, 292.65it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6318/10570 [00:23<00:14, 299.29it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6349/10570 [00:23<00:14, 294.37it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6379/10570 [00:23<00:14, 280.72it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6408/10570 [00:23<00:14, 282.44it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6437/10570 [00:24<00:14, 276.24it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6465/10570 [00:24<00:14, 277.18it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6494/10570 [00:24<00:14, 277.74it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6522/10570 [00:24<00:14, 277.21it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6552/10570 [00:24<00:14, 282.36it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6581/10570 [00:24<00:14, 276.33it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6609/10570 [00:24<00:15, 259.88it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6636/10570 [00:24<00:15, 255.81it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6666/10570 [00:24<00:14, 267.75it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6697/10570 [00:24<00:13, 278.14it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6726/10570 [00:25<00:13, 280.00it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6756/10570 [00:25<00:13, 284.06it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6786/10570 [00:25<00:13, 286.71it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6817/10570 [00:25<00:12, 292.04it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6849/10570 [00:25<00:12, 298.03it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6880/10570 [00:25<00:12, 298.50it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6910/10570 [00:25<00:12, 286.48it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6939/10570 [00:25<00:12, 281.06it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6968/10570 [00:25<00:13, 263.05it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6995/10570 [00:26<00:13, 259.12it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7022/10570 [00:26<00:13, 255.48it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7052/10570 [00:26<00:13, 265.72it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7082/10570 [00:26<00:12, 272.89it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7112/10570 [00:26<00:12, 280.19it/s]\u001b[A\n",
      " 68%|████████████████████████▉            | 7141/10570 [00:26<00:12, 278.44it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7169/10570 [00:26<00:12, 273.62it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7197/10570 [00:26<00:12, 273.26it/s]\u001b[A\n",
      " 68%|█████████████████████████▎           | 7227/10570 [00:26<00:11, 279.31it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7256/10570 [00:26<00:11, 281.21it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7285/10570 [00:27<00:11, 282.77it/s]\u001b[A\n",
      " 69%|█████████████████████████▌           | 7314/10570 [00:27<00:11, 284.18it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7343/10570 [00:27<00:12, 260.09it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7374/10570 [00:27<00:11, 272.85it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7405/10570 [00:27<00:11, 282.06it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7436/10570 [00:27<00:10, 288.55it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7466/10570 [00:27<00:10, 283.55it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7495/10570 [00:27<00:11, 259.34it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7522/10570 [00:28<00:12, 239.44it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7547/10570 [00:28<00:12, 240.52it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7576/10570 [00:28<00:11, 253.62it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7605/10570 [00:28<00:11, 263.72it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7634/10570 [00:28<00:10, 270.34it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7662/10570 [00:28<00:11, 261.11it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7689/10570 [00:28<00:11, 254.76it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7715/10570 [00:28<00:11, 248.37it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7742/10570 [00:28<00:11, 252.63it/s]\u001b[A\n",
      " 73%|███████████████████████████▏         | 7768/10570 [00:28<00:11, 245.88it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7793/10570 [00:29<00:11, 241.30it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7818/10570 [00:29<00:11, 232.47it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7842/10570 [00:29<00:11, 229.42it/s]\u001b[A\n",
      " 74%|███████████████████████████▌         | 7869/10570 [00:29<00:11, 239.35it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7894/10570 [00:29<00:11, 240.84it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7922/10570 [00:29<00:10, 249.85it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7949/10570 [00:29<00:10, 253.20it/s]\u001b[A\n",
      " 75%|███████████████████████████▉         | 7976/10570 [00:29<00:10, 256.40it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8004/10570 [00:29<00:09, 263.13it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8032/10570 [00:30<00:09, 267.89it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8059/10570 [00:30<00:09, 256.36it/s]\u001b[A\n",
      " 76%|████████████████████████████▎        | 8085/10570 [00:30<00:09, 255.90it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8111/10570 [00:30<00:09, 255.66it/s]\u001b[A\n",
      " 77%|████████████████████████████▍        | 8137/10570 [00:30<00:09, 256.76it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8166/10570 [00:30<00:09, 264.67it/s]\u001b[A\n",
      " 78%|████████████████████████████▋        | 8193/10570 [00:30<00:08, 266.06it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8221/10570 [00:30<00:08, 268.88it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8248/10570 [00:30<00:08, 258.12it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8275/10570 [00:30<00:08, 257.58it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8301/10570 [00:31<00:08, 253.10it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8327/10570 [00:31<00:09, 240.15it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8352/10570 [00:31<00:10, 219.04it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8375/10570 [00:31<00:10, 206.19it/s]\u001b[A\n",
      " 79%|█████████████████████████████▍       | 8399/10570 [00:31<00:10, 214.17it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8429/10570 [00:31<00:09, 237.13it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8457/10570 [00:31<00:08, 247.34it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8483/10570 [00:31<00:08, 249.03it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8510/10570 [00:31<00:08, 252.94it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8538/10570 [00:32<00:07, 260.65it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8565/10570 [00:32<00:07, 261.94it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8592/10570 [00:32<00:07, 263.76it/s]\u001b[A\n",
      " 82%|██████████████████████████████▏      | 8621/10570 [00:32<00:07, 271.15it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8651/10570 [00:32<00:06, 276.77it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8679/10570 [00:32<00:06, 277.42it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8709/10570 [00:32<00:06, 281.35it/s]\u001b[A\n",
      " 83%|██████████████████████████████▌      | 8738/10570 [00:32<00:06, 281.57it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8768/10570 [00:32<00:06, 285.72it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8799/10570 [00:32<00:06, 292.45it/s]\u001b[A\n",
      " 84%|██████████████████████████████▉      | 8829/10570 [00:33<00:05, 291.43it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8860/10570 [00:33<00:05, 295.62it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8890/10570 [00:33<00:05, 294.54it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8922/10570 [00:33<00:05, 299.93it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8952/10570 [00:33<00:05, 299.19it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8984/10570 [00:33<00:05, 304.54it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9017/10570 [00:33<00:04, 310.68it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9049/10570 [00:33<00:05, 297.81it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9079/10570 [00:33<00:05, 297.46it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9109/10570 [00:34<00:05, 285.86it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9138/10570 [00:34<00:05, 281.32it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9167/10570 [00:34<00:05, 279.75it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9197/10570 [00:34<00:04, 283.31it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9227/10570 [00:34<00:04, 285.74it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9257/10570 [00:34<00:04, 287.11it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9286/10570 [00:34<00:04, 274.43it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9314/10570 [00:34<00:04, 252.45it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9342/10570 [00:34<00:04, 257.85it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9371/10570 [00:35<00:04, 264.31it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9398/10570 [00:35<00:04, 265.81it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9427/10570 [00:35<00:04, 271.21it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9457/10570 [00:35<00:04, 278.11it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9489/10570 [00:35<00:03, 290.10it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9521/10570 [00:35<00:03, 297.50it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9551/10570 [00:35<00:03, 295.33it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9581/10570 [00:35<00:03, 278.14it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9610/10570 [00:35<00:03, 246.17it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9636/10570 [00:35<00:03, 244.80it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▊   | 9662/10570 [00:36<00:04, 226.78it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9686/10570 [00:36<00:04, 220.95it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9714/10570 [00:36<00:03, 236.32it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9744/10570 [00:36<00:03, 252.86it/s]\u001b[A\n",
      " 92%|██████████████████████████████████▏  | 9774/10570 [00:36<00:03, 264.40it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9801/10570 [00:36<00:02, 264.69it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9830/10570 [00:36<00:02, 270.07it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▌  | 9858/10570 [00:36<00:02, 267.56it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▌  | 9885/10570 [00:36<00:02, 256.84it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9915/10570 [00:37<00:02, 267.67it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9946/10570 [00:37<00:02, 279.51it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▉  | 9975/10570 [00:37<00:02, 280.45it/s]\u001b[A\n",
      " 95%|██████████████████████████████████  | 10004/10570 [00:37<00:02, 275.93it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10034/10570 [00:37<00:01, 282.06it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10063/10570 [00:37<00:01, 280.11it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10092/10570 [00:37<00:01, 281.46it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10122/10570 [00:37<00:01, 284.18it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10151/10570 [00:37<00:01, 272.33it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10179/10570 [00:38<00:01, 258.72it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10207/10570 [00:38<00:01, 263.10it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10237/10570 [00:38<00:01, 270.99it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10265/10570 [00:38<00:01, 271.08it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10295/10570 [00:38<00:00, 277.23it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10323/10570 [00:38<00:00, 264.57it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10350/10570 [00:38<00:00, 257.92it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10376/10570 [00:38<00:00, 235.66it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10405/10570 [00:38<00:00, 248.74it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10434/10570 [00:39<00:00, 259.44it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10463/10570 [00:39<00:00, 267.63it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10494/10570 [00:39<00:00, 278.76it/s]\u001b[A\n",
      "100%|███████████████████████████████████▊| 10525/10570 [00:39<00:00, 285.26it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:39<00:00, 267.73it/s]\u001b[A\n",
      "05/04/2022 15:48:29 - INFO - utils_qa - Saving predictions to ./squad_bert_base_uncased_advanced/eval_predictions.json.\n",
      "05/04/2022 15:48:29 - INFO - utils_qa - Saving nbest_preds to ./squad_bert_base_uncased_advanced/eval_nbest_predictions.json.\n",
      "05/04/2022 15:48:35 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████| 899/899 [04:59<00:00,  3.00it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     2.0\n",
      "  eval_exact_match = 80.9272\n",
      "  eval_f1          =  88.312\n",
      "  eval_samples     =   10784\n"
     ]
    }
   ],
   "source": [
    "!sh train_bert_squadv1_advanced.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18f3c3-0739-48b4-9401-075a1ecf9f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

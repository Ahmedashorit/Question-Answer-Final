{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fb23da4-43c3-4d30-baab-1609a984ac03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Question-Answer' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Ahmedashorit/Question-Answer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "350c62aa-6b49-4aaf-a0f5-7fd201042379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==1.9.0 in /opt/conda/lib/python3.8/site-packages (1.9.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.9.0) (4.0.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.9.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccbb317e-db3a-4b25-9f25-5bc0fb4fbe11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.18.0)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.8/site-packages (2.1.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers) (0.0.47)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.1.18)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (5.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.1.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (18.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ff6fb8-8ee5-4d40-ad9e-e77842ff49b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/Question-Answer\n"
     ]
    }
   ],
   "source": [
    "cd Question-Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c404bc54-c761-497c-8501-75a94db13f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/28/2022 15:04:53 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
      "04/28/2022 15:04:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=IntervalStrategy.NO,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./squad_roberta_base_uncased_advanced/runs/Apr28_15-04-53_nrtcpzpq71,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=OptimizerNames.ADAMW_HF,\n",
      "output_dir=./squad_roberta_base_uncased_advanced,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=12,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./squad_roberta_base_uncased_advanced,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpnla9yzsm\n",
      "Downloading builder script: 5.27kB [00:00, 1.98MB/s]                            \n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/d9c3aa7f62a649420879b459590308637e57da131d55b38c7f9f1a53ea04d795.88910a81ad509b864eb2728ed18e25076f86eaa3cd11c5587ab5ceea8903a4bc.py\n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpjnc3dm3h\n",
      "Downloading metadata: 2.36kB [00:00, 1.58MB/s]                                  \n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/datasets/squad/dataset_infos.json in cache at /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/28/2022 15:04:53 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/ffc75978c532e6d7de83fa7cedfdc48f4dd70d8f4df32efb61a230b144055188.36bd0df82ceb24eeafc05394b25c534952fd7b2eaacf2b1f49933a8330f5800b\n",
      "04/28/2022 15:04:53 - INFO - datasets.builder - No config specified, defaulting to first: squad/plain_text\n",
      "04/28/2022 15:04:53 - INFO - datasets.info - Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\n",
      "04/28/2022 15:04:53 - INFO - datasets.builder - Generating dataset squad (/root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "Downloading and preparing dataset squad/plain_text (download: 33.51 MiB, generated: 85.63 MiB, post-processed: Unknown size, total: 119.14 MiB) to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n",
      "04/28/2022 15:04:53 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "Downloading data files:   0%|                             | 0/2 [00:00<?, ?it/s]04/28/2022 15:04:54 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpqfrn6ux0\n",
      "\n",
      "Downloading data:   0%|                             | 0.00/8.12M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:  72%|██████████████▎     | 5.81M/8.12M [00:00<00:00, 58.0MB/s]\u001b[A\n",
      "Downloading data: 11.6MB [00:00, 54.5MB/s]                                      \u001b[A\n",
      "Downloading data: 17.1MB [00:00, 52.2MB/s]\u001b[A\n",
      "Downloading data: 23.1MB [00:00, 55.4MB/s]\u001b[A\n",
      "Downloading data: 30.3MB [00:00, 55.3MB/s]\u001b[A\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/b8bb19735e1bb591510a01cc032f4c9f969bc0eeb081ae1b328cd306f3b24008\n",
      "Downloading data files:  50%|██████████▌          | 1/2 [00:01<00:01,  1.36s/it]04/28/2022 15:04:55 - INFO - datasets.utils.file_utils - https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpl06kpbif\n",
      "\n",
      "Downloading data: 4.85MB [00:00, 72.4MB/s]                                      \u001b[A\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.file_utils - storing https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json in cache at /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9d5462987ef5f814fe15a369c1724f6ec39a2018b3b6271a9d7d2598686ca2ff\n",
      "Downloading data files: 100%|█████████████████████| 2/2 [00:01<00:00,  1.25it/s]\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
      "Extracting data files: 100%|████████████████████| 2/2 [00:00<00:00, 1586.35it/s]\n",
      "04/28/2022 15:04:55 - INFO - datasets.utils.info_utils - All the checksums matched successfully for dataset source files\n",
      "04/28/2022 15:04:55 - INFO - datasets.builder - Generating train split\n",
      "04/28/2022 15:05:03 - INFO - datasets.builder - Generating validation split     \n",
      "04/28/2022 15:05:05 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
      "Dataset squad downloaded and prepared to /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 404.95it/s]\n",
      "[INFO|hub.py:583] 2022-04-28 15:05:05,087 >> https://huggingface.co/roberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfr0__qw9\n",
      "Downloading: 100%|██████████████████████████████| 481/481 [00:00<00:00, 345kB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 15:05:05,184 >> storing https://huggingface.co/roberta-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|hub.py:595] 2022-04-28 15:05:05,184 >> creating metadata file for /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 15:05:05,184 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 15:05:05,185 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:344] 2022-04-28 15:05:05,260 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 15:05:05,330 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 15:05:05,331 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-28 15:05:05,509 >> https://huggingface.co/roberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpsyk7yd4s\n",
      "Downloading: 100%|███████████████████████████| 878k/878k [00:00<00:00, 14.8MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 15:05:05,713 >> storing https://huggingface.co/roberta-base/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:595] 2022-04-28 15:05:05,714 >> creating metadata file for /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|hub.py:583] 2022-04-28 15:05:05,785 >> https://huggingface.co/roberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0vnq73sr\n",
      "Downloading: 100%|███████████████████████████| 446k/446k [00:00<00:00, 7.06MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 15:05:05,946 >> storing https://huggingface.co/roberta-base/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:595] 2022-04-28 15:05:05,946 >> creating metadata file for /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|hub.py:583] 2022-04-28 15:05:06,021 >> https://huggingface.co/roberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp76d24rme\n",
      "Downloading: 100%|█████████████████████████| 1.29M/1.29M [00:00<00:00, 12.7MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 15:05:06,245 >> storing https://huggingface.co/roberta-base/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|hub.py:595] 2022-04-28 15:05:06,245 >> creating metadata file for /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1778] 2022-04-28 15:05:06,464 >> loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:654] 2022-04-28 15:05:06,536 >> loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n",
      "[INFO|configuration_utils.py:690] 2022-04-28 15:05:06,537 >> Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "[INFO|hub.py:583] 2022-04-28 15:05:06,748 >> https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpowr71zrt\n",
      "Downloading: 100%|███████████████████████████| 478M/478M [00:21<00:00, 23.1MB/s]\n",
      "[INFO|hub.py:587] 2022-04-28 15:05:28,545 >> storing https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|hub.py:595] 2022-04-28 15:05:28,545 >> creating metadata file for /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[INFO|modeling_utils.py:1772] 2022-04-28 15:05:28,546 >> loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n",
      "[WARNING|modeling_utils.py:2048] 2022-04-28 15:05:30,471 >> Some weights of the model checkpoint at roberta-base were not used when initializing Advanced_RobertaForQuestionAnswering: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing Advanced_RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Advanced_RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "[WARNING|modeling_utils.py:2059] 2022-04-28 15:05:30,471 >> Some weights of Advanced_RobertaForQuestionAnswering were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.embeddings.position_ids', 'qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Running tokenizer on train dataset:   0%|                | 0/88 [00:00<?, ?ba/s]04/28/2022 15:05:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-14f6fd6aca632c07.arrow\n",
      "Running tokenizer on train dataset: 100%|███████| 88/88 [00:52<00:00,  1.67ba/s]\n",
      "Running tokenizer on validation dataset:   0%|           | 0/11 [00:00<?, ?ba/s]04/28/2022 15:06:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-24b4c5e73a71b09e.arrow\n",
      "Running tokenizer on validation dataset: 100%|██| 11/11 [00:42<00:00,  3.83s/ba]\n",
      "04/28/2022 15:07:05 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmpaoqo_9rm\n",
      "Downloading builder script: 4.50kB [00:00, 3.39MB/s]                            \n",
      "04/28/2022 15:07:05 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/squad.py in cache at /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/28/2022 15:07:05 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/169f412d0d43db5c2abc824c816c6aa8813794c2fd2a21712922a1f21b4b33d8.391a9da0201eab4bd2cc35b16f80e4bc05c0ef76af7d1006e3afe33a3188d76f.py\n",
      "04/28/2022 15:07:06 - INFO - datasets.utils.file_utils - https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/tmp28btbqea\n",
      "Downloading extra modules: 3.31kB [00:00, 1.41MB/s]                             \n",
      "04/28/2022 15:07:06 - INFO - datasets.utils.file_utils - storing https://raw.githubusercontent.com/huggingface/datasets/2.1.0/metrics/squad/evaluate.py in cache at /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "04/28/2022 15:07:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/38a29c6f1a240415fc1a1e5e076fff932360037b59763238665c6dbdca543f78.6f69c3ff9e10aa1cbdc6e91d27e158ea86a785f54a36a9e964ef8b3b78cf3cd6.py\n",
      "[INFO|trainer.py:453] 2022-04-28 15:07:09,971 >> Using amp half precision backend\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1290] 2022-04-28 15:07:10,086 >> ***** Running training *****\n",
      "[INFO|trainer.py:1291] 2022-04-28 15:07:10,086 >>   Num examples = 88568\n",
      "[INFO|trainer.py:1292] 2022-04-28 15:07:10,086 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1293] 2022-04-28 15:07:10,086 >>   Instantaneous batch size per device = 12\n",
      "[INFO|trainer.py:1294] 2022-04-28 15:07:10,086 >>   Total train batch size (w. parallel, distributed & accumulation) = 12\n",
      "[INFO|trainer.py:1295] 2022-04-28 15:07:10,086 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1296] 2022-04-28 15:07:10,086 >>   Total optimization steps = 7381\n",
      "  0%|                                                  | 0/7381 [00:00<?, ?it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 1.8304, 'learning_rate': 2.7988077496274216e-05, 'epoch': 0.07}        \n",
      "  7%|██▌                                   | 500/7381 [06:06<1:24:06,  1.36it/s][INFO|trainer.py:2166] 2022-04-28 15:13:16,213 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:13:16,222 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:13:22,285 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:13:22,291 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:13:22,316 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-500/special_tokens_map.json\n",
      "{'loss': 1.2539, 'learning_rate': 2.5955832543015852e-05, 'epoch': 0.14}        \n",
      " 14%|█████                                | 1000/7381 [12:29<1:17:36,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 15:19:39,325 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-1000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:19:39,333 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:19:44,724 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:19:44,731 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:19:44,742 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1000/special_tokens_map.json\n",
      "{'loss': 1.1551, 'learning_rate': 2.3923587589757488e-05, 'epoch': 0.2}         \n",
      " 20%|███████▌                             | 1500/7381 [18:50<1:11:52,  1.36it/s][INFO|trainer.py:2166] 2022-04-28 15:26:00,839 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-1500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:26:00,858 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:26:06,965 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:26:06,970 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:26:07,001 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-1500/special_tokens_map.json\n",
      "{'loss': 1.076, 'learning_rate': 2.189134263649912e-05, 'epoch': 0.27}          \n",
      " 27%|██████████                           | 2000/7381 [25:13<1:05:37,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 15:32:23,786 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-2000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:32:23,791 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:32:29,046 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:32:29,053 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:32:29,057 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2000/special_tokens_map.json\n",
      "{'loss': 1.0146, 'learning_rate': 1.9859097683240756e-05, 'epoch': 0.34}        \n",
      " 34%|█████████████▏                         | 2500/7381 [31:34<59:25,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 15:38:44,863 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-2500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:38:44,873 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:38:50,014 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:38:50,020 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:38:50,026 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-2500/special_tokens_map.json\n",
      "{'loss': 1.0081, 'learning_rate': 1.7826852729982388e-05, 'epoch': 0.41}        \n",
      " 41%|███████████████▊                       | 3000/7381 [37:55<53:54,  1.35it/s][INFO|trainer.py:2166] 2022-04-28 15:45:06,040 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-3000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:45:06,058 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:45:11,176 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:45:11,192 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:45:11,200 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3000/special_tokens_map.json\n",
      "{'loss': 0.9971, 'learning_rate': 1.579460777672402e-05, 'epoch': 0.47}         \n",
      " 47%|██████████████████▍                    | 3500/7381 [44:16<47:09,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 15:51:26,499 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-3500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:51:26,507 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:51:32,061 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:51:32,069 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:51:32,072 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-3500/special_tokens_map.json\n",
      "{'loss': 0.9154, 'learning_rate': 1.3762362823465655e-05, 'epoch': 0.54}        \n",
      " 54%|█████████████████████▏                 | 4000/7381 [50:37<41:14,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 15:57:47,463 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-4000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 15:57:47,471 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 15:57:52,206 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 15:57:52,211 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 15:57:52,229 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4000/special_tokens_map.json\n",
      " 57%|██████████████████████                 | 4179/7381 [53:02<39:02,  1.37it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.9526, 'learning_rate': 1.1734182360113807e-05, 'epoch': 0.61}        \n",
      " 61%|███████████████████████▊               | 4500/7381 [56:58<34:59,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 16:04:08,298 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-4500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:04:08,312 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:04:13,499 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:04:13,507 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:04:13,513 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-4500/special_tokens_map.json\n",
      " 64%|███████████████████████▊             | 4749/7381 [1:00:15<32:10,  1.36it/s]/opt/conda/lib/python3.8/site-packages/transformers/trainer.py:1465: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "{'loss': 0.8846, 'learning_rate': 9.706001896761956e-06, 'epoch': 0.68}         \n",
      " 68%|█████████████████████████            | 5000/7381 [1:03:19<28:58,  1.37it/s][INFO|trainer.py:2166] 2022-04-28 16:10:29,711 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-5000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:10:29,719 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:10:35,084 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:10:35,087 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:10:35,090 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5000/special_tokens_map.json\n",
      "{'loss': 0.9057, 'learning_rate': 7.67375694350359e-06, 'epoch': 0.75}          \n",
      " 75%|███████████████████████████▌         | 5500/7381 [1:09:41<22:58,  1.36it/s][INFO|trainer.py:2166] 2022-04-28 16:16:51,533 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-5500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:16:51,578 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:16:57,118 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:16:57,124 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:16:57,127 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-5500/special_tokens_map.json\n",
      "{'loss': 0.8703, 'learning_rate': 5.641511990245224e-06, 'epoch': 0.81}         \n",
      " 81%|██████████████████████████████       | 6000/7381 [1:15:59<16:39,  1.38it/s][INFO|trainer.py:2166] 2022-04-28 16:23:09,966 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-6000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:23:09,972 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:23:14,654 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:23:14,658 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:23:14,664 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6000/special_tokens_map.json\n",
      "{'loss': 0.8787, 'learning_rate': 3.609267036986858e-06, 'epoch': 0.88}         \n",
      " 88%|████████████████████████████████▌    | 6500/7381 [1:22:15<10:39,  1.38it/s][INFO|trainer.py:2166] 2022-04-28 16:29:25,967 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-6500\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:29:25,990 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:29:30,947 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:29:30,953 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:29:30,955 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-6500/special_tokens_map.json\n",
      "{'loss': 0.8784, 'learning_rate': 1.577022083728492e-06, 'epoch': 0.95}         \n",
      " 95%|███████████████████████████████████  | 7000/7381 [1:28:32<04:36,  1.38it/s][INFO|trainer.py:2166] 2022-04-28 16:35:42,339 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced/checkpoint-7000\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:35:42,342 >> Configuration saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:35:43,386 >> Model weights saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:35:43,389 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:35:43,390 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/checkpoint-7000/special_tokens_map.json\n",
      "100%|█████████████████████████████████████| 7381/7381 [1:33:11<00:00,  1.51it/s][INFO|trainer.py:1530] 2022-04-28 16:40:21,690 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 5591.6039, 'train_samples_per_second': 15.839, 'train_steps_per_second': 1.32, 'train_loss': 1.035190253056835, 'epoch': 1.0}\n",
      "100%|█████████████████████████████████████| 7381/7381 [1:33:11<00:00,  1.32it/s]\n",
      "[INFO|trainer.py:2166] 2022-04-28 16:40:21,794 >> Saving model checkpoint to ./squad_roberta_base_uncased_advanced\n",
      "[INFO|configuration_utils.py:441] 2022-04-28 16:40:21,795 >> Configuration saved in ./squad_roberta_base_uncased_advanced/config.json\n",
      "[INFO|modeling_utils.py:1378] 2022-04-28 16:40:22,814 >> Model weights saved in ./squad_roberta_base_uncased_advanced/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2086] 2022-04-28 16:40:22,827 >> tokenizer config file saved in ./squad_roberta_base_uncased_advanced/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2092] 2022-04-28 16:40:22,828 >> Special tokens file saved in ./squad_roberta_base_uncased_advanced/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     1.0352\n",
      "  train_runtime            = 1:33:11.60\n",
      "  train_samples            =      88568\n",
      "  train_samples_per_second =     15.839\n",
      "  train_steps_per_second   =       1.32\n",
      "04/28/2022 16:40:22 - INFO - __main__ - *** Evaluate ***\n",
      "[INFO|trainer.py:566] 2022-04-28 16:40:22,995 >> The following columns in the evaluation set  don't have a corresponding argument in `Advanced_RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `Advanced_RobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
      "[INFO|trainer.py:2416] 2022-04-28 16:40:22,999 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:2418] 2022-04-28 16:40:22,999 >>   Num examples = 10790\n",
      "[INFO|trainer.py:2421] 2022-04-28 16:40:22,999 >>   Batch size = 12\n",
      "100%|████████████████████████████████████████▉| 899/900 [03:56<00:00,  3.80it/s]04/28/2022 16:44:30 - INFO - utils_qa - Post-processing 10570 example predictions split into 10790 features.\n",
      "\n",
      "  0%|                                                 | 0/10570 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|▏                                      | 42/10570 [00:00<00:25, 411.66it/s]\u001b[A\n",
      "  1%|▎                                      | 92/10570 [00:00<00:22, 457.99it/s]\u001b[A\n",
      "  1%|▌                                     | 141/10570 [00:00<00:22, 470.58it/s]\u001b[A\n",
      "  2%|▋                                     | 192/10570 [00:00<00:21, 482.85it/s]\u001b[A\n",
      "  2%|▊                                     | 241/10570 [00:00<00:22, 458.27it/s]\u001b[A\n",
      "  3%|█                                     | 288/10570 [00:00<00:26, 387.31it/s]\u001b[A\n",
      "  3%|█▏                                    | 334/10570 [00:00<00:25, 407.51it/s]\u001b[A\n",
      "  4%|█▍                                    | 386/10570 [00:00<00:23, 437.66it/s]\u001b[A\n",
      "  4%|█▌                                    | 432/10570 [00:00<00:23, 435.33it/s]\u001b[A\n",
      "  5%|█▋                                    | 479/10570 [00:01<00:22, 443.48it/s]\u001b[A\n",
      "  5%|█▉                                    | 526/10570 [00:01<00:22, 449.38it/s]\u001b[A\n",
      "  5%|██                                    | 572/10570 [00:01<00:22, 447.69it/s]\u001b[A\n",
      "  6%|██▏                                   | 621/10570 [00:01<00:21, 458.75it/s]\u001b[A\n",
      "  6%|██▍                                   | 671/10570 [00:01<00:21, 468.53it/s]\u001b[A\n",
      "  7%|██▌                                   | 719/10570 [00:01<00:21, 459.92it/s]\u001b[A\n",
      "  7%|██▊                                   | 766/10570 [00:01<00:21, 447.05it/s]\u001b[A\n",
      "  8%|██▉                                   | 811/10570 [00:01<00:22, 430.40it/s]\u001b[A\n",
      "  8%|███                                   | 855/10570 [00:01<00:23, 420.36it/s]\u001b[A\n",
      "  8%|███▏                                  | 898/10570 [00:02<00:22, 421.88it/s]\u001b[A\n",
      "  9%|███▍                                  | 941/10570 [00:02<00:23, 415.69it/s]\u001b[A\n",
      "  9%|███▌                                  | 984/10570 [00:02<00:22, 417.84it/s]\u001b[A\n",
      " 10%|███▌                                 | 1026/10570 [00:02<00:23, 411.88it/s]\u001b[A\n",
      " 10%|███▋                                 | 1068/10570 [00:02<00:24, 388.05it/s]\u001b[A\n",
      " 10%|███▉                                 | 1108/10570 [00:02<00:26, 355.92it/s]\u001b[A\n",
      " 11%|████                                 | 1145/10570 [00:02<00:26, 351.00it/s]\u001b[A\n",
      " 11%|████▏                                | 1181/10570 [00:02<00:27, 347.14it/s]\u001b[A\n",
      " 12%|████▎                                | 1216/10570 [00:02<00:27, 344.26it/s]\u001b[A\n",
      " 12%|████▍                                | 1253/10570 [00:03<00:26, 351.28it/s]\u001b[A\n",
      " 12%|████▌                                | 1292/10570 [00:03<00:25, 359.90it/s]\u001b[A\n",
      " 13%|████▋                                | 1332/10570 [00:03<00:24, 370.65it/s]\u001b[A\n",
      " 13%|████▊                                | 1370/10570 [00:03<00:24, 371.89it/s]\u001b[A\n",
      " 13%|████▉                                | 1408/10570 [00:03<00:25, 362.68it/s]\u001b[A\n",
      " 14%|█████                                | 1445/10570 [00:03<00:25, 353.11it/s]\u001b[A\n",
      " 14%|█████▏                               | 1481/10570 [00:03<00:26, 343.79it/s]\u001b[A\n",
      " 14%|█████▎                               | 1516/10570 [00:03<00:26, 345.05it/s]\u001b[A\n",
      " 15%|█████▍                               | 1553/10570 [00:03<00:25, 352.02it/s]\u001b[A\n",
      " 15%|█████▌                               | 1592/10570 [00:03<00:24, 360.94it/s]\u001b[A\n",
      " 15%|█████▋                               | 1630/10570 [00:04<00:24, 365.41it/s]\u001b[A\n",
      " 16%|█████▊                               | 1668/10570 [00:04<00:24, 368.34it/s]\u001b[A\n",
      " 16%|█████▉                               | 1705/10570 [00:04<00:24, 368.17it/s]\u001b[A\n",
      " 16%|██████                               | 1744/10570 [00:04<00:23, 372.60it/s]\u001b[A\n",
      " 17%|██████▏                              | 1782/10570 [00:04<00:23, 370.25it/s]\u001b[A\n",
      " 17%|██████▎                              | 1820/10570 [00:04<00:23, 366.33it/s]\u001b[A\n",
      " 18%|██████▌                              | 1857/10570 [00:04<00:24, 359.03it/s]\u001b[A\n",
      " 18%|██████▋                              | 1893/10570 [00:04<00:24, 355.43it/s]\u001b[A\n",
      " 18%|██████▊                              | 1930/10570 [00:04<00:24, 357.42it/s]\u001b[A\n",
      " 19%|██████▉                              | 1969/10570 [00:05<00:23, 364.91it/s]\u001b[A\n",
      " 19%|███████                              | 2009/10570 [00:05<00:22, 373.26it/s]\u001b[A\n",
      " 19%|███████▏                             | 2049/10570 [00:05<00:22, 380.83it/s]\u001b[A\n",
      " 20%|███████▎                             | 2088/10570 [00:05<00:22, 380.11it/s]\u001b[A\n",
      " 20%|███████▍                             | 2127/10570 [00:05<00:23, 353.07it/s]\u001b[A\n",
      " 20%|███████▌                             | 2165/10570 [00:05<00:23, 359.51it/s]\u001b[A\n",
      " 21%|███████▋                             | 2203/10570 [00:05<00:22, 365.10it/s]\u001b[A\n",
      " 21%|███████▊                             | 2240/10570 [00:05<00:22, 362.84it/s]\u001b[A\n",
      " 22%|███████▉                             | 2277/10570 [00:05<00:23, 356.94it/s]\u001b[A\n",
      " 22%|████████                             | 2313/10570 [00:05<00:23, 349.47it/s]\u001b[A\n",
      " 22%|████████▏                            | 2349/10570 [00:06<00:23, 351.39it/s]\u001b[A\n",
      " 23%|████████▎                            | 2385/10570 [00:06<00:23, 344.89it/s]\u001b[A\n",
      " 23%|████████▍                            | 2422/10570 [00:06<00:23, 351.29it/s]\u001b[A\n",
      " 23%|████████▌                            | 2459/10570 [00:06<00:22, 354.78it/s]\u001b[A\n",
      " 24%|████████▋                            | 2497/10570 [00:06<00:22, 359.71it/s]\u001b[A\n",
      " 24%|████████▊                            | 2534/10570 [00:06<00:22, 361.71it/s]\u001b[A\n",
      " 24%|████████▉                            | 2571/10570 [00:06<00:22, 358.73it/s]\u001b[A\n",
      " 25%|█████████▏                           | 2610/10570 [00:06<00:21, 365.37it/s]\u001b[A\n",
      " 25%|█████████▎                           | 2650/10570 [00:06<00:21, 374.34it/s]\u001b[A\n",
      " 25%|█████████▍                           | 2688/10570 [00:06<00:21, 373.48it/s]\u001b[A\n",
      " 26%|█████████▌                           | 2726/10570 [00:07<00:21, 371.82it/s]\u001b[A\n",
      " 26%|█████████▋                           | 2765/10570 [00:07<00:20, 375.90it/s]\u001b[A\n",
      " 27%|█████████▊                           | 2803/10570 [00:07<00:20, 374.90it/s]\u001b[A\n",
      " 27%|█████████▉                           | 2841/10570 [00:07<00:21, 366.15it/s]\u001b[A\n",
      " 27%|██████████                           | 2878/10570 [00:07<00:21, 357.86it/s]\u001b[A\n",
      " 28%|██████████▏                          | 2915/10570 [00:07<00:21, 359.25it/s]\u001b[A\n",
      " 28%|██████████▎                          | 2953/10570 [00:07<00:20, 364.04it/s]\u001b[A\n",
      " 28%|██████████▍                          | 2990/10570 [00:07<00:20, 362.00it/s]\u001b[A\n",
      " 29%|██████████▌                          | 3027/10570 [00:07<00:20, 361.89it/s]\u001b[A\n",
      " 29%|██████████▋                          | 3064/10570 [00:08<00:21, 349.20it/s]\u001b[A\n",
      " 29%|██████████▊                          | 3100/10570 [00:08<00:22, 339.44it/s]\u001b[A\n",
      " 30%|██████████▉                          | 3136/10570 [00:08<00:21, 343.56it/s]\u001b[A\n",
      " 30%|███████████                          | 3173/10570 [00:08<00:21, 350.33it/s]\u001b[A\n",
      " 30%|███████████▏                         | 3209/10570 [00:08<00:20, 351.71it/s]\u001b[A\n",
      " 31%|███████████▎                         | 3245/10570 [00:08<00:21, 334.67it/s]\u001b[A\n",
      " 31%|███████████▍                         | 3280/10570 [00:08<00:21, 336.67it/s]\u001b[A\n",
      " 31%|███████████▌                         | 3319/10570 [00:08<00:20, 350.92it/s]\u001b[A\n",
      " 32%|███████████▊                         | 3357/10570 [00:08<00:20, 358.69it/s]\u001b[A\n",
      " 32%|███████████▉                         | 3395/10570 [00:08<00:19, 363.01it/s]\u001b[A\n",
      " 32%|████████████                         | 3432/10570 [00:09<00:19, 363.93it/s]\u001b[A\n",
      " 33%|████████████▏                        | 3469/10570 [00:09<00:19, 361.74it/s]\u001b[A\n",
      " 33%|████████████▎                        | 3506/10570 [00:09<00:19, 355.96it/s]\u001b[A\n",
      " 34%|████████████▍                        | 3544/10570 [00:09<00:19, 361.91it/s]\u001b[A\n",
      " 34%|████████████▌                        | 3581/10570 [00:09<00:19, 351.98it/s]\u001b[A\n",
      " 34%|████████████▋                        | 3618/10570 [00:09<00:19, 355.47it/s]\u001b[A\n",
      " 35%|████████████▊                        | 3654/10570 [00:09<00:20, 339.31it/s]\u001b[A\n",
      " 35%|████████████▉                        | 3689/10570 [00:09<00:20, 330.95it/s]\u001b[A\n",
      " 35%|█████████████                        | 3724/10570 [00:09<00:20, 333.35it/s]\u001b[A\n",
      " 36%|█████████████▏                       | 3758/10570 [00:10<00:20, 330.05it/s]\u001b[A\n",
      " 36%|█████████████▎                       | 3793/10570 [00:10<00:20, 335.47it/s]\u001b[A\n",
      " 36%|█████████████▍                       | 3829/10570 [00:10<00:19, 341.20it/s]\u001b[A\n",
      " 37%|█████████████▌                       | 3866/10570 [00:10<00:19, 349.46it/s]\u001b[A\n",
      " 37%|█████████████▋                       | 3902/10570 [00:10<00:19, 349.93it/s]\u001b[A\n",
      " 37%|█████████████▊                       | 3939/10570 [00:10<00:18, 354.89it/s]\u001b[A\n",
      " 38%|█████████████▉                       | 3977/10570 [00:10<00:18, 361.88it/s]\u001b[A\n",
      " 38%|██████████████                       | 4014/10570 [00:10<00:18, 361.97it/s]\u001b[A\n",
      " 38%|██████████████▏                      | 4051/10570 [00:10<00:18, 362.11it/s]\u001b[A\n",
      " 39%|██████████████▎                      | 4088/10570 [00:10<00:17, 362.64it/s]\u001b[A\n",
      " 39%|██████████████▍                      | 4125/10570 [00:11<00:18, 350.91it/s]\u001b[A\n",
      " 39%|██████████████▌                      | 4161/10570 [00:11<00:20, 315.98it/s]\u001b[A\n",
      " 40%|██████████████▋                      | 4194/10570 [00:11<00:21, 294.05it/s]\u001b[A\n",
      " 40%|██████████████▊                      | 4225/10570 [00:11<00:21, 289.09it/s]\u001b[A\n",
      " 40%|██████████████▉                      | 4258/10570 [00:11<00:21, 299.17it/s]\u001b[A\n",
      " 41%|███████████████                      | 4289/10570 [00:11<00:26, 233.11it/s]\u001b[A\n",
      " 41%|███████████████                      | 4315/10570 [00:11<00:27, 229.13it/s]\u001b[A\n",
      " 41%|███████████████▏                     | 4352/10570 [00:11<00:23, 262.43it/s]\u001b[A\n",
      " 41%|███████████████▎                     | 4386/10570 [00:12<00:22, 280.75it/s]\u001b[A\n",
      " 42%|███████████████▍                     | 4422/10570 [00:12<00:20, 301.58it/s]\u001b[A\n",
      " 42%|███████████████▌                     | 4458/10570 [00:12<00:19, 315.86it/s]\u001b[A\n",
      " 43%|███████████████▋                     | 4494/10570 [00:12<00:18, 327.21it/s]\u001b[A\n",
      " 43%|███████████████▊                     | 4529/10570 [00:12<00:18, 331.61it/s]\u001b[A\n",
      " 43%|███████████████▉                     | 4565/10570 [00:12<00:17, 338.08it/s]\u001b[A\n",
      " 44%|████████████████                     | 4600/10570 [00:12<00:17, 334.13it/s]\u001b[A\n",
      " 44%|████████████████▏                    | 4635/10570 [00:12<00:17, 338.35it/s]\u001b[A\n",
      " 44%|████████████████▎                    | 4670/10570 [00:12<00:18, 323.45it/s]\u001b[A\n",
      " 45%|████████████████▍                    | 4707/10570 [00:13<00:17, 334.49it/s]\u001b[A\n",
      " 45%|████████████████▌                    | 4744/10570 [00:13<00:16, 344.05it/s]\u001b[A\n",
      " 45%|████████████████▋                    | 4779/10570 [00:13<00:17, 330.07it/s]\u001b[A\n",
      " 46%|████████████████▊                    | 4815/10570 [00:13<00:17, 337.38it/s]\u001b[A\n",
      " 46%|████████████████▉                    | 4851/10570 [00:13<00:16, 337.63it/s]\u001b[A\n",
      " 46%|█████████████████                    | 4885/10570 [00:13<00:17, 321.90it/s]\u001b[A\n",
      " 47%|█████████████████▏                   | 4918/10570 [00:13<00:17, 317.89it/s]\u001b[A\n",
      " 47%|█████████████████▎                   | 4953/10570 [00:13<00:17, 325.44it/s]\u001b[A\n",
      " 47%|█████████████████▍                   | 4988/10570 [00:13<00:16, 330.53it/s]\u001b[A\n",
      " 48%|█████████████████▌                   | 5022/10570 [00:13<00:16, 330.99it/s]\u001b[A\n",
      " 48%|█████████████████▋                   | 5058/10570 [00:14<00:16, 337.86it/s]\u001b[A\n",
      " 48%|█████████████████▊                   | 5092/10570 [00:14<00:16, 338.11it/s]\u001b[A\n",
      " 49%|█████████████████▉                   | 5130/10570 [00:14<00:15, 349.31it/s]\u001b[A\n",
      " 49%|██████████████████                   | 5166/10570 [00:14<00:15, 351.04it/s]\u001b[A\n",
      " 49%|██████████████████▏                  | 5202/10570 [00:14<00:15, 347.81it/s]\u001b[A\n",
      " 50%|██████████████████▎                  | 5239/10570 [00:14<00:15, 353.01it/s]\u001b[A\n",
      " 50%|██████████████████▍                  | 5275/10570 [00:14<00:15, 350.57it/s]\u001b[A\n",
      " 50%|██████████████████▌                  | 5311/10570 [00:14<00:15, 345.84it/s]\u001b[A\n",
      " 51%|██████████████████▋                  | 5346/10570 [00:14<00:15, 336.53it/s]\u001b[A\n",
      " 51%|██████████████████▊                  | 5381/10570 [00:15<00:15, 340.30it/s]\u001b[A\n",
      " 51%|██████████████████▉                  | 5418/10570 [00:15<00:14, 348.55it/s]\u001b[A\n",
      " 52%|███████████████████                  | 5453/10570 [00:15<00:15, 339.40it/s]\u001b[A\n",
      " 52%|███████████████████▏                 | 5488/10570 [00:15<00:15, 327.66it/s]\u001b[A\n",
      " 52%|███████████████████▎                 | 5525/10570 [00:15<00:14, 337.86it/s]\u001b[A\n",
      " 53%|███████████████████▍                 | 5562/10570 [00:15<00:14, 345.02it/s]\u001b[A\n",
      " 53%|███████████████████▌                 | 5598/10570 [00:15<00:14, 347.99it/s]\u001b[A\n",
      " 53%|███████████████████▋                 | 5636/10570 [00:15<00:13, 354.52it/s]\u001b[A\n",
      " 54%|███████████████████▊                 | 5672/10570 [00:15<00:14, 340.28it/s]\u001b[A\n",
      " 54%|███████████████████▉                 | 5707/10570 [00:15<00:14, 340.37it/s]\u001b[A\n",
      " 54%|████████████████████                 | 5745/10570 [00:16<00:13, 349.06it/s]\u001b[A\n",
      " 55%|████████████████████▏                | 5781/10570 [00:16<00:14, 335.66it/s]\u001b[A\n",
      " 55%|████████████████████▎                | 5816/10570 [00:16<00:14, 338.17it/s]\u001b[A\n",
      " 55%|████████████████████▍                | 5854/10570 [00:16<00:13, 348.97it/s]\u001b[A\n",
      " 56%|████████████████████▌                | 5890/10570 [00:16<00:13, 347.77it/s]\u001b[A\n",
      " 56%|████████████████████▋                | 5925/10570 [00:16<00:13, 346.64it/s]\u001b[A\n",
      " 56%|████████████████████▊                | 5963/10570 [00:16<00:13, 354.38it/s]\u001b[A\n",
      " 57%|████████████████████▉                | 5999/10570 [00:16<00:12, 352.00it/s]\u001b[A\n",
      " 57%|█████████████████████▏               | 6035/10570 [00:16<00:13, 341.24it/s]\u001b[A\n",
      " 57%|█████████████████████▎               | 6072/10570 [00:17<00:12, 347.40it/s]\u001b[A\n",
      " 58%|█████████████████████▍               | 6107/10570 [00:17<00:13, 342.06it/s]\u001b[A\n",
      " 58%|█████████████████████▌               | 6145/10570 [00:17<00:12, 351.25it/s]\u001b[A\n",
      " 58%|█████████████████████▋               | 6183/10570 [00:17<00:12, 356.21it/s]\u001b[A\n",
      " 59%|█████████████████████▊               | 6219/10570 [00:17<00:12, 356.05it/s]\u001b[A\n",
      " 59%|█████████████████████▉               | 6255/10570 [00:17<00:12, 344.23it/s]\u001b[A\n",
      " 60%|██████████████████████               | 6292/10570 [00:17<00:12, 349.47it/s]\u001b[A\n",
      " 60%|██████████████████████▏              | 6328/10570 [00:17<00:12, 339.68it/s]\u001b[A\n",
      " 60%|██████████████████████▎              | 6364/10570 [00:17<00:12, 344.64it/s]\u001b[A\n",
      " 61%|██████████████████████▍              | 6399/10570 [00:17<00:12, 332.21it/s]\u001b[A\n",
      " 61%|██████████████████████▌              | 6436/10570 [00:18<00:12, 341.71it/s]\u001b[A\n",
      " 61%|██████████████████████▋              | 6471/10570 [00:18<00:15, 264.72it/s]\u001b[A\n",
      " 62%|██████████████████████▊              | 6508/10570 [00:18<00:14, 289.46it/s]\u001b[A\n",
      " 62%|██████████████████████▉              | 6543/10570 [00:18<00:13, 302.95it/s]\u001b[A\n",
      " 62%|███████████████████████              | 6576/10570 [00:18<00:12, 308.44it/s]\u001b[A\n",
      " 63%|███████████████████████▏             | 6609/10570 [00:18<00:12, 304.95it/s]\u001b[A\n",
      " 63%|███████████████████████▎             | 6645/10570 [00:18<00:12, 319.84it/s]\u001b[A\n",
      " 63%|███████████████████████▍             | 6679/10570 [00:18<00:12, 323.69it/s]\u001b[A\n",
      " 64%|███████████████████████▍             | 6712/10570 [00:19<00:12, 320.89it/s]\u001b[A\n",
      " 64%|███████████████████████▌             | 6745/10570 [00:19<00:11, 318.92it/s]\u001b[A\n",
      " 64%|███████████████████████▋             | 6778/10570 [00:19<00:11, 318.73it/s]\u001b[A\n",
      " 64%|███████████████████████▊             | 6811/10570 [00:19<00:11, 313.97it/s]\u001b[A\n",
      " 65%|███████████████████████▉             | 6847/10570 [00:19<00:11, 326.34it/s]\u001b[A\n",
      " 65%|████████████████████████             | 6883/10570 [00:19<00:11, 334.77it/s]\u001b[A\n",
      " 65%|████████████████████████▏            | 6920/10570 [00:19<00:10, 343.81it/s]\u001b[A\n",
      " 66%|████████████████████████▎            | 6955/10570 [00:19<00:10, 333.63it/s]\u001b[A\n",
      " 66%|████████████████████████▍            | 6989/10570 [00:19<00:10, 330.93it/s]\u001b[A\n",
      " 66%|████████████████████████▌            | 7023/10570 [00:19<00:10, 322.49it/s]\u001b[A\n",
      " 67%|████████████████████████▋            | 7056/10570 [00:20<00:10, 324.26it/s]\u001b[A\n",
      " 67%|████████████████████████▊            | 7094/10570 [00:20<00:10, 337.99it/s]\u001b[A\n",
      " 67%|████████████████████████▉            | 7131/10570 [00:20<00:09, 346.81it/s]\u001b[A\n",
      " 68%|█████████████████████████            | 7169/10570 [00:20<00:09, 354.72it/s]\u001b[A\n",
      " 68%|█████████████████████████▏           | 7207/10570 [00:20<00:09, 359.54it/s]\u001b[A\n",
      " 69%|█████████████████████████▎           | 7245/10570 [00:20<00:09, 363.59it/s]\u001b[A\n",
      " 69%|█████████████████████████▍           | 7283/10570 [00:20<00:08, 367.03it/s]\u001b[A\n",
      " 69%|█████████████████████████▋           | 7321/10570 [00:20<00:08, 370.49it/s]\u001b[A\n",
      " 70%|█████████████████████████▊           | 7359/10570 [00:20<00:09, 344.25it/s]\u001b[A\n",
      " 70%|█████████████████████████▉           | 7397/10570 [00:21<00:09, 351.83it/s]\u001b[A\n",
      " 70%|██████████████████████████           | 7435/10570 [00:21<00:08, 357.07it/s]\u001b[A\n",
      " 71%|██████████████████████████▏          | 7471/10570 [00:21<00:08, 352.84it/s]\u001b[A\n",
      " 71%|██████████████████████████▎          | 7507/10570 [00:21<00:08, 344.08it/s]\u001b[A\n",
      " 71%|██████████████████████████▍          | 7546/10570 [00:21<00:08, 354.82it/s]\u001b[A\n",
      " 72%|██████████████████████████▌          | 7586/10570 [00:21<00:08, 365.37it/s]\u001b[A\n",
      " 72%|██████████████████████████▋          | 7625/10570 [00:21<00:07, 371.19it/s]\u001b[A\n",
      " 72%|██████████████████████████▊          | 7663/10570 [00:21<00:08, 345.73it/s]\u001b[A\n",
      " 73%|██████████████████████████▉          | 7699/10570 [00:21<00:08, 348.81it/s]\u001b[A\n",
      " 73%|███████████████████████████          | 7736/10570 [00:21<00:08, 353.08it/s]\u001b[A\n",
      " 74%|███████████████████████████▏         | 7772/10570 [00:22<00:08, 347.27it/s]\u001b[A\n",
      " 74%|███████████████████████████▎         | 7807/10570 [00:22<00:07, 345.99it/s]\u001b[A\n",
      " 74%|███████████████████████████▍         | 7844/10570 [00:22<00:07, 350.48it/s]\u001b[A\n",
      " 75%|███████████████████████████▌         | 7882/10570 [00:22<00:07, 358.68it/s]\u001b[A\n",
      " 75%|███████████████████████████▋         | 7920/10570 [00:22<00:07, 362.71it/s]\u001b[A\n",
      " 75%|███████████████████████████▊         | 7957/10570 [00:22<00:07, 348.93it/s]\u001b[A\n",
      " 76%|███████████████████████████▉         | 7993/10570 [00:22<00:07, 336.40it/s]\u001b[A\n",
      " 76%|████████████████████████████         | 8030/10570 [00:22<00:07, 343.33it/s]\u001b[A\n",
      " 76%|████████████████████████████▏        | 8065/10570 [00:22<00:07, 322.38it/s]\u001b[A\n",
      " 77%|████████████████████████████▎        | 8104/10570 [00:23<00:07, 339.74it/s]\u001b[A\n",
      " 77%|████████████████████████████▌        | 8142/10570 [00:23<00:06, 350.26it/s]\u001b[A\n",
      " 77%|████████████████████████████▋        | 8178/10570 [00:23<00:06, 349.72it/s]\u001b[A\n",
      " 78%|████████████████████████████▊        | 8214/10570 [00:23<00:07, 335.85it/s]\u001b[A\n",
      " 78%|████████████████████████████▉        | 8251/10570 [00:23<00:06, 343.62it/s]\u001b[A\n",
      " 78%|█████████████████████████████        | 8286/10570 [00:23<00:06, 329.78it/s]\u001b[A\n",
      " 79%|█████████████████████████████        | 8320/10570 [00:23<00:07, 307.95it/s]\u001b[A\n",
      " 79%|█████████████████████████████▏       | 8352/10570 [00:23<00:07, 303.32it/s]\u001b[A\n",
      " 79%|█████████████████████████████▎       | 8384/10570 [00:23<00:07, 307.38it/s]\u001b[A\n",
      " 80%|█████████████████████████████▍       | 8416/10570 [00:24<00:06, 309.17it/s]\u001b[A\n",
      " 80%|█████████████████████████████▌       | 8448/10570 [00:24<00:07, 290.34it/s]\u001b[A\n",
      " 80%|█████████████████████████████▋       | 8479/10570 [00:24<00:07, 295.45it/s]\u001b[A\n",
      " 81%|█████████████████████████████▊       | 8509/10570 [00:24<00:07, 289.52it/s]\u001b[A\n",
      " 81%|█████████████████████████████▉       | 8542/10570 [00:24<00:06, 299.65it/s]\u001b[A\n",
      " 81%|██████████████████████████████       | 8575/10570 [00:24<00:06, 308.09it/s]\u001b[A\n",
      " 81%|██████████████████████████████▏      | 8614/10570 [00:24<00:05, 330.66it/s]\u001b[A\n",
      " 82%|██████████████████████████████▎      | 8648/10570 [00:24<00:05, 329.87it/s]\u001b[A\n",
      " 82%|██████████████████████████████▍      | 8683/10570 [00:24<00:05, 333.13it/s]\u001b[A\n",
      " 82%|██████████████████████████████▌      | 8719/10570 [00:24<00:05, 340.41it/s]\u001b[A\n",
      " 83%|██████████████████████████████▋      | 8754/10570 [00:25<00:05, 339.67it/s]\u001b[A\n",
      " 83%|██████████████████████████████▊      | 8789/10570 [00:25<00:05, 336.97it/s]\u001b[A\n",
      " 83%|██████████████████████████████▉      | 8825/10570 [00:25<00:05, 342.80it/s]\u001b[A\n",
      " 84%|███████████████████████████████      | 8862/10570 [00:25<00:04, 348.81it/s]\u001b[A\n",
      " 84%|███████████████████████████████▏     | 8898/10570 [00:25<00:04, 351.20it/s]\u001b[A\n",
      " 85%|███████████████████████████████▎     | 8934/10570 [00:25<00:04, 344.03it/s]\u001b[A\n",
      " 85%|███████████████████████████████▍     | 8970/10570 [00:25<00:04, 346.36it/s]\u001b[A\n",
      " 85%|███████████████████████████████▌     | 9005/10570 [00:25<00:04, 344.19it/s]\u001b[A\n",
      " 86%|███████████████████████████████▋     | 9041/10570 [00:25<00:04, 345.97it/s]\u001b[A\n",
      " 86%|███████████████████████████████▊     | 9078/10570 [00:25<00:04, 351.11it/s]\u001b[A\n",
      " 86%|███████████████████████████████▉     | 9116/10570 [00:26<00:04, 356.93it/s]\u001b[A\n",
      " 87%|████████████████████████████████     | 9152/10570 [00:26<00:04, 345.96it/s]\u001b[A\n",
      " 87%|████████████████████████████████▏    | 9187/10570 [00:26<00:04, 342.64it/s]\u001b[A\n",
      " 87%|████████████████████████████████▎    | 9224/10570 [00:26<00:03, 349.87it/s]\u001b[A\n",
      " 88%|████████████████████████████████▍    | 9263/10570 [00:26<00:03, 359.13it/s]\u001b[A\n",
      " 88%|████████████████████████████████▌    | 9301/10570 [00:26<00:03, 364.27it/s]\u001b[A\n",
      " 88%|████████████████████████████████▋    | 9338/10570 [00:26<00:03, 359.88it/s]\u001b[A\n",
      " 89%|████████████████████████████████▊    | 9376/10570 [00:26<00:03, 364.89it/s]\u001b[A\n",
      " 89%|████████████████████████████████▉    | 9413/10570 [00:26<00:03, 362.51it/s]\u001b[A\n",
      " 89%|█████████████████████████████████    | 9450/10570 [00:27<00:03, 360.07it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▏   | 9487/10570 [00:27<00:02, 362.40it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▎   | 9524/10570 [00:27<00:02, 351.56it/s]\u001b[A\n",
      " 90%|█████████████████████████████████▍   | 9561/10570 [00:27<00:02, 354.24it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▌   | 9597/10570 [00:27<00:02, 354.98it/s]\u001b[A\n",
      " 91%|█████████████████████████████████▋   | 9635/10570 [00:27<00:02, 359.72it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▊   | 9674/10570 [00:27<00:02, 367.16it/s]\u001b[A\n",
      " 92%|█████████████████████████████████▉   | 9711/10570 [00:27<00:02, 361.71it/s]\u001b[A\n",
      " 92%|██████████████████████████████████   | 9748/10570 [00:27<00:02, 355.17it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▏  | 9784/10570 [00:27<00:02, 347.90it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▎  | 9819/10570 [00:28<00:02, 348.03it/s]\u001b[A\n",
      " 93%|██████████████████████████████████▍  | 9854/10570 [00:28<00:02, 345.59it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▌  | 9891/10570 [00:28<00:01, 352.07it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▋  | 9927/10570 [00:28<00:01, 339.82it/s]\u001b[A\n",
      " 94%|██████████████████████████████████▊  | 9962/10570 [00:28<00:01, 341.87it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▉  | 9997/10570 [00:28<00:01, 340.70it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▏ | 10032/10570 [00:28<00:01, 343.02it/s]\u001b[A\n",
      " 95%|██████████████████████████████████▎ | 10069/10570 [00:28<00:01, 349.18it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▍ | 10108/10570 [00:28<00:01, 359.33it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▌ | 10144/10570 [00:29<00:01, 356.25it/s]\u001b[A\n",
      " 96%|██████████████████████████████████▋ | 10180/10570 [00:29<00:01, 348.35it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▊ | 10216/10570 [00:29<00:01, 349.25it/s]\u001b[A\n",
      " 97%|██████████████████████████████████▉ | 10252/10570 [00:29<00:00, 351.34it/s]\u001b[A\n",
      " 97%|███████████████████████████████████ | 10291/10570 [00:29<00:00, 361.30it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▏| 10328/10570 [00:29<00:00, 350.09it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▎| 10366/10570 [00:29<00:00, 357.65it/s]\u001b[A\n",
      " 98%|███████████████████████████████████▍| 10402/10570 [00:29<00:00, 348.35it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▌| 10437/10570 [00:29<00:00, 342.84it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▋| 10472/10570 [00:29<00:00, 337.55it/s]\u001b[A\n",
      " 99%|███████████████████████████████████▊| 10506/10570 [00:30<00:00, 335.33it/s]\u001b[A\n",
      "100%|████████████████████████████████████| 10570/10570 [00:30<00:00, 349.58it/s]\u001b[A\n",
      "04/28/2022 16:45:01 - INFO - utils_qa - Saving predictions to ./squad_roberta_base_uncased_advanced/eval_predictions.json.\n",
      "04/28/2022 16:45:01 - INFO - utils_qa - Saving nbest_preds to ./squad_roberta_base_uncased_advanced/eval_nbest_predictions.json.\n",
      "04/28/2022 16:45:06 - INFO - datasets.metric - Removing /root/.cache/huggingface/metrics/squad/default/default_experiment-1-0.arrow\n",
      "100%|█████████████████████████████████████████| 900/900 [04:43<00:00,  3.17it/s]\n",
      "***** eval metrics *****\n",
      "  epoch            =     1.0\n",
      "  eval_exact_match =  85.298\n",
      "  eval_f1          = 91.8535\n",
      "  eval_samples     =   10790\n"
     ]
    }
   ],
   "source": [
    "!sh train_roberta_squadv1_advanced.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
